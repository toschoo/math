%% =======================================================
%% (c) Tobias Schoofs
%% =======================================================
%% Math 4 Programmers - The Inverse Element
%% =======================================================

% Plain Style
\documentclass[tikz]{scrreprt}

% Springer Style
%\documentclass[envcountsame]{llncs}

%% ODER: format ==         = "\mathrel{==}"
%% ODER: format /=         = "\neq "
%
%
\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
  {\@namedef{lhs2tex.lhs2tex.sty.read}{}%
   \newcommand\SkipToFmtEnd{}%
   \newcommand\EndFmtInput{}%
   \long\def\SkipToFmtEnd#1\EndFmtInput{}%
  }\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}% suggested by Neil Mitchell
\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

%mathindent has to be defined
\@ifundefined{mathindent}%
  {\newdimen\mathindent\mathindent\leftmargini}%
  {}%

\def\resethooks{%
  \global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]%
  {\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{%
  \let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{%
  \let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}% default is fixed indentation
\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%

\include{cmds}

\begin{document}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\tableofcontents

\chapter{Polynomials} % c07
\section{Numeral Systems}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{NumSystem}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{\Conid{System}.Random}\;(\Varid{randomRIO}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Random}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Quoz}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Real}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

A numeral system consists of a finite set of digits $D$
and a base $b$ for which $b=|D|$, \ie\ $b$ is the cardinality of $D$.
The binary system, for instance, 
uses the digits $D=\lbrace 0,1\rbrace$.
The cardinality of $D$ is 2 and therefore $b=2$.
The decimal system uses the digits $D=\lbrace 0\dots 9\rbrace$ and,
thus, has the base $b=10$.
The hexadecimal system uses the digits $D=\lbrace 0\dots 15\rbrace$,
often given as $D=\lbrace 0\dots 9,a,b,c,d,e,f\rbrace$,
and, therefore, has the base $b=16$.

Numbers in any numeral system are usually represented as strings
of digits. The string

\[
10101010,
\]

for instance, may represent a number in the binary system.
It could be a number in decimal or hexadecimal format, too.
The string 

\[
170,
\]

by contrast, cannot be a binary number, because
it contains the digit 7, which is not element of $D$
in the binary system.
It can represent a number in the decimal (or the hexadecimal) system. 
The string

\[
aa,
\]

can represent a number in the hexadecimal system
but not one in the binary or decimal system.

We interpret such a string, \ie\ convert it
to the decimal system, by rewriting it 
as a formula of the form:

\[
a_nb^n + a_{n-1}b^{n-1} + \dots + a_0b^0,
\]

where $a_i$ are the digits that appear in the string,
$b$ is the base and $n$ the position of the left-most digit
starting to count with 0 on the right-hand side of the string.
The string $10101010$ in binary notation, hence, is interpreted as

\[
1\times 2^7 + 0\times 2^6 + 1\times 2^5 + 0\times 2^4 + 
1\times 2^3 + 0\times 2^2 + 1\times 2^1 + 0\times 2^0,
\]

which can be simplified to

\[
2^7 + 2^5 + 2^3 + 2,
\]

which, in turn, is

\[
128 + 32 + 8 + 2 = 170.
\]

The string 170 in decimal notation is interpreted as

\[
10^2 + 7\times 10 = 170.
\]

Interpreting a string in the notation it is written in
yields just that string.

The string $aa$ in hexadecimal notation is interpreted as

\[
a\times 16 + a.
\]

The digit $a$ corresponds to 10 in the decimal system.
We, therefore, get the equation

\[
10\times 16 + 10 = 160 + 10 = 170.
\]

What do we get, when we relax some of the constraints
defining a numeral system?
Instead of using a finite set of digits,
we could use a number field $F$ (finite or infinite)
so that any member of that field qualifies as a coefficient
in the formulas we used above to interpret numbers
in the decimal system. We would then relax the rule
that the base must be the cardinality of the field.
Instead, we allow any member $x$ of the field 
to serve as a base.
Formulas we get from those new rules would follow the recipe:

\[
a_nx^n + a_{n-1}x^{n-1} + \dots + a_0x^0
\]

or shorter:

\[
\sum_{i=0}^n{a_ix^i}
\]

with $a_i, x \in F$.

Such beasts are indeed well-known
and their name is \term{polynomials}.

The name \emph{poly}nomial stems from the fact
that they may be composed of many terms;
a monomial, by contrast, is a polynomial 
that consists of only one term.
For instance,

\[
5x^2
\]

is a monomial. A binomial is a polynomial
that consists of two terms. This is
an example of a binomial:

\[
x^5 + 2x.
\]

There is nothing special about
monomials and binomials, at least nothing
that would affect their definition as polynomials.
Monomials and binomials are just
polynomials that happen to have only
one or, respectively, two terms.

Polynomials share many properties with numbers.
Like numbers, arithmetic, including
addition, subtraction, multiplication and division
as well as exponentiation, can be defined over polynomials. 
In some cases, numbers reveal their close relation
to polynomials. The binomial theorem states,
for instance, that a product of the form 

\[
(a+b)(a+b)
\]

translates to a formula involving binomial coefficients:

\[
a^2 + 2ab + b^2.
\]

We can interpret this formula as the product 
of the polynomial $x+a$:

\[
(x+a)(x+a),
\]

which yields just another polynomial:

\[
x^2 + 2ax + a^2
\]

Let us replace $a$ for the number 3
and fix $x=10$. We get:

\begin{equation}
(10+3)(10+3) = 10^2 + 2\times 3\times 10 + 3^2 = 100 + 60 + 9 = 169,
\end{equation}

which is just the result of the multiplication $13\times 13$.
Usually, it is harder to recognise this kind of relations 
numbers have with the binomial theorem (and, hence, with polynomials),
because most binomial coefficients are too big to be represented
by a single-digit number. Already in the product $14\times 14$,
the binomial coefficients are `hidden':

\[
(10 + 4) (10 + 4) = 
10^2 + 2\times 4\times 10 + 4^2 =
100 + 2\times 40 + 16.
\]

When we look at the resulting number, we do not recognise
the binomial coefficient anymore -- they are \emph{carried} away:
$100 + 2\times 40 + 16 = 100 + 80 + 16 = 196$.

Indeed, polynomials are not numbers.
Those are different concepts.

Another important difference is that polynomials do not establish
a clear order. For any two distinct numbers, we can clearly say
which of the two is the greater and which is the smaller one.
We cannot decide that based on the formula of the polynomial alone.
One way to decide quickly which of two numbers is the grater one
is to look at the number of their digits. The one with more digits
is necessarily the greater of the two.
In any numeral system it holds that:

\[
a_3b^3 + a_2b^2 + a_1b + a_0 > c_2b^2 + c_1b + c_0
\]

independent of the values of the $a$s and the $c$s.
This is because the base $b$ is fixed.
In the case of polynomials, this is not true.
Consider the following example:

\[
x^3 + x^2 + x + 1 > 100x^2?
\]

For $x=10$, the left-hand side of the inequation is
$1000 + 100 + 10 + 1 = 1111$;
the right-hand side, however, is $100\times 100 = 10000$.

In spite of such differences, we can represent polynomials
very similar to how we represented numbers,
namely as a list of coefficients. This is a valid
implementation in Haskell:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\mathbf{data}\;\Conid{Poly}\;\Varid{a}\mathrel{=}\Conid{P}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{deriving}\;(\Conid{Show}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We add a safe constructor:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{poly}\mathbin{::}(\Conid{Eq}\;\Varid{a},\Conid{Num}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{poly}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{poly}\;\Varid{as}\mathrel{=}\Conid{P}\;(\Varid{cleanz}\;\Varid{as}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{cleanz}\mathbin{::}(\Conid{Eq}\;\Varid{a},\Conid{Num}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cleanz}\;\Varid{xs}\mathrel{=}\Varid{reverse}\mathbin{\$}\Varid{go}\;(\Varid{reverse}\;\Varid{xs}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;[\mskip1.5mu \mskip1.5mu]{}\<[23]%
\>[23]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{go}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[23]%
\>[23]{}\mathrel{=}[\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{go}\;(\mathrm{0}\mathbin{:}\Varid{xs}){}\<[23]%
\>[23]{}\mathrel{=}\Varid{go}\;\Varid{xs}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{xs}{}\<[23]%
\>[23]{}\mathrel{=}\Varid{xs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The constructor makes sure that the resulting polynomial
has at least one coefficient and that all the coefficients
are actually numbers and comparable for equality.
The function \ensuremath{\Varid{cleanz}} called in the constructor
removes leading zeros (which are redundant), just as we did 
when we defined natural numbers.
But note that we reverse, first, the list of coefficients
passed to \ensuremath{\Varid{go}} and, second, the result of \ensuremath{\Varid{go}}.
This means that we store the coefficients from left to right
in ascending order. Usually, we write polynomials out
in descending order of their weight, \ie\:

\[
x^n + x^{n-1} + \dots + x^0.
\]

But, here, we store them in the order:

\[
x^0 + x^1 + \dots + x^{n-1} + x^n.
\]

We will soon see why that is an advantage.

The following function gets the list of coefficients back:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{coeffs}\mathbin{::}\Conid{Poly}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{coeffs}\;(\Conid{P}\;\Varid{as})\mathrel{=}\Varid{as}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Here is a function to pretty-print polynomials:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{38}{@{}>{\hspre}c<{\hspost}@{}}%
\column{38E}{@{}l@{}}%
\column{41}{@{}>{\hspre}l<{\hspost}@{}}%
\column{52}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{pretty}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Show}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{String}{}\<[E]%
\\
\>[3]{}\Varid{pretty}\;\Varid{p}\mathrel{=}\Varid{go}\;(\Varid{reverse}\mathbin{\$}\Varid{weigh}\;\Varid{p}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;{}\<[16]%
\>[16]{}[\mskip1.5mu \mskip1.5mu]\mathrel{=}\text{\tt \char34 \char34}{}\<[E]%
\\
\>[12]{}\Varid{go}\;{}\<[16]%
\>[16]{}((\Varid{i},\Varid{c})\mathbin{:}\Varid{cs})\mathrel{=}{}\<[30]%
\>[30]{}\mathbf{let}\;{}\<[35]%
\>[35]{}\Varid{x}{}\<[38]%
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{i}\equiv \mathrm{0}{}\<[52]%
\>[52]{}\mathrel{=}\text{\tt \char34 \char34}{}\<[E]%
\\
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{i}\equiv \mathrm{1}{}\<[52]%
\>[52]{}\mathrel{=}\text{\tt \char34 x\char34}{}\<[E]%
\\
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{otherwise}{}\<[52]%
\>[52]{}\mathrel{=}\text{\tt \char34 x\char94 \char34}\plus \Varid{show}\;\Varid{i}{}\<[E]%
\\
\>[35]{}\Varid{t}{}\<[38]%
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{c}\equiv \mathrm{1}{}\<[52]%
\>[52]{}\mathrel{=}\Varid{x}{}\<[E]%
\\
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{otherwise}{}\<[52]%
\>[52]{}\mathrel{=}\Varid{show}\;\Varid{c}\plus \Varid{x}{}\<[E]%
\\
\>[35]{}\Varid{o}{}\<[38]%
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{null}\;\Varid{cs}{}\<[52]%
\>[52]{}\mathrel{=}\text{\tt \char34 \char34}{}\<[E]%
\\
\>[38]{}\mid {}\<[38E]%
\>[41]{}\Varid{otherwise}{}\<[52]%
\>[52]{}\mathrel{=}\text{\tt \char34 ~+~\char34}{}\<[E]%
\\
\>[30]{}\mathbf{in}\;\mathbf{if}\;\Varid{c}\equiv \mathrm{0}\;\mathbf{then}\;\Varid{go}\;\Varid{cs}\;\mathbf{else}\;\Varid{t}\plus \Varid{o}\plus \Varid{go}\;\Varid{cs}{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{weigh}\mathbin{::}(\Conid{Num}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to [\mskip1.5mu (\Conid{Integer},\Varid{a})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{weigh}\;(\Conid{P}\;[\mskip1.5mu \mskip1.5mu])\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{weigh}\;(\Conid{P}\;\Varid{as})\mathrel{=}(\Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;\Varid{as}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function demonstrates how we actually 
interpret the list of coefficients.
We first \ensuremath{\Varid{weigh}} them by zipping the list of coefficients 
with a list of integers starting at 0.
One could say: we count the coefficients.
Note that we start with 0, so that
the first coefficient gets the weight 0, 
the second gets the weight 1 and so on.
That, again, reflects our descending
ordering of coefficients.

The reversed weighted list is then passed to \ensuremath{\Varid{go}},
which does the actual printing.
We first determine the substring describing $x$:
if $i$, the weight, is 0, we do not want to write the $x$,
since $x^0=1$. If $i=1$, we just write $x$.
Otherwise we write $x^i$.

Then we determine the term composed of coefficient and $x$.
If the coefficient, $c$ is 1, we just write $x$;
otherwise, we concatenate $c$ with $x$.
Note, however, that we later consider an additional case,
namely, when $c=0$. In this case, we ignore the whole term.

We still consider the operation. 
If the remainder of the list is \ensuremath{\Varid{null}}, \ie\ 
we are now handling the last term, \ensuremath{\Varid{o}} is the empty string.
Otherwise, it is the plus symbol.
Here is room for improvement:
when the coefficient is negative, we do not really need
the operation, since we then write $+ -cx$.
Nicer would be to write only $-cx$.

Finally, we put everything together concatenating a string
composed of term, operation and \ensuremath{\Varid{go}} applied on the remainder 
of the list. 

Here is a list of polynomials and how they are
represented with our Haksell type:

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|c||c|}
\hline
$x^2 + x + 1$ & \ensuremath{\Varid{poly}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\\hline
$5x^5 + 4x^4 + 3x^3 + 2x^2 + x$ &
\ensuremath{\Varid{poly}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]}\\\hline
$5x^4 + 4x^3 + 3x^2 + 2x + 1$  &
\ensuremath{\Varid{poly}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]}\\\hline
$5x^4 + 3x^2 + 1$  &
\ensuremath{\Varid{poly}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{3},\mathrm{0},\mathrm{5}\mskip1.5mu]}\\\hline
\end{tabular}
\endgroup
\end{center}

An important concept related to polynomials is the 
\term{degree}. The degree is a measure of the
\emph{size} of the polynomial. In concrete terms,
it is the greatest exponent in the polynomial.
For us, it is the weight of the right-most element
in the polynomial or, much simpler, the length
of the list of coefficients minus one -- since
we start with zero!
The following function computes the degree
of a given polynomial:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{degree}\mathbin{::}\Conid{Poly}\;\Varid{a}\to \Conid{Int}{}\<[E]%
\\
\>[3]{}\Varid{degree}\;(\Conid{P}\;\Varid{as})\mathrel{=}\Varid{length}\;\Varid{as}\mathbin{-}\mathrm{1}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Note, by the way, that polynomials of degree 0,
those with only one trivial term, 
are just constant numbers.

Finally, here is a useful function that 
creates random polynomials with \ensuremath{\Conid{Natural}}
coefficients:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{randomPoly}\mathbin{::}\Conid{Natural}\to \Conid{Int}\to \Conid{IO}\;(\Conid{Poly}\;\Conid{Natural}){}\<[E]%
\\
\>[3]{}\Varid{randomPoly}\;\Varid{n}\;\Varid{d}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{cs}\leftarrow \Varid{cleanz}\mathbin{<\$>}\Varid{mapM}\;(\mathbin{\char92 \char95 }\to \Varid{randomCoeff}\;\Varid{n})\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{d}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{if}\;\Varid{length}\;\Varid{cs}\mathbin{<}\Varid{d}\;\mathbf{then}\;\Varid{randomPoly}\;\Varid{n}\;\Varid{d}{}\<[E]%
\\
\>[5]{}\hsindent{17}{}\<[22]%
\>[22]{}\mathbf{else}\;\Varid{return}\;(\Conid{P}\;\Varid{cs}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{randomCoeff}\mathbin{::}\Conid{Natural}\to \Conid{IO}\;\Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{randomCoeff}\;\Varid{n}\mathrel{=}\Varid{randomNatural}\;(\mathrm{0},\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives a \ensuremath{\Conid{Natural}} and an \ensuremath{\Conid{Int}}.
The \ensuremath{\Conid{Int}} indicates the number of coefficients
of the polynomial
we want to obtain. The \ensuremath{\Conid{Natural}} is used to
restrict the size of the coefficients we want
to see in the polynomial.
In \ensuremath{\Varid{randomCoeff}}, we use the \ensuremath{\Varid{randomNatural}}
defined in the previous chapter to generate 
a random number between 0 and $n-1$. You might
suspect already where that will lead us:
to polynomials modulo some number.
But before we get there, 
we will study polynomial arithmetic.
\section{Polynomial Arithmetic}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{PolyArith}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{\Conid{Data}.List}\;(\Varid{nub},\Varid{foldl'}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Quoz}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Real}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Zahl}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{NumSystem}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Varid{qualified}\;\Conid{Modular}\;\Varid{as}\;\Conid{M}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

We start with addition and subtraction,
which, in German, are summarised by
the beautiful word \term{strichrechnung}
meaning literally ``dash calculation'' as
opposed to \term{punkt\-rech\-nung} or ``dot calculation'',
which would be multiplication and division.

Polynomial \term{strichrechnung} is easy.
Key is to realise that the structure of polynomials 
is already defined by \term{strichrechnung}:
it is composed of terms each of which is a product
of some number and a power of $x$.
When we add (or subtract) two polynomials,
we just merge them keeping order
according to the exponents of their terms
and add (or subtract) terms with equal exponents:

\begin{equation}
\begin{array}{crcrcccr}
  & ax^n     & + & bx^{n-1}     & + & \dots & + & c\\
+ & dx^n     & + & ex^{n-1}     & + & \dots & + & f\\
= & (a+d)x^n & + & (b+e)x^{n-1} & + & \dots & + & c+f
\end{array}
\end{equation}

With our polynomial representation, it is easy 
to implement this kind of operation. One might think
it was designed especially to support addition and
subtraction. Here is a valid implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{add}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{add}\mathrel{=}\Varid{strich}\;(\mathbin{+}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{sub}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{sub}\mathrel{=}\Varid{strich}\;(\mathbin{-}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{strich}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow (\Varid{a}\to \Varid{a}\to \Varid{a})\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{strich}\;\Varid{o}\;(\Conid{P}\;\Varid{x})\;(\Conid{P}\;\Varid{y}){}\<[25]%
\>[25]{}\mathrel{=}\Conid{P}\;(\Varid{strichlist}\;\Varid{o}\;\Varid{x}\;\Varid{y}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{strichlist}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow (\Varid{a}\to \Varid{a}\to \Varid{a})\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{strichlist}\;\Varid{o}\;\Varid{xs}\;\Varid{ys}\mathrel{=}\Varid{cleanz}\;(\Varid{go}\;\Varid{xs}\;\Varid{ys}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;[\mskip1.5mu \mskip1.5mu]\;\Varid{bs}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{bs}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{as}\;[\mskip1.5mu \mskip1.5mu]{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{as}{}\<[E]%
\\
\>[12]{}\Varid{go}\;(\Varid{a}\mathbin{:}\Varid{as})\;(\Varid{b}\mathbin{:}\Varid{bs}){}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{a}\mathbin{`\Varid{o}`}\Varid{b}\mathbin{:}\Varid{go}\;\Varid{as}\;\Varid{bs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Based on addition, we can also implement \ensuremath{\Varid{sum}}
for polynomials:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{sump}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Conid{Poly}\;\Varid{a}\mskip1.5mu]\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{sump}\mathrel{=}\Varid{foldl'}\;\Varid{add}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Here is one more function that might be useful 
later on; it folds \ensuremath{\Varid{strichlist}} on a list of lists of coefficients:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{strichf}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow (\Varid{a}\to \Varid{a}\to \Varid{a})\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{strichf}\;\Varid{o}\mathrel{=}\Varid{foldl'}\;(\Varid{strichlist}\;\Varid{o})\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

What if we add a polynomial to itself more than once?
With numbers, that would be multiplication.
With polynomials, this is a bit different.
There is in fact an operation that is between
\term{strichrechnung} and \term{punktrechnung},
namely \term{scaling}. Scaling maps multiplication by $n$,
for $n$ some integer,
on all coefficients and, as such, corresponds
to adding a polynomial $n$ times to itself:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{scale}\mathbin{::}(\Conid{Num}\;\Varid{a})\Rightarrow \Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{scale}\;\Varid{n}\;(\Conid{P}\;\Varid{cs})\mathrel{=}\Varid{poly}\;(\Varid{map}\;(\Varid{n}\mathbin{*})\;\Varid{cs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

\term{Punktrechnung}, \ie\ multiplication and division,
is a bit more complex -- because of the distribution law.
Let us start with the simple case where we distribute
a monomial over a polynomial:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mul1}\mathbin{::}\Conid{Num}\;\Varid{a}\Rightarrow (\Varid{a}\to \Varid{a}\to \Varid{a})\to \Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{mul1}\;\Varid{o}\;\Varid{i}\;\Varid{cs}\;\Varid{x}\mathrel{=}\Varid{zeros}\;\Varid{i}\plus [\mskip1.5mu \Varid{c}\mathbin{`\Varid{o}`}\Varid{x}\mid \Varid{c}\leftarrow \Varid{cs}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{zeros}\mathbin{::}\Conid{Num}\;\Varid{a}\Rightarrow \Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{zeros}\;\Varid{i}\mathrel{=}\Varid{take}\;\Varid{i}\;(\Varid{repeat}\;\mathrm{0}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function \ensuremath{\Varid{mul1}} takes a single term (the monomial)
and distributes it over the coefficients of a polynomial 
using the operation \ensuremath{\Varid{o}}.
Each term in the polynomial 
is combined with the single term.
This corresponds to the operation:

\begin{equation}
\begin{array}{lcrlclcccl}
dx^m & \times & ( & ax^n      & + & bx^{n-1}    & + & \dots & + & c)\\
     & =      &   & adx^{m+n} & + & bdx^{n-1+m} & + & \dots & + & cdx^m
\end{array}
\end{equation}

The function \ensuremath{\Varid{mul1}} receives on more parameter,
namely the \ensuremath{\Conid{Int}} $i$ and uses it to generate a sequence of zeros
that is put in front of the resulting coefficient list.
As we will see shortly, the list of zeros reflects the weight
of the single term. In fact, we do not implement the manipulation
of the exponents we see in the abstract formula directly.
Instead, the addition $+m$ is implicitly handled by placing
$m$ zeros at the head of the list resulting in a new polynomial
of degree $m+d$ where $d$ is the degree of the original polynomial.
A simple example:

\[
5x^2 \times (4x^3 + 3x^2 + 2x + 1) = 20x^5 + 15x^4 + 10x^3 + 5x^2
\]

would be:

\ensuremath{\Varid{mul1}\;\mathrm{2}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]\;\mathrm{5}}

which is: 

\ensuremath{\Varid{zero}\;\mathrm{2}\plus (\mathrm{5}\mathbin{*}[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu])} $=$ \ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{5},\mathrm{10},\mathrm{15},\mathrm{20}\mskip1.5mu]}

We, hence, would add 2 zeros, since 2 is the degree
of the monomial.

Now, when we multiply two polynomials, we need to map
all terms in one of the polynomials on the other polynomial
using \ensuremath{\Varid{mul1}}. We further need to pass the weight of
the individual terms of the first polynomial as the \ensuremath{\Conid{Int}}
parameter of \ensuremath{\Varid{mul1}}. What we want to do is:

\ensuremath{[\mskip1.5mu \Varid{mul1}\;(\mathbin{*})\;\Varid{i}\;(\Varid{coeffs}\;\Varid{p1})\;\Varid{p}\mid (\Varid{i},\Varid{p})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;(\Varid{coeffs}\;\Varid{p2})\mskip1.5mu]}.

What would we get applying this formula on
the polynomials, say, 
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]} and \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu]}?
Let us have a look:

\ensuremath{[\mskip1.5mu \Varid{mul1}\;(\mathbin{*})\;\Varid{i}\;([\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu])\;\Varid{p}\mid (\Varid{i},\Varid{p})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{10},\mathrm{12},\mathrm{14},\mathrm{16}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{15},\mathrm{18},\mathrm{21},\mathrm{24}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{20},\mathrm{24},\mathrm{28},\mathrm{32}\mskip1.5mu]\mskip1.5mu]}.

We see a list of four lists, 
one for each coefficient of \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]}.
The first list is the result of distributing 1 
over all the coefficients in \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu]}.
Since 1 is the first element,
its weight is 0: no zeros are put before the resulting list.
The second list results from distributing 2 over \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu]}.
Since 2 is the second element, its weight is 1:
we add one zero.
The same process is repeated for 3 and 4 resulting
in the third and fourth result list.
Since 3 is the the third element, the third resulting list
gets two zeros and, since 4 is the fourth element,
the fourth list gets three zeros.

How do we transform this list of lists back
into a single list of coefficients? Very easy:
we add them together using \ensuremath{\Varid{strichf}}:

\ensuremath{\Varid{strichf}\;(\mathbin{+})}
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{10},\mathrm{12},\mathrm{14},\mathrm{16}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{15},\mathrm{18},\mathrm{21},\mathrm{24}\mskip1.5mu],[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{20},\mathrm{24},\mathrm{28},\mathrm{32}\mskip1.5mu]\mskip1.5mu]}

which is

\ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{16},\mathrm{34},\mathrm{60},\mathrm{61},\mathrm{52},\mathrm{32}\mskip1.5mu]}.

This means that

\begin{equation}
\begin{split}
(4x^3 + 3x^2 + 2x + 1) \times (8x^3 + 7x^2 + 6x + 5) \\
= 32x^6 + 52x^5 + 61x^4 + 60x^3 + 34x^2 + 16x + 5.
\end{split}
\end{equation}

Here is the whole algorithm:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}c<{\hspost}@{}}%
\column{14E}{@{}l@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{28}{@{}>{\hspre}c<{\hspost}@{}}%
\column{28E}{@{}l@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mul}\mathbin{::}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{mul}\;\Varid{p1}\;\Varid{p2}{}\<[14]%
\>[14]{}\mid {}\<[14E]%
\>[17]{}\Varid{d2}\mathbin{>}\Varid{d1}{}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Varid{mul}\;\Varid{p2}\;\Varid{p1}{}\<[E]%
\\
\>[14]{}\mid {}\<[14E]%
\>[17]{}\Varid{otherwise}{}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Conid{P}\;(\Varid{strichf}\;(\mathbin{+})\;\Varid{ms}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{d1}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{degree}\;\Varid{p1}{}\<[E]%
\\
\>[12]{}\Varid{d2}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{degree}\;\Varid{p2}{}\<[E]%
\\
\>[12]{}\Varid{ms}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}[\mskip1.5mu \Varid{mul1}\;(\mathbin{*})\;\Varid{i}\;(\Varid{coeffs}\;\Varid{p1})\;\Varid{p}\mid (\Varid{i},\Varid{p})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;(\Varid{coeffs}\;\Varid{p2})\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

On top of multiplication, we can implement power.
We will, of course, not implement a na\"ive approach
based on repeated multiplication alone. Instead,
we will use the \term{square-and-multiply} approach
we have already used before for numbers.
Here is the code:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{23}{@{}>{\hspre}c<{\hspost}@{}}%
\column{23E}{@{}l@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{37}{@{}>{\hspre}l<{\hspost}@{}}%
\column{59}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{powp}\mathbin{::}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Natural}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{powp}\;\Varid{f}\;\Varid{poly}\mathrel{=}\Varid{go}\;\Varid{f}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu])\;\Varid{poly}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\mathrm{0}\;\Varid{y}\;\anonymous {}\<[23]%
\>[23]{}\mathrel{=}{}\<[23E]%
\>[26]{}\Varid{y}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\mathrm{1}\;\Varid{y}\;\Varid{x}{}\<[23]%
\>[23]{}\mathrel{=}{}\<[23E]%
\>[26]{}\Varid{mul}\;\Varid{y}\;\Varid{x}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{n}\;\Varid{y}\;\Varid{x}{}\<[23]%
\>[23]{}\mid {}\<[23E]%
\>[26]{}\Varid{even}\;\Varid{n}{}\<[37]%
\>[37]{}\mathrel{=}\Varid{go}\;(\Varid{n}\mathbin{\Varid{`div`}}\mathrm{2})\;\Varid{y}\;{}\<[59]%
\>[59]{}(\Varid{mul}\;\Varid{x}\;\Varid{x}){}\<[E]%
\\
\>[23]{}\mid {}\<[23E]%
\>[26]{}\Varid{otherwise}{}\<[37]%
\>[37]{}\mathrel{=}\Varid{go}\;((\Varid{n}\mathbin{-}\mathrm{1})\mathbin{\Varid{`div`}}\mathrm{2})\;{}\<[59]%
\>[59]{}(\Varid{mul}\;\Varid{y}\;\Varid{x})\;{}\<[E]%
\\
\>[59]{}(\Varid{mul}\;\Varid{x}\;\Varid{x}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function \ensuremath{\Varid{powp}} receives a natural number,
that is the exponent, and a polynomial.
We kick off by calling \ensuremath{\Varid{go}} with the exponent $f$,
a base polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]}, \ie\ unity, and the polynomial
we want to raise to the power of \ensuremath{\Varid{f}}.
If $f=0$, we are done and return the base polynomial.
This reflects the case $x^0=1$.
If $f=1$, we multiply the base polynomial by the input polynomial.
It we have called \ensuremath{\Varid{powp}} with one, this has no effect,
since the base polynomial, in this case, is unity.

Otherwise, if the exponent is even,
we halve it, pass the base polynomial on and square the input.
Otherwise, if the exponent is odd,
we subtract one form the exponent and half the result
and pass the product of the base polynomial and the input
on instead of the base polynomial as it is and,
of course, still square the input.

This implementation differs a bit from the implementation
we presented before for numbers, but it implements the same
algorithm.

Here is a simple example: we raise the polynomial
$x + 1$ to the power of 5. In the first round, we compute

\ensuremath{\Varid{go}\;\mathrm{5}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])},

which, since 5 is odd, results in 

\ensuremath{\Varid{go}\;\mathrm{2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu])}.

This, in its turn, results in

\ensuremath{\Varid{go}\;\mathrm{1}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu])}.

This is the final step and results in 

\ensuremath{\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu])}, 

which is

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10},\mathrm{5},\mathrm{1}\mskip1.5mu]},

the polynomial $x^5 + 5x^4 + 10x^3 + 10x^2 + 5x + 1$.

You might have noticed that the different
states of the algorithm given in our Haskell notation
shows the binomial coefficients $\binom{n}{k}$ for
$n=1$, $n=2$, $n=4$ and $n=5$.
We never see $n=3$, which would be 
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu]}, because we leave the multiplication
\ensuremath{\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu])} out.
For this specific case with exponent 5,
leaving out this step is where square-and-multiply
is more efficient than multiplying five times.
With growing exponents, the saving quickly grows
to a significant order.

Division is, as usual, still more complicated than multiplication.
But it is not too different from number division. First,
we define polynomial division as Euclidean division, that is
we search the solution for the equation

\begin{equation}
\frac{a}{b} = q + r
\end{equation}

where $r < b$ and $bq+r=a$.

The manual process is as follows:
we divide the first term of $a$ by the first term of $b$.
The quotient goes to the result; then we multiply it by $b$
and set $a$ to $a$ minus that result.
Now we repeat the process
until the degree of $a$
is less than that of $b$.

Here is an example:

\[
\frac{4x^5 - x^4 + 2x^3 + x^2 - 1}{x^2 + 1}.
\]

We start by dividing $4x^5$ by $x^2$.
The quotient is $4x^3$, which we add to the result.
We multiply: $4x^3 \times (x^2 + 1) = 4x^5 + 4x^3$
and subtract the result from $a$:

\begin{equation}
\begin{array}{crcrcrcrcr}
  & 4x^5 & - &  x^4 & + & 2x^3 & + & x^2 & - & 1\\
- & 4x^5 &   &      & + & 4x^3 &   &     &   &  \\
= &      & - &  x^4 & - & 2x^3 & + & x^2 & - & 1
\end{array}
\end{equation}

We continue with
$-x^4$ and divide it by $x^2$, which is
$-x^2$. 
The overall result now is $4x^3 - x^2$.
We multiply $-x^2 \times (x^2 + 1) = -x^4 - x^2$
and subtract that from what remains from $a$:

\begin{equation}
\begin{array}{ccrcrcrcr}
  & - &  x^4 & - & 2x^3 & + &  x^2 & - & 1\\
- & - &  x^4 &   &      & - &  x^2 &   &  \\
= &   &      & - & 2x^3 & + & 2x^2 & - & 1
\end{array}
\end{equation}

We continue with $-2x^3$, which, divided by
$x^2$ is $-2x$. This goes to the result:
$4x^3 - x^2 - 2x$.
We multiply $-2x \times (x^2 + 1) = -2x^3 - 2x$
and subtract:

\begin{equation}
\begin{array}{ccrcrcrcr}
  & - & 2x^3 & + & 2x^2 & + &    & - & 1\\
- & - & 2x^3 &   &      & - & 2x &   &  \\
= &   &      &   & 2x^2 & + & 2x & - & 1 
\end{array}
\end{equation}

We continue with $2x^2$, which,
divided by $x^2$ is 2. 
We multiply $2\times (x^2 + 1) = 2x^2 + 2$
and subtract:

\begin{equation}
\begin{array}{ccrcrcrcr}
  & 2x^2 & + & 2x & - & 1\\
- & 2x^2 &   &    & + & 2\\
= &      &   & 2x & - & 3 
\end{array}
\end{equation}

The result now is
$4x^3 - x^2 - 2x + 2$.
We finally have $2x - 3$,
which is smaller in degree than $b$.
The result, hence, is
$(4x^3 - x^2 - 2x + 2, 2x - 3)$.

Here is an implementation of division in Haskell:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{28}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}c<{\hspost}@{}}%
\column{32E}{@{}l@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}c<{\hspost}@{}}%
\column{43E}{@{}l@{}}%
\column{46}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{divp}\mathbin{::}{}\<[12]%
\>[12]{}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Ord}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[12]{}\Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to (\Conid{Poly}\;\Varid{a},\Conid{Poly}\;\Varid{a}){}\<[E]%
\\
\>[3]{}\Varid{divp}\;(\Conid{P}\;\Varid{as})\;(\Conid{P}\;\Varid{bs})\mathrel{=}\mathbf{let}\;(\Varid{q},\Varid{r})\mathrel{=}\Varid{go}\;[\mskip1.5mu \mskip1.5mu]\;\Varid{as}\;\mathbf{in}\;(\Conid{P}\;\Varid{q},\Conid{P}\;\Varid{r}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{db}\mathrel{=}\Varid{degree}\;(\Conid{P}\;\Varid{bs}){}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{q}\;\Varid{r}{}\<[20]%
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{degree}\;(\Conid{P}\;\Varid{r})\mathbin{<}\Varid{db}{}\<[43]%
\>[43]{}\mathrel{=}{}\<[43E]%
\>[46]{}(\Varid{q},\Varid{r}){}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{null}\;\Varid{r}\mathrel{\vee}\Varid{r}\equiv [\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[43]%
\>[43]{}\mathrel{=}{}\<[43E]%
\>[46]{}(\Varid{q},\Varid{r}){}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{otherwise}{}\<[43]%
\>[43]{}\mathrel{=}{}\<[43E]%
\\
\>[23]{}\mathbf{let}\;{}\<[28]%
\>[28]{}\Varid{t}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{last}\;\Varid{r}\mathbin{/}\Varid{last}\;\Varid{bs}{}\<[E]%
\\
\>[28]{}\Varid{d}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{degree}\;(\Conid{P}\;\Varid{r})\mathbin{-}\Varid{db}{}\<[E]%
\\
\>[28]{}\Varid{ts}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{zeros}\;\Varid{d}\plus [\mskip1.5mu \Varid{t}\mskip1.5mu]{}\<[E]%
\\
\>[28]{}\Varid{m}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{mulist}\;\Varid{ts}\;\Varid{bs}{}\<[E]%
\\
\>[23]{}\mathbf{in}\;\Varid{go}\;{}\<[30]%
\>[30]{}(\Varid{cleanz}\mathbin{\$}\Varid{strichlist}\;(\mathbin{+})\;\Varid{q}\;\Varid{ts})\;{}\<[E]%
\\
\>[30]{}(\Varid{cleanz}\mathbin{\$}\Varid{strichlist}\;(\mathbin{-})\;\Varid{r}\;\Varid{m}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{mulist}\mathbin{::}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{mulist}\;\Varid{c1}\;\Varid{c2}\mathrel{=}\Varid{coeffs}\mathbin{\$}\Varid{mul}\;(\Conid{P}\;\Varid{c1})\;(\Conid{P}\;\Varid{c2}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

First note that division expects its arguments
to be polynomials over a \ensuremath{\Conid{Fractional}} data type.
We do not allow polynomials over integers to be used
with this implementation. The reason is that we do not
want to use Euclidean division on the coefficients.
That could indeed be very confusing. Furthermore,
polynomials are most often used with rational or real
coefficients. Restricting division to integers
(using Euclidean division) would, therefore, not make
much sense.

Observe further that we call \ensuremath{\Varid{go}} with an empty set --
that is the initial value of $q$, \ie\ the final result --
and $as$ -- that is initially the number to be divided,
the number we called $a$ above.
The function \ensuremath{\Varid{go}} has two base cases:
if the degree of $r$, the remainder and initially $as$,
is less than the degree of the divisor $b$, we are done.
The result is our current $(q,r)$. 
The same is true if $r$ is \ensuremath{\Varid{null}} or 
contains only the constant 0.
In this case, there is no remainder: $b$ divides $a$.

Otherwise, we divide the \ensuremath{\Varid{last}} of $r$ by the \ensuremath{\Varid{last}} of $b$.
Note that those are the terms with the highest degree
in each polynomial.
This division is just a number division of the two
coefficients. We still have to compute the new exponent,
which is the exponent of \ensuremath{\Varid{last}\;\Varid{r}} minus the exponent of 
\ensuremath{\Varid{last}\;\Varid{b}}, \ie\ their weight. We do this by subtracting
their degrees and then inserting zeros 
at the head of the result \ensuremath{\Varid{ts}}.
This result, \ensuremath{\Varid{ts}}, is then added to $q$.
We further compute $ts \times bs$ and subtract
the result from $r$. The function \ensuremath{\Varid{mulist}} we use for this purpose
is just a wrapper around \ensuremath{\Varid{mul}} using
lists of coefficients instead of \ensuremath{\Conid{Poly}} variables.
With the resulting $(q,r)$, we go into the next round.

Let us try this with our example from above: 

\[
\frac{4x^5 - x^4 + 2x^3 + x^2 - 1}{x^2 + 1}.
\]

We call \ensuremath{\Varid{divp}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{0},\mathrm{1},\mathrm{2},\mathbin{-}\mathrm{1},\mathrm{4}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu])} and get
\ensuremath{(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathbin{-}\mathrm{2},\mathbin{-}\mathrm{1},\mathrm{4}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{3},\mathrm{2}\mskip1.5mu])}, which translates to the polynomials
$4x^3-x^2-2x+2$ and $2x - 3$. 
This is the same result we obtained above 
with the manual procedure.

\ignore{
consider to go through the whole example
}

From here on, we can implement functions based on division,
such as \ensuremath{\Varid{divides}}:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}c<{\hspost}@{}}%
\column{29E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{divides}\mathbin{::}{}\<[15]%
\>[15]{}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Ord}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[15]{}\Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Bool}{}\<[E]%
\\
\>[3]{}\Varid{divides}\;\Varid{a}\;\Varid{b}\mathrel{=}{}\<[18]%
\>[18]{}\mathbf{case}\;\Varid{b}\mathbin{`\Varid{divp}`}{}\<[33]%
\>[33]{}\Varid{a}\;\mathbf{of}{}\<[E]%
\\
\>[18]{}(\anonymous ,\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]){}\<[29]%
\>[29]{}\to {}\<[29E]%
\>[33]{}\Conid{True}{}\<[E]%
\\
\>[18]{}\anonymous {}\<[29]%
\>[29]{}\to {}\<[29E]%
\>[33]{}\Conid{False}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

the remainder:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{remp}\mathbin{::}{}\<[12]%
\>[12]{}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Ord}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[12]{}\Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{remp}\;\Varid{a}\;\Varid{b}\mathrel{=}{}\<[15]%
\>[15]{}\mathbf{let}\;(\anonymous ,\Varid{r})\mathrel{=}\Varid{b}\mathbin{`\Varid{divp}`}\Varid{a}\;\mathbf{in}\;\Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

and, of course, the \acronym{gcd}:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}c<{\hspost}@{}}%
\column{13E}{@{}l@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{gcdp}\mathbin{::}{}\<[12]%
\>[12]{}(\Conid{Show}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Ord}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[12]{}\Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{gcdp}\;\Varid{a}\;\Varid{b}{}\<[13]%
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{degree}\;\Varid{b}\mathbin{>}\Varid{degree}\;\Varid{a}\mathrel{=}\Varid{gcdp}\;\Varid{b}\;\Varid{a}{}\<[E]%
\\
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{zerop}\;\Varid{b}{}\<[27]%
\>[27]{}\mathrel{=}\Varid{a}{}\<[E]%
\\
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{otherwise}{}\<[27]%
\>[27]{}\mathrel{=}\mathbf{let}\;(\anonymous ,\Varid{r})\mathrel{=}\Varid{divp}\;\Varid{a}\;\Varid{b}\;\mathbf{in}\;\Varid{gcdp}\;\Varid{b}\;\Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We use a simple function to check whether
a polynomial is zero:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{zerop}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Bool}{}\<[E]%
\\
\>[3]{}\Varid{zerop}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]){}\<[18]%
\>[18]{}\mathrel{=}\Conid{True}{}\<[E]%
\\
\>[3]{}\Varid{zerpo}\;\anonymous {}\<[18]%
\>[18]{}\mathrel{=}\Conid{False}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We can demonstrate \ensuremath{\Varid{gcdp}} nicely on binomial coefficients.
For instance, the \acronym{gcd} of the polynomials
$x^5 + 5x^4 + 10x^3 + 10x^2 + 5x + 1$ and
$x^3 + 3x^2 + 3x + 1$, thus

\ensuremath{\Varid{gcdp}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10},\mathrm{5},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu])}

is $x^3 + 3x^2 + 3x + 1$.

Since polynomials consisting of binomial coefficients of $n$,
where $n$ is the degree of the polynomial,
are always a product
of polynomials composed of smaller binomial coefficients,
the \acronym{gcd} of two polynomials
consisting only of binomial coefficients,
is always the smaller of the two.
In other cases, that is, when the smaller does not divide
the greater, this implementation of the \acronym{gcd}
can lead to confusing results. For instance,
we multiply \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]} by another polynomial, say,
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]}. The result is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{8},\mathrm{8},\mathrm{3}\mskip1.5mu]}. Now,

\ensuremath{\Varid{gcdp}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10},\mathrm{5},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{8},\mathrm{8},\mathrm{3}\mskip1.5mu])}

does not yield the expected result \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]},
but polynomials with fractions as coefficients.
The reason is that the \acronym{gcd} is an operation
defined on integers, but we implemented it on top
of fractionals. That is not what we want.
In fact, we confuse concepts: the \acronym{gcd} is
a concept defined on integral numbers, not on fractions.

And this is the prompt to 
turn our attention to polynomial arithmetic
over a finite field and, thus, to modular polynomial arithmetic.
With modular arithmetic, all coefficients in the polynomial
are modulo $n$. That means we have to reduce those numbers.
This, of course, does only make sense with integers.
We first implement some helpers to reduce numbers modulo $n$
reusing functions implemented in the previous chapter.

The first function takes an integer modulo $n$:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}c<{\hspost}@{}}%
\column{13E}{@{}l@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}c<{\hspost}@{}}%
\column{35E}{@{}l@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mmod}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{mmod}\;\Varid{n}\;\Varid{p}{}\<[13]%
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{n}\mathbin{<}\mathrm{0}\mathrel{\wedge}(\mathbin{-}\Varid{n})\mathbin{>}\Varid{p}{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{mmod}\;(\mathbin{-}(\Varid{mmod}\;(\mathbin{-}\Varid{n}))\;\Varid{p})\;\Varid{p}{}\<[E]%
\\
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{n}\mathbin{<}\mathrm{0}{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{mmod}\;(\Varid{p}\mathbin{+}\Varid{n})\;\Varid{p}{}\<[E]%
\\
\>[13]{}\mid {}\<[13E]%
\>[16]{}\Varid{otherwise}{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{n}\mathbin{\Varid{`rem`}}\Varid{p}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Equipped with this function, we can easily implement multiplication:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{modmul}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{modmul}\;\Varid{p}\;\Varid{f1}\;\Varid{f2}\mathrel{=}(\Varid{f1}\mathbin{*}\Varid{f2})\mathbin{`\Varid{mmod}`}\Varid{p}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

For division, we reuse the \ensuremath{\Varid{inverse}} function:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{41}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{modiv}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{modiv}\;\Varid{p}\;\Varid{n}\;\Varid{d}\mathrel{=}\Varid{modmul}\;\Varid{p}\;\Varid{n}\;\Varid{d'}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{d'}\mathrel{=}\Varid{fromIntegral}\;(\Varid{\Conid{M}.inverse}\;{}\<[41]%
\>[41]{}(\Varid{fromIntegral}\;\Varid{d})\;{}\<[E]%
\\
\>[41]{}(\Varid{fromIntegral}\;\Varid{p})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Now, we turn to polynomials. Here is, first, a function
that transforms a polynomial into one modulo $n$:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{pmod}\mathbin{::}\Conid{Poly}\;\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{pmod}\;(\Conid{P}\;\Varid{cs})\;\Varid{p}\mathrel{=}\Conid{P}\;[\mskip1.5mu \Varid{c}\mathbin{`\Varid{mmod}`}\Varid{p}\mid \Varid{c}\leftarrow \Varid{cs}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

In other words, we just map \ensuremath{\Varid{mmod}} on all coefficients.
Let us look at some polynomials modulo a number, say, 7.
The polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]}
we already used above is just the same modulo 7.
The polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu]}, however, changes:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu]\mathbin{`\Varid{pmod}`}\mathrm{7}}

is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{0},\mathrm{1}\mskip1.5mu]} or, in other words,
$8x^3 + 7x^2 + 6x + 5$ turns, modulo 7, into 
$x^3 + 6x + 5$.

The polynomial $x + 1$ raised to the power of 5 is
$x^5 + 5x^4 + 10x^3 + 10x^2 + 5x + 1$. Modulo 7, this
reduces to $x^5 + 5x^4 + 3x^3 + 3x^3 + 5x + 1$.
That is: the binomial coefficients modulo $n$ change.
For instance,

\ensuremath{\Varid{map}\;(\Varid{choose2}\;\mathrm{6})\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{6}\mskip1.5mu]}

is

1,6,15,20,15,6,1.

Modulo 7, we get

1,6,1,6,1,6,1.

\ensuremath{\Varid{map}\;(\Varid{choose2}\;\mathrm{7})\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{7}\mskip1.5mu]}

is

1,7,21,35,35,21,7,1.

Without big surprise, we see this modulo 7
drastically simplified:

1,0,0,0,0,0,0,1.

Here are addition and subtraction, which are very easy
to convert to modular arithmetic:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{addmp}\mathbin{::}\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{addmp}\;\Varid{n}\;\Varid{p1}\;\Varid{p2}\mathrel{=}\Varid{strich}\;(\mathbin{+})\;\Varid{p1}\;\Varid{p2}\mathbin{`\Varid{pmod}`}\Varid{n}{}\<[E]%
\\
\>[3]{}\Varid{submp}\mathbin{::}\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{submp}\;\Varid{n}\;\Varid{p1}\;\Varid{p2}\mathrel{=}\Varid{strich}\;(\mathbin{-})\;\Varid{p1}\;\Varid{p2}\mathbin{`\Varid{pmod}`}\Varid{n}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Multiplication:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{18}{@{}>{\hspre}c<{\hspost}@{}}%
\column{18E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}c<{\hspost}@{}}%
\column{32E}{@{}l@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mulmp}\mathbin{::}\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{mulmp}\;\Varid{p}\;\Varid{p1}\;\Varid{p2}{}\<[18]%
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{d2}\mathbin{>}\Varid{d1}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{mulmp}\;\Varid{p}\;\Varid{p2}\;\Varid{p1}{}\<[E]%
\\
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{otherwise}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Conid{P}\;[\mskip1.5mu \Varid{m}\mathbin{`\Varid{mmod}`}\Varid{p}\mid \Varid{m}\leftarrow \Varid{strichf}\;(\mathbin{+})\;\Varid{ms}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{ms}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}[\mskip1.5mu \Varid{mul1}\;\Varid{o}\;\Varid{i}\;(\Varid{coeffs}\;\Varid{p1})\;\Varid{c}\mid (\Varid{i},\Varid{c})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;(\Varid{coeffs}\;\Varid{p2})\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{d1}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{degree}\;\Varid{p1}{}\<[E]%
\\
\>[12]{}\Varid{d2}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{degree}\;\Varid{p2}{}\<[E]%
\\
\>[12]{}\Varid{o}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{modmul}\;\Varid{p}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

and product:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mulmlist}\mathbin{::}\Conid{Zahl}\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{mulmlist}\;\Varid{p}\;\Varid{c1}\;\Varid{c2}\mathrel{=}\Varid{coeffs}\mathbin{\$}\Varid{mulmp}\;\Varid{p}\;(\Conid{P}\;\Varid{c1})\;(\Conid{P}\;\Varid{c2}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We repeat the multiplication from above 

\ensuremath{\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu])} 

which was

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{16},\mathrm{34},\mathrm{60},\mathrm{61},\mathrm{52},\mathrm{32}\mskip1.5mu]}

Modulo 7, this result is

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{2},\mathrm{6},\mathrm{4},\mathrm{5},\mathrm{3},\mathrm{4}\mskip1.5mu]}.

The modulo multiplication

\ensuremath{\Varid{mulmp}\;\mathrm{7}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{0},\mathrm{1}\mskip1.5mu])}

yields the same result:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{2},\mathrm{6},\mathrm{4},\mathrm{5},\mathrm{3},\mathrm{4}\mskip1.5mu]}

Division:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{28}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}c<{\hspost}@{}}%
\column{32E}{@{}l@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{divmp}\mathbin{::}\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to (\Conid{Poly}\;\Conid{Zahl},\Conid{Poly}\;\Conid{Zahl}){}\<[E]%
\\
\>[3]{}\Varid{divmp}\;\Varid{p}\;(\Conid{P}\;\Varid{as})\;(\Conid{P}\;\Varid{bs})\mathrel{=}\mathbf{let}\;(\Varid{q},\Varid{r})\mathrel{=}\Varid{go}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]\;\Varid{as}\;\mathbf{in}\;(\Conid{P}\;\Varid{q},\Conid{P}\;\Varid{r}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{db}\mathrel{=}\Varid{degree}\;(\Conid{P}\;\Varid{bs}){}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{q}\;\Varid{r}{}\<[20]%
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{degree}\;(\Conid{P}\;\Varid{r})\mathbin{<}\Varid{db}{}\<[43]%
\>[43]{}\mathrel{=}(\Varid{q},\Varid{r}){}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{null}\;\Varid{r}\mathrel{\vee}\Varid{r}\equiv [\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[43]%
\>[43]{}\mathrel{=}(\Varid{q},\Varid{r}){}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{otherwise}{}\<[43]%
\>[43]{}\mathrel{=}{}\<[E]%
\\
\>[23]{}\mathbf{let}\;{}\<[28]%
\>[28]{}\Varid{t}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{modiv}\;\Varid{p}\;(\Varid{last}\;\Varid{r})\;(\Varid{last}\;\Varid{bs}){}\<[E]%
\\
\>[28]{}\Varid{d}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{degree}\;(\Conid{P}\;\Varid{r})\mathbin{-}\Varid{db}{}\<[E]%
\\
\>[28]{}\Varid{ts}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{zeros}\;\Varid{d}\plus [\mskip1.5mu \Varid{t}\mskip1.5mu]{}\<[E]%
\\
\>[28]{}\Varid{m}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{mulmlist}\;\Varid{p}\;\Varid{ts}\;\Varid{bs}{}\<[E]%
\\
\>[23]{}\mathbf{in}\;\Varid{go}\;{}\<[30]%
\>[30]{}(\Varid{cleanz}\;[\mskip1.5mu \Varid{c}\mathbin{`\Varid{mmod}`}\Varid{p}\mid \Varid{c}\leftarrow \Varid{strichlist}\;(\mathbin{+})\;\Varid{q}\;\Varid{ts}\mskip1.5mu])\;{}\<[E]%
\\
\>[30]{}(\Varid{cleanz}\;[\mskip1.5mu \Varid{c}\mathbin{`\Varid{mmod}`}\Varid{p}\mid \Varid{c}\leftarrow \Varid{strichlist}\;(\mathbin{-})\;\Varid{r}\;\Varid{m}\mskip1.5mu]){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Division works exactly like the variant for infinite fields,
except that we now use multiplication with the modulo inverse 
instead of fractional division.

Here is the \acronym{gcd}:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{gcdmp}\mathbin{::}\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{gcdmp}\;\Varid{p}\;\Varid{a}\;\Varid{b}{}\<[16]%
\>[16]{}\mid {}\<[16E]%
\>[19]{}\Varid{degree}\;\Varid{b}\mathbin{>}\Varid{degree}\;\Varid{a}\mathrel{=}\Varid{gcdmp}\;\Varid{p}\;\Varid{b}\;\Varid{a}{}\<[E]%
\\
\>[16]{}\mid {}\<[16E]%
\>[19]{}\Varid{zerop}\;\Varid{b}\mathrel{=}\Varid{a}{}\<[E]%
\\
\>[16]{}\mid {}\<[16E]%
\>[19]{}\Varid{otherwise}\mathrel{=}\mathbf{let}\;(\anonymous ,\Varid{r})\mathrel{=}\Varid{divmp}\;\Varid{p}\;\Varid{a}\;\Varid{b}\;\mathbf{in}\;\Varid{gcdmp}\;\Varid{p}\;\Varid{b}\;\Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Let us try \ensuremath{\Varid{gcdmp}} on the variation we already tested above. 
We multiply the polynomial
$x^2 + 2x + 1$ by $3x^2 + 2x + 1$ modulo 7:

\ensuremath{\Varid{mulmp}\;\mathrm{7}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu])}.

The result is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{1},\mathrm{1},\mathrm{3}\mskip1.5mu]}.

Now, we compute the \acronym{gcd} with \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10},\mathrm{5},\mathrm{1}\mskip1.5mu]} modulo 7:

\ensuremath{\Varid{gcdmp}\;\mathrm{7}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{3},\mathrm{3},\mathrm{5},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{1},\mathrm{1},\mathrm{3}\mskip1.5mu])}.

The result is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]}, as expected.

Finally, power:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}c<{\hspost}@{}}%
\column{35E}{@{}l@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{58}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{powmp}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}\to \Conid{Poly}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{powmp}\;\Varid{p}\;\Varid{f}\;\Varid{poly}\mathrel{=}\Varid{go}\;\Varid{f}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu])\;\Varid{poly}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\mathrm{0}\;\Varid{y}\;\anonymous {}\<[22]%
\>[22]{}\mathrel{=}\Varid{y}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\mathrm{1}\;\Varid{y}\;\Varid{x}{}\<[22]%
\>[22]{}\mathrel{=}\Varid{mulmp}\;\Varid{p}\;\Varid{y}\;\Varid{x}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{n}\;\Varid{y}\;\Varid{x}{}\<[22]%
\>[22]{}\mid \Varid{even}\;\Varid{n}{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{go}\;(\Varid{n}\mathbin{\Varid{`div`}}\mathrm{2})\;\Varid{y}\;{}\<[58]%
\>[58]{}(\Varid{mulmp}\;\Varid{p}\;\Varid{x}\;\Varid{x}){}\<[E]%
\\
\>[22]{}\mid \Varid{otherwise}{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{go}\;((\Varid{n}\mathbin{-}\mathrm{1})\mathbin{\Varid{`div`}}\mathrm{2})\;{}\<[58]%
\>[58]{}(\Varid{mulmp}\;\Varid{p}\;\Varid{y}\;\Varid{x})\;{}\<[E]%
\\
\>[58]{}(\Varid{mulmp}\;\Varid{p}\;\Varid{x}\;\Varid{x}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Here is a nice variant of Pascal's triangle generated by\\
\ensuremath{\Varid{map}\;(\lambda \Varid{x}\to \Varid{powmp}\;\mathrm{7}\;\Varid{x}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]))\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{14}\mskip1.5mu]}:

\begin{minipage}{\textwidth}
\begin{center}
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{3},\mathrm{3},\mathrm{5},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{3},\mathrm{3},\mathrm{5},\mathrm{1},\mathrm{0},\mathrm{1},\mathrm{5},\mathrm{3},\mathrm{3},\mathrm{5},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1},\mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1},\mathrm{6},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{2},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}
\end{center}
\end{minipage}

Before we continue with modular arithmetic,
which we need indeed to understand some of the deeper problems
related to polynomials, we will
investigate the application of polynomials using a famous device: 
Babbage's difference engine.
\section{The Difference Engine}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{PolyArith}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Zahl}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Quoz}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Real}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{NumSystem}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{PolyArith}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Polynomial arithmetic, as we have seen,
is very similar to number arithmetic.
What is the correspondent of interpreting
a number in a given numeral system
in the domain of polynomials?
Well, that is the \term{application} of the polynomial
to a given number. We would substitute $x$
for a number in the Field in which we are working
and just compute the formula.
For instance, the polynomial

\[
x^2 + x + 1
\]

can be applied to, say, 2.
Then we get the formula

\[
2^2 + 2 + 1,
\]

which is $4 + 2 + 1 = 7$.

For other values of $x$, it would of course
generate other values. For $x=0$, for instance,
it would give $0^2 + 0 + 1 = 1$; for $x=1$,
it is $1^2 + 1 + 1 = 3$; for $x=3$, it yields
$3^2 + 3 + 1 = 13$.

How would we apply a polynomial represented
by our Haskell type? We would need to go through the list
of coefficients, raise $x$ to the power of the weight
of each particular coefficient, multiply it by the coefficient
and, finally, add all the values together.
Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{apply}\mathbin{::}\Conid{Num}\;\Varid{a}{}\<[21]%
\>[21]{}\Rightarrow \Conid{Poly}\;\Varid{a}\to \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{apply}\;(\Conid{P}\;\Varid{cs})\;\Varid{x}{}\<[21]%
\>[21]{}\mathrel{=}\Varid{sum}\;[\mskip1.5mu \Varid{c}\mathbin{*}\Varid{x}\mathbin{\uparrow}\Varid{i}\mid (\Varid{i},\Varid{c})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;\Varid{cs}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Let us try with a very simple polynomial, $x + 1$:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{apply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;\mathrm{0}} gives 1.\\
\ensuremath{\Varid{apply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;\mathrm{1}} gives 2.\\
\ensuremath{\Varid{apply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;\mathrm{2}} gives 3.\\
\ensuremath{\Varid{apply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;\mathrm{3}} gives 4.
\end{minipage}

This polynomial, apparently, just counts the integers
adding one to the value to which we apply it.
It implements {\texttt i++}.

On the first sight, this result appears to be boring.
However, after a quick thought, there is a lesson to learn:
we get to know the polynomial, when we look
at the \term{sequence} it produces. So, let us implement
a function that maps \ensuremath{\Varid{apply}} to lists of numbers:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{mapply}\mathbin{::}\Conid{Num}\;\Varid{a}{}\<[22]%
\>[22]{}\Rightarrow \Conid{Poly}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{mapply}\;\Varid{p}\mathrel{=}\Varid{map}\;(\Varid{apply}\;\Varid{p}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

For simple polynomials, the sequences are predictable.
$x^2$, obviously, just produces the squares;
$x^3$ produces the cubes and so on.
Sequences created by powers of the simple polynomial $x+1$,
like $(x+1)^2$, $(x+1)^3$ and so on,
still, are quite predictable, \eg\:

\begin{minipage}{\textwidth}
\begin{tabular}{lp{7cm}}
\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}: & 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121\\
\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}: & 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331\\
\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}: & 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561, 10000, 14641\\
\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10},\mathrm{5},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}:
 & 1, 32, 243, 1024, 3125, 7776, 16807, 32768, 59049, 100000, 161051\\
\end{tabular}
\end{minipage}

The first line, easy to recognise, is the squares, but pushed one up,
\ie\ the application to 0 yields the value for $1^2$, 
the application to 1 yields the value for $2^2$ and so on.
The second, still easy to recognise,
is the cubes -- again pushed up by one.
The third line is the powers of four 
and the fourth line is the powers of five,
both pushed up by one.

That is not too surprising at the end, since
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]} is the result of squaring \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]}, 
which generates the integers pushed one up;
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu]} is the result of raising \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]} to the third power
and so on.

Things become more interesting, when we deviate
from binomial coefficients. The sequence
produced by \ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu])\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}, for instance,
does not resemble such a simple pattern:

1, 10, 49, 142, 313, 586, 985, 1534, 2257, 3178, 4321.

Even the Online Encyclopedia has nothing interesting
to say about it.

The same is true for \ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{6},\mathrm{7},\mathrm{8}\mskip1.5mu])\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]},
which is 

5, 26, 109, 302, 653, 1210, 2021, 3134, 4597, 6458, 8765.

This raises another interesting question:
given a sequence, is there a method by which 
we can identify the polynomial that created it?
Yes, there is. In fact, there are.
There was even a machine that helped guessing
polynomials from sequences. It was built in the early
$19^{th}$ century by Charles Babbage (1791 -- 1871),
an English polymath, mathematician, philosopher,
economist and inventor.

Babbage stands in the tradition of designers and constructors
of early computing machinery; predecessors of his
in this tradition were, for instance, 
Blaise Pascal (1623 -- 1662) and
Gottfried Wilhelm Leibniz (1646 -- 1716).
Babbage designed two series of machines,
first, the difference engines and, later, 
the analytical engines.

The analytical engine, unfortunately, was not built in his lifetime.
The final collapse of the project came 
in 1878, after Babbage's death in 1871, 
due to lack of finance. 
The analytical engine would have been 
a universal (Turing-complete) computer
very similar to our computers today,
but not working on electricity, but on steam and brawn.
It would have been programmed by punch cards that,
in Babbage's time, were used for controlling looms.
Programs would have resembled modern assembly languages
allowing control structures like selection and iteration.
In the context of a description of the analytical engine,
Ada Lovelace (1815 -- 1852), 
a friend of Babbage and daughter of Lord Byron,
described how to compute Bernoulli numbers with the machine.
She is, therefore, considered the first computer programmer
in history.

The difference engine, at which we will look here,
is much simpler. It was designed to analyse polynomials
and what it did was, according to Babbage, ``computing differences''.
During Babbage's lifetime, a first version was built and
sucessfully demonstrated. The construction
of a second, much more powerful version
which was financially backed by the government,
failed due to disputes between Babbage and his engineers.
This machine was finally built by the London Science Museum
in 1991 using material and engineering techniques available
in the $19^{th}$ century proving this way
that it was actually possible for Babbage and his engineers
to build such a machine.

The difference engine, as Babbage put it, computes differences,
namely the differences in a sequence of numbers.
It would take as input a sequence of the form

0,1,16,81,256,625,1296,2401,4096,6561,10000

and compute the differences between the individual numbers:

\begin{equation}
\begin{array}{rcrcr}
  1 & - &  0 & = &   1 \\
 16 & - &  1 & = &  15 \\
 81 & - & 16 & = &  65 \\
256 & - & 81 & = & 175\\
\dots
\end{array}
\end{equation}

Here is a simple function that does this job for us:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{diffs}\mathbin{::}[\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{diffs}\;[\mskip1.5mu \mskip1.5mu]{}\<[19]%
\>[19]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{diffs}\;[\mskip1.5mu \anonymous \mskip1.5mu]{}\<[19]%
\>[19]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{diffs}\;(\Varid{a}\mathbin{:}\Varid{b}\mathbin{:}\Varid{cs}){}\<[19]%
\>[19]{}\mathrel{=}(\Varid{b}\mathbin{-}\Varid{a})\mathbin{:}\Varid{diffs}\;(\Varid{b}\mathbin{:}\Varid{cs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Applied on the sequence above, \ensuremath{\Varid{diffs}} yields:

1,15,65,175,369,671,1105,1695,2465,3439

What is so special about it?
Perhaps, nothing. But let us repeat the process
using this result. The repetition yields:

14,50,110,194,302,434,590,770,974

One more time:

36,60,84,108,132,156,180,204

And once again:

24,24,24,24,24,24,24

Suddenly, we have a constant list.
How often did we apply \ensuremath{\Varid{diffs}}?
Four times -- and, as you may have realised,
the original sequence was generated by the polynomial
$x^4$, a polynomial of degree 4.
Is that coincidence?

For further investigation, we implement
the complete difference engine, which takes differences,
until it reaches a constant sequence.

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{dengine}\mathbin{::}[\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{dengine}\;\Varid{cs}{}\<[16]%
\>[16]{}\mid {}\<[16E]%
\>[19]{}\Varid{constant}\;\Varid{cs}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[16]{}\mid {}\<[16E]%
\>[19]{}\Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{ds}\mathbin{:}\Varid{dengine}\;\Varid{ds}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{ds}\mathrel{=}\Varid{diffs}\;\Varid{cs}{}\<[E]%
\\
\>[12]{}\Varid{constant}\;[\mskip1.5mu \mskip1.5mu]{}\<[29]%
\>[29]{}\mathrel{=}\Conid{True}{}\<[E]%
\\
\>[12]{}\Varid{constant}\;(\Varid{x}\mathbin{:}\Varid{xs}){}\<[29]%
\>[29]{}\mathrel{=}\Varid{all}\;(\equiv \Varid{x})\;\Varid{xs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Note that we restrict coefficients to integers.
This is just for clarity.
Usually, polynomials are defined over a field,
such as the rational or the real numbers.

To confirm our suspicion that the difference engine
creates $n$ difference sequences for a polynomial of degree $n$,
we apply the engine on $x$, $x^2$, $x^3$, $x^4$ and $x^5$
and count the sequences it creates:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{length}\;(\Varid{dengine}\;(\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{32}\mskip1.5mu]))}: 1\\
\ensuremath{\Varid{length}\;(\Varid{dengine}\;(\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{32}\mskip1.5mu]))}: 2\\
\ensuremath{\Varid{length}\;(\Varid{dengine}\;(\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{32}\mskip1.5mu]))}: 3\\
\ensuremath{\Varid{length}\;(\Varid{dengine}\;(\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{32}\mskip1.5mu]))}: 4\\
\ensuremath{\Varid{length}\;(\Varid{dengine}\;(\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{32}\mskip1.5mu]))}: 5
\end{minipage}

The engine already has a purpose:
it tells us the degree of the polynomial
that generates a given sequence.
It can do much more, though.
For instance, it lets us predict the next value
in the sequence.
To do so, we take the constant difference 
from the last sequence and add it to 
the last difference of the previous sequence;
we take that result and add it to the previous sequence
and so on, until we reach the first sequence.
Consider the sequence and its differences from above:

\begin{minipage}{\textwidth}
0,1,16,81,256,625,1296,2401,4096,6561,10000\\
1,15,65,175,369,671,1105,1695,2465,3439\\
14,50,110,194,302,434,590,770,974\\
36,60,84,108,132,156,180,204\\
24,24,24,24,24,24,24
\end{minipage}

We start at the bottom and compute $204 + 24 = 228$.
This is the next difference of the previous sequence.
We compute $974 + 228 = 1202$. We go one line up and
compute $3439 + 1202 = 4641$. This, finally, is the difference
to the next value in the input sequence, which, hence, is
$10000 + 4641 = 14641$ and, indeed, $11^4$.
Even without knowing the polynomial that actually generates
the sequence, we are now able to continue it.
Here is a function that does that for us:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{predict}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to \Conid{Maybe}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{predict}\;\Varid{ds}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}{}\<[21]%
\>[21]{}\Conid{Nothing}{}\<[E]%
\\
\>[3]{}\Varid{predict}\;\Varid{ds}\;\Varid{xs}\mathrel{=}{}\<[21]%
\>[21]{}\mathbf{case}\;\Varid{go}\;(\Varid{reverse}\;\Varid{ds})\;\mathbf{of}{}\<[E]%
\\
\>[21]{}\mathrm{0}{}\<[24]%
\>[24]{}\to \Conid{Nothing}{}\<[E]%
\\
\>[21]{}\Varid{d}{}\<[24]%
\>[24]{}\to \Conid{Just}\;(\Varid{d}\mathbin{+}(\Varid{last}\;\Varid{xs})){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\mathrel{=}\Varid{foldl'}\;(\lambda \Varid{x}\;\Varid{c}\to \Varid{last}\;\Varid{c}\mathbin{+}\Varid{x})\;\mathrm{0}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function takes two arguments:
the first is the list of difference sequences and
the second is the original sequence.
We apply \ensuremath{\Varid{go}} on the reverse of the sequences
(because we are working backwards).
For each sequence in this list, we get the last
and add it to the last of the previous until
we have exhausted the list.
If \ensuremath{\Varid{go}} yields 0, we assume that something went wrong.
The list of sequences may have been empty in the first place.
Otherwise, we add the result to the last of the original list.

Here are some more examples:

\begin{minipage}{\textwidth}
\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{predict}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}: 11\\
\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{predict}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}: 121\\
\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{predict}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}: 1331\\
\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{predict}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}: 14641\\
\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{predict}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}: 161051
\end{minipage}

But how can we find the polynomial itself that generates the given sequence?
With the help of the difference engine, we already know
the degree of the polynomial. 
Supposed, we know that the first element in the sequence
was generated applying 0 to the unknown polynomial and
the second one was generated applying 1,
the third by applying 2 and so on,
we have all information we need.

From the degree, we know the form of the polynomial.
A polynomial of degree 1 has the form $a_1x + a_2$;
a polynomial of degree 2 has the form $a_1x^2 + a_2x + a_3$;
a polynomial of degree 3 has the form $a_1x^3 + a_2x^2 + a_3x + a_4$
and so on.

Since we know the values to which the polynomial is applied,
we can easily compute the value of the $x$-part of the terms.
They are that value raised to the power of the weight.
The challenge, then, is to find the coefficient by which
that value is multiplied.

The first element in the sequence, the one created by applying
the polynomial to 0, is just the last coefficient,
the one ``without'' any $x$, since the other terms ``disappear'',
when we apply to 0. Consider for example a polynomial of the form
$x^2 + x + a$. When we apply it to 0,
we get $0^2 + 0 + a = c$, where $c$ is the first
(or, in this notation, the last)
value in the sequence. Thus, $a=c$.

The second element is 1 applied to the formula and, therefore,
all terms equal their coefficients, since $cx^n$, for $x=1$, 
is just $c$. The third element results from applying 2 to the polynomial,
it hence adheres to a formula where unknown values (the coefficients)
are multiplied by $2$, $2^2=4$, $2^3=8$ and so on.

In other words, for a polynomial of degree $n$, we can devise
a system of linear equations with $n+1$ unknowns and
the $n+1$ first elements of the sequence as constant values.
A polynomial of degree 2, for instance, yields the system

\begin{equation}
\begin{array}{rcrcrcr}
    &   &    &   & a  & = &  a_1 \\
  a & + & b  & + & c  & = &  a_2 \\
  a & + & 2b & + & 4c & = &  a_3
\end{array}
\end{equation}

where the constant numbers $a_1$, $a_2$ and $a_3$
are the first three elements of the sequence.
A polynomial of degree 3 would generate the system

\begin{equation}
\begin{array}{rcrcrcrcr}
    &   &    &   &    &   &   a & = &  a_1 \\
  a & + &  b & + &  c & + &   d & = &  a_2 \\
  a & + & 2b & + & 4c & + &  8d & = &  a_3 \\
  a & + & 3b & + & 9c & + & 27d & = &  a_4 
\end{array}
\end{equation}

We have already learnt how to solve such systems:
we can apply Gaussian elimination.
The result of the elminiation is 
the coefficients of the generating polynomial,
which are the unknowns in the linear equations.
The known values (which we would call the coefficients
in a linear equation) are the values obtained
by computing $x^i$ where $i$ is the weight 
of the coefficient.
Here is a function to extract the known values,
the $x$es raised to the weight, from a given 
sequence with a given degree:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{genCoeff}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Zahl}\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{genCoeff}\;\Varid{d}\;\Varid{n}\;\Varid{x}\mathrel{=}\Varid{map}\;(\Varid{n}\mathbin{\uparrow})\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{d}\mskip1.5mu]\plus [\mskip1.5mu \Varid{x}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Here, $d$ is the degree of the polynomial,
$n$ is the value to which the polynomial is applied
and $x$ is the result, \ie\ the value from the sequence.
We create the sequence $n^i$, for $0 \le i \le d$ and
append $x$ yielding one line of the system
of linear equations.

When we apply \ensuremath{\Varid{genCoeff}} on the the sequence
generated by $x^4$, we would have:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{0}\;\mathrm{0}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{1}\;\mathrm{1}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{2}\;\mathrm{16}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{8},\mathrm{16},\mathrm{16}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{3}\;\mathrm{81}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{9},\mathrm{27},\mathrm{81},\mathrm{81}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{4}\;\mathrm{256}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{16},\mathrm{64},\mathrm{256},\mathrm{256}\mskip1.5mu]}
\end{minipage}

Note that the results are very regular:
we see constant 1 in the first column,
the natural numbers in the second column,
the squares in the third, the cubes in the fourth and
$n^4$ in the fifth and sixth column.
Those are just the values for $x^i$, 
for $i \in \lbrace 0\dots 4\rbrace$.
Since the value in the sixth column, the one we took
from the sequence, equals the value in the fifth column,
we can already guess that the polynomial is simply $x^4$.
Here is another sequence, generated by a secret polynomial:

14, 62, 396, 1544, 4322, 9834, 19472, 34916, 58134, 91382, 137204

We compute the difference lists using 
\ensuremath{\Varid{dengine}} as \ensuremath{\Varid{ds}} and compute the degree of the polynomial
using \ensuremath{\Varid{length}\;\Varid{ds}}. The result is 4.
Now we call \ensuremath{\Varid{genCoeff}} on the first four elements of the sequence:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{0}\;\mathrm{14}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{14}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{1}\;\mathrm{62}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{62}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{2}\;\mathrm{396}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{8},\mathrm{16},\mathrm{396}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{3}\;\mathrm{1544}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{9},\mathrm{27},\mathrm{81},\mathrm{1544}\mskip1.5mu]}\\
\ensuremath{\Varid{genCoeff}\;\mathrm{4}\;\mathrm{4}\;\mathrm{4322}} resulting in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{16},\mathrm{64},\mathrm{256},\mathrm{4322}\mskip1.5mu]}
\end{minipage}

We already see that this is a less trivial case:
the last two numbers are not equal!

Now we use \ensuremath{\Varid{genCoeff}} to create a matrix representing
the entire system of equations:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{findCoeffs}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to \Conid{\Conid{L}.Matrix}\;\Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{findCoeffs}\;\Varid{ds}\;\Varid{sq}\mathrel{=}\Conid{\Conid{L}.M}\;[\mskip1.5mu \Varid{genCoeff}\;\Varid{d}\;\Varid{n}\;\Varid{x}\mid (\Varid{n},\Varid{x})\leftarrow \Varid{zip}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{d}\mskip1.5mu]\;\Varid{sq}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{d}\mathrel{=}\Varid{fromIntegral}\;(\Varid{length}\;\Varid{ds}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function \ensuremath{\Varid{findCoeffs}} receives 
the list of difference sequences created by \ensuremath{\Varid{dengine}} and
the original sequence.
It computes the degree of the generating polynomial
as \ensuremath{\Varid{length}\;\Varid{ds}} and, then, it goes through the 
first \ensuremath{\Varid{d}} elements of the sequence calling \ensuremath{\Varid{genCoeff}}
with \ensuremath{\Varid{d}}, the known input value $n$, and $x$,
the element of the sequence.
For the sequence generated by $x^4$, we obtain
\ensuremath{\Conid{M}\;[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{8},\mathrm{16},\mathrm{16}\mskip1.5mu],}
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{9},\mathrm{27},\mathrm{81},\mathrm{81}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{16},\mathrm{64},\mathrm{256},\mathrm{256}\mskip1.5mu]\mskip1.5mu]}, 
which corresponds to the matrix

\[
\begin{pmatrix}
 1 &  0 &   0 &   0 &   0 &    0\\
 1 &  1 &   1 &   1 &   1 &    1\\
 1 &  2 &   4 &   8 &  16 &   16\\
 1 &  3 &   9 &  27 &  81 &   81\\
 1 &  4 &  16 &  64 & 256 &  256
\end{pmatrix}
\]

For the sequence of the unknown polynomial, we obtain
\ensuremath{\Conid{M}\;[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{14}\mskip1.5mu],\mid [\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{62}\mskip1.5mu],}
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{8},\mathrm{16},\mathrm{396}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{9},\mathrm{27},\mathrm{81},\mathrm{1544}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{16},\mathrm{64},\mathrm{256},\mathrm{4322}\mskip1.5mu]\mskip1.5mu]},
which corresponds to the matrix:

\[
\begin{pmatrix}
 1 &  0 &   0 &   0 &   0 &   14\\
 1 &  1 &   1 &   1 &   1 &   62\\
 1 &  2 &   4 &   8 &  16 &  396\\
 1 &  3 &   9 &  27 &  81 & 1544\\
 1 &  4 &  16 &  64 & 256 & 4322
\end{pmatrix}
\]

The next steps are simple. We create the echelon form
and solve by back-substitution. The following function
puts all the bits together to find the generating polynomial:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{findGen}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu \Conid{Quoz}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{findGen}\;\Varid{ds}\mathrel{=}\Varid{\Conid{L}.backsub}\mathbin{\circ}\Varid{\Conid{L}.echelon}\mathbin{\circ}\Varid{findCoeffs}\;\Varid{ds}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Applied on the difference list and the sequence
generated by $x^4$, \ensuremath{\Varid{findGen}} yields:

\ensuremath{[\mskip1.5mu \mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which indeed corresponds to the polynomial $x^4$. 
For the sequence generated by the unknown polynomial,
we get:

\ensuremath{[\mskip1.5mu \mathrm{14}\mathbin{\%}\mathrm{1},\mathrm{9}\mathbin{\%}\mathrm{1},\mathrm{11}\mathbin{\%}\mathrm{1},\mathrm{16}\mathbin{\%}\mathrm{1},\mathrm{12}\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which corresponds to the polynomial
$12x^4 + 16x^3 + 11x^2 + 9x + 14$.
Let us test:

\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{14},\mathrm{9},\mathrm{11},\mathrm{16},\mathrm{12}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} yields:

14,62,396,1544,4322,9834,19472,34916,58134,91382,137204,

which indeed is the same sequence as we saw above!

Now, what about the differences generated
by the difference engine? Those, too, are sequences
of numbers. Are there polynomials
that generate those sequences?
The first difference sequence of our formerly unknown polynomial is

48,334,1148,2778,5512,9638,15444,23218,33248,45822

The next three difference sequences could be derived
from this sequence -- so, we can assume that this sequence
is generated by a polynomial of degree 3. Let us see
what \ensuremath{\Varid{findGen}\;(\Varid{tail}\;\Varid{ds})\;(\Varid{head}\;\Varid{ds})} yields (with \ensuremath{\Varid{ds}}
being the list of difference sequences of that polynomial):

\ensuremath{[\mskip1.5mu \mathrm{48}\mathbin{\%}\mathrm{1},\mathrm{118}\mathbin{\%}\mathrm{1},\mathrm{120}\mathbin{\%}\mathrm{1},\mathrm{48}\mathbin{\%}\mathrm{1}\mskip1.5mu]}, 

which corresponds to the polynomial 
$48x^3 + 120x^2 + 118x + 48$.
Let us test again:

\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{48},\mathrm{118},\mathrm{120},\mathrm{48}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} yields:

48,334,1148,2778,5512,9638,15444,23218,33248,45822,61228

The next difference sequence should then be generated
by a polynomial of degree 2. We try with\\
\ensuremath{\mathbf{let}\;\Varid{ds'}\mathrel{=}\Varid{tail}\;\Varid{ds}\;\mathbf{in}\;\Varid{findGen}\;(\Varid{tail}\;\Varid{ds'})\;(\Varid{head}\;\Varid{ds'})}\\
 and get

\ensuremath{[\mskip1.5mu \mathrm{286}\mathbin{\%}\mathrm{1},\mathrm{384}\mathbin{\%}\mathrm{1},\mathrm{144}\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which corresponds to the polynomial 
$144x^2 + 384x + 286$.

\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{286},\mathrm{384},\mathrm{144}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} yields:

286,814,1630,2734,4126,5806,7774,10030,12574,15406,18526

which, indeed, is the third difference sequence.

Finally, the last but one sequence, the last
that is not constant, should be generated by a polynomial
of degree 1. We try with\\
\ensuremath{\mathbf{let}\;\Varid{ds''}\mathrel{=}\Varid{tail}\;(\Varid{tail}\;\Varid{ds})\;\mathbf{in}\;\Varid{findGen}\;(\Varid{tail}\;\Varid{ds''})\;(\Varid{head}\;\Varid{ds''})}\\
and get 

\ensuremath{\mathrm{528}\mathbin{\%}\mathrm{1},\mathrm{288}\mathbin{\%}\mathrm{1}}

which corresponds to the polynomial $288x + 528$.

\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{528},\mathrm{288}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} yields:

528,816,1104,1392,1680,1968,2256,2544,2832,3120,3408

which, again is the expected difference sequence.

The differences are closely related to the tremendously
important concept of the \term{derivative} of a function.
The derivative of a polynomial $\pi$ of degree $n$
is a polynomial $\pi'$ of degree $n-1$ that measures
the \term{rate of change} or \term{slope} of $\pi$.
The derivative expresses the rate of change precisely
for any point in $\pi$. We will look at this with
much more attention in the next section; 
the third part will then be entirely dedicated 
to derivatives and
related concepts.

The difference sequences and the polynomials that generate them
are also a measure of the rate of change.
Actually, the difference between two points \emph{is}
the rate of change of that polynomial between those two points.
The difference, however, is a sloppy measure.

Without going into too much detail here,
we can quickly look at how the derivative of a polynomial
is computed, which, in fact, is very easy.
For a polynomial of the form

\[
ax^n + bx^m + \dots + cx + d,
\]

the derivative is

\[
nax^{n-1} + mbx^{m-1} + \dots + c.
\]

In other words, we drop the last term (which is the first term
in our Haskell representation of polynomials)
and, for all other terms, we multiply the term by the exponent
and reduce the exponent by one.

The derivative of the polynomial $x^4$, for instance,
is $4x^3$; in the notation of our polynomial type,
we have \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]} and its derivative \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{4}\mskip1.5mu]}.
The derivative of $4x^3$ is $12x^2$, whose derivative then
is $24x$, whose derivative is just $24$ (a number you have
already seen in this very section!).
The deriviative of our polynomial

\[
12x^4 + 16x^3 + 11x^2 + 9x + 14
\]

is 

\[
48x^3 + 48x^2 + 22x + 9.
\]

Note that the first term equals the first term
of the polynomial that we identified as the generator
of the first difference sequence. Indeed,
the differences are sloppy as a measure for
the rate of change -- but they are not completely wrong!

Here is a function to compute the derivative:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{derivative}\mathbin{::}{}\<[18]%
\>[18]{}(\Conid{Eq}\;\Varid{a},\Conid{Num}\;\Varid{a},\Conid{Enum}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[18]{}(\Varid{a}\to \Varid{a}\to \Varid{a})\to \Conid{Poly}\;\Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{derivative}\;\Varid{o}\;(\Conid{P}\;\Varid{as})\mathrel{=}\Conid{P}\;(\Varid{cleanz}\;(\Varid{map}\;\Varid{op}\;(\Varid{zip}\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;(\Varid{drop}\;\mathrm{1}\;\Varid{as})))){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{op}\;(\Varid{x},\Varid{c})\mathrel{=}\Varid{x}\mathbin{`\Varid{o}`}\Varid{c}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Note that we keep the implementation of \ensuremath{\Varid{derivative}}
flexible. Instead of hardcoding $\times$,
we use a function parameter `\ensuremath{\Varid{o}}', so we can pass in the operation
we need. We will later see how this is useful.

What is the sequence generated by the derivative of our polynomial?
Well, we define the derivative as
\ensuremath{\mathbf{let}\;\Varid{p'}\mathrel{=}\Varid{derivative}\;(\Conid{P}\;[\mskip1.5mu \mathrm{14},\mathrm{9},\mathrm{11},\mathrm{16},\mathrm{12}\mskip1.5mu])}, which is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{9},\mathrm{22},\mathrm{48},\mathrm{48}\mskip1.5mu]},
apply it using \ensuremath{\Varid{mapply}\;\Varid{p'}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} and see:

9,127,629,1803,3937,7319,12237,18979,27833,39087,53029

Quite different from the first difference sequence we saw above!

What about the second derivative? We define
\ensuremath{\mathbf{let}\;\Varid{p''}\mathrel{=}\Varid{derivative}\;\Varid{p'}} and get \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{22},\mathrm{96},\mathrm{144}\mskip1.5mu]}.
This polynomial creates the sequence

22,262,790,1606,2710,4102,5782,7750,10006,12550,15382

The next derivative, \ensuremath{\mathbf{let}\;\Varid{p'''}\mathrel{=}\Varid{derivative}\;\Varid{p''}},
is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{96},\mathrm{288}\mskip1.5mu]} and generates the sequence

96,384,672,960,1248,1536,1824,2112,2400,2688,2976.

You can already predict the next derivative,
which is a polynomial of degree 0: it is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{288}\mskip1.5mu]}.
This is a constant polynomial and will generate a constant
sequence, namely the sequence 288. That, however,
was also the constant sequence generated by the
difference engine. Of course, when the rate of change
is the same everywhere in the original polynomial,
then precision does not make any difference anymore.
The two methods shall come to the same result.

Consider the simple polynomial $x^2$.
It generates the sequence

\[
0,1,4,9,16,25,36,49,\dots
\]

The differences are

\[
1,3,5,7,9,11,13,\dots
\]

The differences of this list are all 2.

The derivative of $x^2$ is $2x$.
It would generate the sequence

\[
0,2,4,6,8,10,12,14,\dots
\]

which does not equal the differences.
However, we can already see that the derivative
of $2x$, $2$, is constant and generates the constant
sequence 

\[
2,2,2,2,2,2,2,2,\dots
\]
\section{Differences and Binomial Coefficients}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{DiffBinom}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Zahl}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Quoz}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Real}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{NumSystem}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{PolyArith}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{DMachine}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Isaac Newton studied the relation
between sequences and their differences 
intensely and came up with a formula. Before we go right
to it, let us observe on our own.
The following table shows the values and differences
of a certain polynomial. In the first row, it shows
the value of $n$, \ie\ the value to which the polynomial
is applied; in the second row, we see the result
for this $n$; in the first column we have the first
values from the sequence and its difference lists:

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{||c||c|c|c|c|c||}
\hline
     &  0 &  1  &  2  &  3   &  4   \\\hline
     & 14 & 62  & 396 & 1544 & 4322 \\\hline\hline
  14 &  1 &  1  &  1  &  1   &  1   \\\hline
  48 &  0 &  1  &  2  &  3   &  4   \\\hline
 286 &  0 &  0  &  1  &  3   &  6   \\\hline
 528 &  0 &  0  &  0  &  1   &  4   \\\hline
 288 &  0 &  0  &  0  &  0   &  1   \\\hline
\end{tabular}
\endgroup
\end{center}

What we see in the cells of the table
are factors. With their help, we can compute
the values in the sequence by formulas of the type:

\begin{equation}
\begin{array}{rcrcrcrcrcrcrcrcrcrcr}
  1 & \times & 14 &   &   &        &    &   &   &        &     &   &   &        &     &   &   &        &     & = & 14\\
  1 & \times & 14 & + & 1 & \times & 48 &   &   &        &     &   &   &        &     &   &   &        &     & = & 62\\
  1 & \times & 14 & + & 2 & \times & 48 & + & 1 & \times & 286 &   &   &        &     &   &   &        &     & = & 396\\
  1 & \times & 14 & + & 3 & \times & 48 & + & 3 & \times & 286 & + & 1 & \times & 528 &   &   &        &     & = & 1544\\
  1 & \times & 14 & + & 4 & \times & 48 & + & 6 & \times & 286 & + & 4 & \times & 528 & + & 1 & \times & 288 & = & 4322
\end{array}
\end{equation}

The next question would then be: what are those numbers?
But, here, I have to ask you to look a bit more closely at the table.
What we see in the columns left-to-right is:

\begin{center}
\begin{tabular}{cccccccccc}
  &   &     &    &     & 1 &      &   &   &   \\
  &   &     &    &  1  &   &  1   &   &   &   \\
  &   &     &  1 &     & 2 &      & 1 &   &   \\
  &   &  1  &    &  3  &   &  3   &   & 1 &    \\
  & 1 &     & 4  &     & 6 &      & 4 &   & 1 
\end{tabular}
\end{center}

Those are binomial coefficients!
Indeed. We could rewrite the table as

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{||c||c|c|c|c|c||}
\hline
     &  0 &  1  &  2  &  3   &  4   \\\hline
     & 14         & 62         & 396        & 1544       & 4322       \\\hline\hline
  14 &$\binom{0}{0}$&$\binom{1}{0}$&$\binom{2}{0}$&$\binom{3}{0}$&$\binom{4}{0}$\\\hline
  48 &$\binom{0}{1}$&$\binom{1}{1}$&$\binom{2}{1}$&$\binom{3}{1}$&$\binom{4}{1}$\\\hline
 286 &$\binom{0}{2}$&$\binom{1}{2}$&$\binom{2}{2}$&$\binom{3}{2}$&$\binom{4}{2}$\\\hline
 528 &$\binom{0}{3}$&$\binom{1}{3}$&$\binom{2}{3}$&$\binom{3}{3}$&$\binom{4}{3}$\\\hline
 288 &$\binom{0}{3}$&$\binom{1}{4}$&$\binom{2}{4}$&$\binom{3}{4}$&$\binom{4}{4}$\\\hline
\end{tabular}
\endgroup
\end{center}

If this were universally true, we could devise a 
much better prediction function. The one we wrote
in the previous section has the disadvantage
that we can only predict the next number in the sequence.
To predict a value way ahead we need to generate
number by number before we are there.
With Newton's trick, we could compute any number
in the sequence in one step.
All we have to do is to get the \ensuremath{\Varid{head}}s of the sequences
and to calculate the formula:

\[
\sum_{k=0}^{d}{h_k\binom{n}{k}} 
\]

where $d$ is the degree of the polynomial, $n$
the position in the sequence, \ie\ the number
to which we apply the polynomial, and $h_k$
the head of the sequence starting to count
with the original sequence as $k=0$.
The sixth value ($n=5$) of the sequence would then be

\[
  14 \times \binom{5}{0} + 
  48 \times \binom{5}{1} + 
 286 \times \binom{5}{2} + 
 528 \times \binom{5}{3} + 
 288 \times \binom{5}{4}, 
\]

which is

\[
  14           + 
  48 \times  5 + 
 286 \times 10 + 
 528 \times 10 + 
 288 \times  5, 
\]

which, in its turn, is

\[
14 + 240 + 2860 + 5280 + 1440 = 9834,
\]

which is indeed the next value in the sequence.

Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{newton}\mathbin{::}\Conid{Zahl}\to [\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to \Conid{Zahl}{}\<[E]%
\\
\>[3]{}\Varid{newton}\;\Varid{n}\;\Varid{ds}\;\Varid{seq}\mathrel{=}\Varid{sum}\;\Varid{ts}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{hs}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{getHeads}\;\Varid{seq}\;\Varid{ds}{}\<[E]%
\\
\>[12]{}\Varid{ts}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}[\mskip1.5mu \Varid{h}\mathbin{*}(\Varid{choose}\;\Varid{n}\;\Varid{k})\mid (\Varid{h},\Varid{k})\leftarrow \Varid{zip}\;\Varid{hs}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{getHeads}\mathbin{::}[\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{getHeads}\;\Varid{seq}\;\Varid{ds}\mathrel{=}\Varid{map}\;\Varid{head}\;(\Varid{seq}\mathbin{:}\Varid{ds}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

To perform some experiments, here, as a reminder,
are the first 14 numbers of the sequence generated
by our polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{14},\mathrm{9},\mathrm{11},\mathrm{16},\mathrm{12}\mskip1.5mu]}:

14,62,396,1544,4322,9834,19472,34916,58134,91382,137204,198432,278186,379874

We set \ensuremath{\Varid{s}\mathrel{=}\Varid{mapply}\;\Conid{P}\;[\mskip1.5mu \mathrm{14},\mathrm{9},\mathrm{11},\mathrm{16},\mathrm{12}\mskip1.5mu]\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} and \ensuremath{\Varid{d}\mathrel{=}\Varid{dengine}\;\Varid{s}}.
Now we perform some tests:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{newton}\;\mathrm{0}\;\Varid{d}\;\Varid{s}} gives      14.\\
\ensuremath{\Varid{newton}\;\mathrm{1}\;\Varid{d}\;\Varid{s}} gives      62.\\
\ensuremath{\Varid{newton}\;\mathrm{5}\;\Varid{d}\;\Varid{s}} gives    9834.\\
\ensuremath{\Varid{newton}\;\mathrm{11}\;\Varid{d}\;\Varid{s}} gives  198432.\\
\ensuremath{\Varid{newton}\;\mathrm{13}\;\Varid{d}\;\Varid{s}} gives  379874.
\end{minipage}

The approach seems to work.
But there is more.
The function \ensuremath{\Varid{newton}} gives us a closed form 
to compute any number in the sequence,
given that we have the beginning of that sequence
and its difference lists.
A closed form, however, is a generating formula --
it is the polynomial that generates the entire sequence.
We just need a way to make the formula implicit in
\ensuremath{\Varid{newton}} explicit.

We can do that using our polynomial data type.
When we can express the binomial coefficients
in terms of polynomials and apply them
to the formula used above, we will get the polynomial out
that generates this sequence.
Here is a function that does that:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}c<{\hspost}@{}}%
\column{18E}{@{}l@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{bin2poly}\mathbin{::}\Conid{Zahl}\to \Conid{Zahl}\to \Conid{Poly}\;\Conid{Quoz}{}\<[E]%
\\
\>[3]{}\Varid{bin2poly}\;\Varid{h}\;\mathrm{0}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Conid{P}\;[\mskip1.5mu \Varid{h}\mathbin{\%}\mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{bin2poly}\;\Varid{h}\;\mathrm{1}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Conid{P}\;[\mskip1.5mu \mathrm{0},\Varid{h}\mathbin{\%}\mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{bin2poly}\;\Varid{h}\;\Varid{k}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Conid{P}\;[\mskip1.5mu \Varid{h}\mathbin{\%}(\Varid{\Conid{B}.fac}\;\Varid{k})\mskip1.5mu]\mathbin{`\Varid{mul}`}\Varid{go}\;(\Varid{k}\mathbin{\%}\mathrm{1}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\mathrm{1}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{i}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Conid{P}\;[\mskip1.5mu \mathbin{-}(\Varid{i}\mathbin{-}\mathrm{1}),\mathrm{1}\mskip1.5mu]\mathbin{`\Varid{mul}`}(\Varid{go}\;(\Varid{i}\mathbin{-}\mathrm{1})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives two integers:
the first one is a factor (the head) 
by which we multiply the resulting binomial polynomial
and the second one is $k$ in $\binom{n}{k}$.
Note that we do not need $n$, since $n$ is the unknown,
the base of our polynomial.

If $k=0$, the binomial is 1,
since for all binomial coefficients:
$\binom{n}{0} = 1$. We, hence, return a constant polynomial
consisting of the factor. This corresponds to 
$h_0 \times \binom{n}{0}$. The result is just $h_0$.
Note that we convert the coefficients to rational numbers,
since that is the type the function is supposed to yield.

If $k=1$, the binomial is $n$, since for all binomials:
$\binom{n}{1} = n$. Because $n$ is the base of the polynomial,
$n$ itself is expressed by \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]}. 
This is just $n+0$ and, hence, $n$.
Since we multiply with $h$, the result in this case is
$h \times n = hn$, or, in the language of our Haskell
polynomials \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\Varid{h}\mskip1.5mu]}.

Otherwise, we go into the recursive \ensuremath{\Varid{go}} function.
The function receives one rational number, namely $k$
(which, de facto, is an integer).
The base case is $k=1$. In that case we yield \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]},
which is just $n$.
Otherwise, we create the polynomial
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathbin{-}(\Varid{i}\mathbin{-}\mathrm{1}),\mathrm{1}\mskip1.5mu]}, that is $n-(k-1)$ and multiply
with the result of \ensuremath{\Varid{go}} applied to $i-1$.
The function, hence, creates the numerator
of the fraction formula of the binomial coefficient:

\[
n(n-1)(n-2)\dots (n-k+1).
\]

The result of the function is then multiplied by
$h$ divided by $k!$. The former, still, is some head
from the difference sequences and
the latter is the denominator
of the fraction formula. We, thus, compute:

\[
\frac{hn(n-1)(n-2)\dots (n-k+1)}{k!}.
\]

Now, we can use this formula represented by a 
polynomial to compute the generating polynomial.
The function that does so has exactly the same
structure as the \ensuremath{\Varid{newton}} function. The difference
is just that it expresses binomial coefficients
as polynomials and that it does not receive 
a concrete number $n$ for which we want to compute
the corresponding value (because we want to compute
the formula generating all the values):

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{newtonGen}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to \Conid{Poly}\;[\mskip1.5mu \Conid{Quoz}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{newtonGen}\;\Varid{ds}\;\Varid{seq}\mathrel{=}\Varid{sump}\;\Varid{ts}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{hs}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{getHeads}\;\Varid{seq}\;\Varid{ds}{}\<[E]%
\\
\>[12]{}\Varid{ts}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}[\mskip1.5mu \Varid{bin2poly}\;\Varid{h}\;\Varid{k}\mid (\Varid{h},\Varid{k})\leftarrow \Varid{zip}\;\Varid{hs}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{n}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{fromIntegral}\;(\Varid{length}\mathbin{\$}\Varid{ds}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

When we call \ensuremath{\Varid{newtonGen}\;\Varid{ds}\;\Varid{s}}, $ds$ 
still being the difference lists and
$s$ the sequence in question, we see:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{14}\mathbin{\%}\mathrm{1},\mathrm{9}\mathbin{\%}\mathrm{1},\mathrm{11}\mathbin{\%}\mathrm{1},\mathrm{16}\mathbin{\%}\mathrm{1},\mathrm{12}\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which we immediately recognise as our polynomial
$12x^4 + 16x^3 + 11x^2 + 9x + 14$.

For another test, we apply the monomial $x^5$ as

\ensuremath{\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]\;\mathbf{in}\;\Varid{newtonGen}\;(\Varid{dengine}\;\Varid{s})\;\Varid{s}}

and see

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{0}\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which is indeed the polynomial $x^5$.

But now comes the hard question:
why does that work at all???

To answer this question, we should make sure to understand
how Newton's formula works. The point is that
we restrict ourselves to the heads of the sequences as basic
building blocks. When we compute some value $x_n$ in the sequence,
we need to recursively compute $x_{n-1}$ and the difference between
$x_{n-1}$ and $x_{n}$ and add them together.
Let us build a model that simulates this approach
and that allows us to reason about 
what is going on more easily.

We use as a model a polynomial of degree 3;
that model is sufficiently complex to simulate the problem
completely and is, on the other hand, somewhat simpler
than a model based on a polynomial of degree 4,
like the one we have studied above --
not to mention a model for polynomials of any degree.

The model consists of a data type:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\mathbf{data}\;\Conid{Newton}\mathrel{=}\Conid{H}\mid \Conid{X}\mid \Conid{Y}\mid \Conid{Z}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{deriving}\;(\Conid{Show},\Conid{Eq}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The \ensuremath{\Conid{Newton}} type has four constructors:
\ensuremath{\Conid{H}} represents the head of the original sequence;
\ensuremath{\Conid{X}} is the head of the first difference list;
\ensuremath{\Conid{Y}} is the head of the second difference list and
\ensuremath{\Conid{Z}} is the constant element repeated in the last difference list.
(Remember that a polynomial of degree 3 
generates 3 difference lists.)

The model also contains a function
to compute positions in the sequence.
This function, called \ensuremath{\Varid{cn}} (for ``computeNewton''),
takes two arguments: a \ensuremath{\Conid{Newton}} constructor and an integer.
The integer tells us the position we want to compute
starting with the head $H = 0$:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{cn}\mathbin{::}\Conid{Newton}\to \Conid{Natural}\to [\mskip1.5mu \Conid{Newton}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cn}\;\Conid{H}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \Conid{H}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cn}\;\Conid{H}\;\Varid{n}\mathrel{=}\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})\plus \Varid{cn}\;\Conid{X}\;(\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

When we want to compute the first element in the sequence,
\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}}, we just return \ensuremath{[\mskip1.5mu \Conid{H}\mskip1.5mu]}. When we want to compute
any other number, we recursively call \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})},
which computes the previous data point, and add \ensuremath{\Varid{cn}\;\Conid{X}\;(\Varid{n}\mathbin{-}\mathrm{1})},
which computes the difference between $n$ and $n-1$.
Here is how we compute the difference:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{cn}\;\Conid{X}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \Conid{X}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cn}\;\Conid{X}\;\Varid{n}\mathrel{=}\Varid{cn}\;\Conid{X}\;(\Varid{n}\mathbin{-}\mathrm{1})\plus \Varid{cn}\;\Conid{Y}\;(\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

If we need the first difference, \ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}}, we just return
\ensuremath{[\mskip1.5mu \Conid{X}\mskip1.5mu]}. Otherwise, we call \ensuremath{\Varid{cn}\;\Conid{X}\;(\Varid{n}\mathbin{-}\mathrm{1})}, this computes
the previous difference, and compute \ensuremath{\Varid{cn}\;\Conid{Y}\;(\Varid{n}\mathbin{-}\mathrm{1})},
the difference between the previous and the current difference.
Here is how we compute the difference of the difference:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{cn}\;\Conid{Y}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \Conid{Y}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cn}\;\Conid{Y}\;\Varid{n}\mathrel{=}\Conid{Z}\mathbin{:}\Varid{cn}\;\Conid{Y}\;(\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

If we need the first difference, \ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{0}}, we just return
\ensuremath{[\mskip1.5mu \Conid{Y}\mskip1.5mu]}. Otherwise, we compute the previous difference \ensuremath{\Varid{cn}\;\Conid{Y}\;(\Varid{n}\mathbin{-}\mathrm{1})}
adding \ensuremath{\Conid{Z}}, the constant difference, to the result.

The simplest case is of course 
computing the first in the sequence.
This is just:

\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}}, which yields \ensuremath{[\mskip1.5mu \Conid{H}\mskip1.5mu]}.

Computing the second in the sequence is slightly more work:

\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{1}} goes to\\
\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}\plus \Varid{cn}\;\Conid{X}\;\mathrm{0}} which is\\
\ensuremath{[\mskip1.5mu \Conid{H}\mskip1.5mu]\plus [\mskip1.5mu \Conid{X}\mskip1.5mu]}.

We, hence, get \ensuremath{[\mskip1.5mu \Conid{H},\Conid{X}\mskip1.5mu]}. That is the head of the sequence
plus the head of the first difference list.

Computing the third in the sequence

\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{2}} calls\\
\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{1}\plus \Varid{cn}\;\Conid{X}\;\mathrm{1}}, which is\\
\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}\plus \Varid{cn}\;\Conid{X}\;\mathrm{0}} and \ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}\plus \Varid{cn}\;\Conid{Y}\;\mathrm{0}}.

We hence get \ensuremath{[\mskip1.5mu \Conid{H},\Conid{X},\Conid{X},\Conid{Y}\mskip1.5mu]}.
This is the head of the original sequence
plus the head of the first difference sequence
(we are now at \ensuremath{\Conid{H}\;\mathrm{1}})
plus this difference plus the first of
the second difference sequence.

This looks simple, but already after a few steps,
the result looks weird. For \ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{5}}, for example, we see

\ensuremath{[\mskip1.5mu \Conid{H},\Conid{X},\Conid{X},\Conid{Y},\Conid{X},\Conid{Y},\Conid{Z},\Conid{Y},\Conid{X},\Conid{Y},\Conid{Z},\Conid{Y},\Conid{Z},\Conid{Z},\Conid{Y},\Conid{X},\Conid{Y},\Conid{Z},\Conid{Y},\Conid{Z},\Conid{Z},\Conid{Y},\Conid{Z},\Conid{Z},\Conid{Z},\Conid{Y}\mskip1.5mu]},

which is somewhat confusing. The result, however,
is correct. We can illustrate that by comparing
the result with a real polynomial of degree 3, say,
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{28},\mathrm{15},\mathrm{22}\mskip1.5mu]}, this is the polynomial
$22x^3 + 15x^2 + 28x + 2$; 
this polynomial generates the sequence
2, 67, 294, 815, 1762, 3267, 5462, 8479, 12450, 17507, 23782.

We now define a function that substitutes the symbols
of our model by the heads of the sequence and the
difference lists:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{new2a}\mathbin{::}(\Varid{a},\Varid{a},\Varid{a},\Varid{a})\to \Conid{Newton}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{new2a}\;(\Varid{h},\Varid{x},\Varid{y},\Varid{z})\;\Varid{n}\mathrel{=}\mathbf{case}\;\Varid{n}\;\mathbf{of}{}\<[E]%
\\
\>[3]{}\hsindent{23}{}\<[26]%
\>[26]{}\Conid{H}\to \Varid{h}{}\<[E]%
\\
\>[3]{}\hsindent{23}{}\<[26]%
\>[26]{}\Conid{X}\to \Varid{x}{}\<[E]%
\\
\>[3]{}\hsindent{23}{}\<[26]%
\>[26]{}\Conid{Y}\to \Varid{y}{}\<[E]%
\\
\>[3]{}\hsindent{23}{}\<[26]%
\>[26]{}\Conid{Z}\to \Varid{z}{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{subst}\mathbin{::}(\Varid{a},\Varid{a},\Varid{a},\Varid{a})\to [\mskip1.5mu \Conid{Newton}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{subst}\;\Varid{as}\mathrel{=}\Varid{map}\;(\Varid{new2a}\;\Varid{as}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The head of the sequence is 2; the heads of the difference
sequences are 65, 162 and 132.
We call the function as \ensuremath{\Varid{subst}\;(\mathrm{2},\mathrm{65},\mathrm{162},\mathrm{132})\;(\Varid{cn}\;\Conid{H}\;\mathrm{5})}
and see

\begin{minipage}{\textwidth}
2, 65, 65, 162, 65, 162, 132, 162, 65, 162, 132, 162, 132, 132, 162,
65, 162, 132, 162, 132, 132, 162, 132, 132, 132, 162.
\end{minipage}

When we sum this together,
\ensuremath{\Varid{sum}\;(\Varid{subst}\;(\mathrm{2},\mathrm{65},\mathrm{162},\mathrm{132})\;(\Varid{cn}\;\Conid{H}\;\mathrm{5}))},
we get 3267, which is indeed the number appearing at
position 5 in the sequence (starting to count from 0).

We implement one more function: \ensuremath{\Varid{ccn}}, for
``count cn'':

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{ccn}\mathbin{::}[\mskip1.5mu \Conid{Newton}\mskip1.5mu]\to (\Conid{Int},\Conid{Int},\Conid{Int},\Conid{Int}){}\<[E]%
\\
\>[3]{}\Varid{ccn}\;\Varid{ls}\mathrel{=}({}\<[15]%
\>[15]{}\Varid{length}\;(\Varid{filter}\;(\equiv \Conid{H})\;\Varid{ls}),{}\<[E]%
\\
\>[15]{}\Varid{length}\;(\Varid{filter}\;(\equiv \Conid{X})\;\Varid{ls}),{}\<[E]%
\\
\>[15]{}\Varid{length}\;(\Varid{filter}\;(\equiv \Conid{Y})\;\Varid{ls}),{}\<[E]%
\\
\>[15]{}\Varid{length}\;(\Varid{filter}\;(\equiv \Conid{Z})\;\Varid{ls})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

When we apply this function, \eg\ \ensuremath{\Varid{ccn}\;(\Varid{cn}\;\Conid{H}\;\mathrm{3})},
we see:

\ensuremath{(\mathrm{1},\mathrm{3},\mathrm{3},\mathrm{1})}

The binomial coefficients $\binom{3}{k}$, 
for $k \in \lbrace 0\dots 3\rbrace$.

To see some more examples we call
\ensuremath{\Varid{map}\;(\Varid{ccn}\mathbin{\circ}\Varid{cn}\;\Conid{H})\;[\mskip1.5mu \mathrm{4}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]} and get

\begin{minipage}{\textwidth}
\ensuremath{[\mskip1.5mu (\mathrm{1},\mathrm{4},\mathrm{6},\mathrm{4}),}\\
\ensuremath{(\mathrm{1},\mathrm{5},\mathrm{10},\mathrm{10}),}\\
\ensuremath{(\mathrm{1},\mathrm{6},\mathrm{15},\mathrm{20}),}\\
\ensuremath{(\mathrm{1},\mathrm{7},\mathrm{21},\mathrm{35}),}\\
\ensuremath{(\mathrm{1},\mathrm{8},\mathrm{28},\mathrm{56}),}\\
\ensuremath{(\mathrm{1},\mathrm{9},\mathrm{36},\mathrm{84}),}\\
\ensuremath{(\mathrm{1},\mathrm{10},\mathrm{45},\mathrm{120})\mskip1.5mu]}
\end{minipage}

What we see, in terms of the table we used above, is

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{||c||c|c|c|c||}
\hline
     &  0    &  1    &  2    &  3   \\\hline
     & $n_0$ & $n_1$ & $n_2$ & $n_3$ \\\hline\hline
   H &$\binom{0}{0}$&$\binom{1}{0}$&$\binom{2}{0}$&$\binom{3}{0}$\\\hline
   X &$\binom{0}{1}$&$\binom{1}{1}$&$\binom{2}{1}$&$\binom{3}{1}$\\\hline
   Y &$\binom{0}{2}$&$\binom{1}{2}$&$\binom{2}{2}$&$\binom{3}{2}$\\\hline
   Z &$\binom{0}{3}$&$\binom{1}{3}$&$\binom{2}{3}$&$\binom{3}{3}$\\\hline
\end{tabular}
\endgroup
\end{center}

So, why do we see binomial coefficients and
can we prove that we will always see binomial coefficients?
To answer the first question, we will analyse the
execution tree of \ensuremath{\Varid{cn}}. Here is the tree for \ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{3}}:

\begin{center}
\begin{tikzpicture}
% root
\node (H3) at (6,     5) {\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{3}}};

\node (H2) at (5 ,  4) {\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{2}}};
\node (X2) at (7,   4) {\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{2}}};

\node (X1) at (7  ,   3) {\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{1}}};
\node (Y1) at (10 ,   3) {\ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{1}}};

\node (X0) at (7  ,   2) {\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}}};
\node (Y0) at (8.5,   2) {\ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{0}}};

\node (X) at (7  ,   1) {\ensuremath{[\mskip1.5mu \Conid{X}\mskip1.5mu]}};
\node (Y) at (8.5,   1) {\ensuremath{[\mskip1.5mu \Conid{Y}\mskip1.5mu]}};

\node (Z)   at (10 ,  2) {\ensuremath{\Conid{Z}\mathbin{:}}};
\node (Y02) at (11.5,   2) {\ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{0}}};
\node (Y')  at (11.5,  1) {\ensuremath{[\mskip1.5mu \Conid{Y}\mskip1.5mu]}};

\node (H1) at (4   ,  3) {\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{1}}};
\node (X12) at (5.5 ,  3) {\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{1}}};

\node (H0)  at (3,   2) {\ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}}};
\node (X02) at (4.5 ,2) {\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}}};

\node (H) at  (3 ,    1) {\ensuremath{[\mskip1.5mu \Conid{H}\mskip1.5mu]}};

\connect{H3} {H2};
\connect{H3} {X2};
\connect{X2} {X1};
\connect{X2} {Y1};
\connect{X1} {X0};
\connect{X1} {Y0};
\connect{X0} {X};
\connect{Y0} {Y};
\connect{Y1} {Z};
\connect{Y1} {Y02};
\connect{Y02} {Y'};
\connect{H2} {H1};
\connect{H2} {X12};
\connect{H1} {H0};
\connect{H0} {H};
\connect{H1} {X02};
\end{tikzpicture}
\end{center}

On the left-hand side of the tree,
you see the main execution path 
calling \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})} and \ensuremath{\Varid{cn}\;\Conid{X}\;(\Varid{n}\mathbin{-}\mathrm{1})}
on each level. The sketch expands \ensuremath{\Varid{cn}\;\Conid{X}}
only for one case, namely the top-level call
\ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{2}} on the right-hand side. 
Otherwise, the tree would be quite confusing.

Anyway, what we can see:
\begin{itemize}
\item Any top-level call of type \ensuremath{\Varid{cn}\;\Conid{A}} 
      (for $A \in \lbrace H,X,Y\rbrace$)
      creates only one \ensuremath{\Conid{A}};
      we therefore have always exactly one \ensuremath{\Conid{H}}.
\item Every call to \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}, for $n > 0$,
      calls one instance of \ensuremath{\Varid{cn}\;\Conid{X}}.
      We therefore have exactly $n$ \ensuremath{\Conid{X}}.
\item Every call to \ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}}, for $n > 0$,
      calls one instance of \ensuremath{\Varid{cn}\;\Conid{Y}}.
      We therefore have exactly $n$ \ensuremath{\Conid{Y}} per \ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}},
      $n > 0$.
\item Every call to \ensuremath{\Varid{cn}\;\Conid{Y}\;\Varid{n}}, for $n>0$, creates one \ensuremath{\Conid{Z}}.
\item The call to \ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{1}} would expand to
      \ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}\plus \Varid{cn}\;\Conid{Y}\;\mathrm{0}}; it would, hence,
      create one more \ensuremath{\Conid{X}} and one more \ensuremath{\Conid{Y}}.
\item The call to \ensuremath{\Varid{cn}\;\Conid{X}\;\mathrm{0}} would create one more \ensuremath{\Conid{X}}.
\item This execution, thus, creates
      1 \ensuremath{\Conid{H}}, 3 \ensuremath{\Conid{X}}, 3 \ensuremath{\Conid{Y}} and 1 \ensuremath{\Conid{Z}}.
\end{itemize}

We now prove by induction that if a call to \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}
creates 

\[
\binom{n}{0}H, \binom{n}{1}X, \binom{n}{2}Y 
\text{ and } \binom{n}{3}Z
\]

(and the previous calls to \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})}, \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{2})},
$\dots$, \ensuremath{\Varid{cn}\;\Conid{H}\;\mathrm{0}} created similar patterns including
the binomial coefficients),
then \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})} creates

\[
\binom{n+1}{0}H, \binom{n+1}{1}X, \binom{n+1}{2}Y 
\text{ and } \binom{n+1}{3}Z.
\]

Note that the number of \ensuremath{\Conid{H}} does not increase,
because, as observed, each top-level call to \ensuremath{\Varid{cn}\;\Conid{A}\;\Varid{n}}
creates exactly one \ensuremath{\Conid{A}}.
If \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}} creates one \ensuremath{\Conid{H}},
\ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})} creates exactly one \ensuremath{\Conid{H}}, too.
We conclude that we create $\binom{n+1}{0}H$ as requested.

When we call \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})}, we will call \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}.
We, therefore, create all instances of \ensuremath{\Conid{X}} created by \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}
plus those created in the first level of \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})}.
This new level calls \ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}} exaclty once,
which creates one \ensuremath{\Conid{X}} (because any top-level call to \ensuremath{\Varid{cn}\;\Conid{A}\;\Varid{n}}
creates exactly one \ensuremath{\Conid{A}}).
We, hence, create one \ensuremath{\Conid{X}} more.
This, however, is 
$\binom{n}{0} + \binom{n}{1} = \binom{n+1}{1}$
according to Pascal's Rule.
We conclude that we create $\binom{n+1}{1}X$ as requested.

Since we call \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}, when we call \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})}, 
we also create all instances of \ensuremath{\Conid{Y}} that were created
by \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}. We additionally create all instances of \ensuremath{\Conid{Y}}
that are created by the new call to \ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}}.
This, in its turn, calls $n$ instances of \ensuremath{\Varid{cn}\;\Conid{Y}}.
Since $n = \binom{n}{1}$ and any top-level call to 
\ensuremath{\Varid{cn}\;\Conid{Y}\;\Varid{n}} creates exactly one \ensuremath{\Conid{Y}}, we create
$\binom{n}{1} + \binom{n}{2} = \binom{n+1}{2}Y$ as requested.

Finally, since we call \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}, when we call \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})},
we also create all instances of \ensuremath{\Conid{Z}} that were created before.
But we call one more instance of \ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}}, which creates a
certain amount of new \ensuremath{\Conid{Z}}. How many?
We create again all \ensuremath{\Conid{Z}} that were created anew by \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}},
those that did not exist in \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})}.
Let us call the number of \ensuremath{\Conid{Z}} created by \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}} $z_n$
and the number of \ensuremath{\Conid{Z}} created by \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})} $z_{n-1}$.
The number of \ensuremath{\Conid{Z}} created anew in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}} is then 
$z_n - z_{n-1}$.

But since, in \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})}, we call \ensuremath{\Varid{cn}\;\Conid{X}} one level up,
more \ensuremath{\Conid{Z}} are created than before.
All calls to \ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{0}}, those that did not create a new \ensuremath{\Conid{Z}}
in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}},
are now called as \ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{1}} and, hence, create a \ensuremath{\Conid{Z}}
that was not created before. The calls to \ensuremath{\Varid{cn}\;\Conid{Y}\;\mathrm{0}} create
\ensuremath{\Conid{Y}} that were not created by \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})}.
We, therefore, need to add to the number of \ensuremath{\Conid{Z}} the number
of \ensuremath{\Conid{Y}} that did not exist in \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{-}\mathrm{1})}.
We use the same convention as for \ensuremath{\Conid{Z}}, \ie\
the  number of \ensuremath{\Conid{Y}} created anew in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}} is
$y_n - y_{n-1}$.
The number of additional \ensuremath{\Conid{Z}} 
created by the additional call to 
\ensuremath{\Varid{cn}\;\Conid{X}\;\Varid{n}}, hence, is

\[
y_n - y_{n-1} + z_n - z_{n-1}
\]

But we are dealing with binomial coefficients.
We, therefore, have $z_n = y_{n-1} + z_{n-1}$
by Pascals' Rule applied backwards.
When we substitute this back, we get

\[
y_n - y_{n-1} + y_{n-1} + z_{n-1} - z_{n-1},
\]

which simplifies to $y_n$, \ie\ the number of 
instances of \ensuremath{\Conid{Y}} created by \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}.
In other words: the number of \ensuremath{\Conid{Z}} we 
additionally create in \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})} is the number
of \ensuremath{\Conid{Y}} in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}.
So, the complete number of \ensuremath{\Conid{Z}} we have 
in \ensuremath{\Varid{cn}\;\Conid{H}\;(\Varid{n}\mathbin{+}\mathrm{1})} is
the number of \ensuremath{\Conid{Y}} in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}} 
plus the number \ensuremath{\Conid{Z}} in \ensuremath{\Varid{cn}\;\Conid{H}\;\Varid{n}}.
Since the number of \ensuremath{\Conid{Y}} is $\binom{n}{2}$
and the number of \ensuremath{\Conid{Z}} is  $\binom{n}{3}$,
we now have 
$\binom{n}{2} + \binom{n}{3} = \binom{n+1}{3}$ 
according to Pascal's Rule as requested
and this completes the proof.\qed
\section{Umbral Calculus}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Umbral}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{DMachine}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

We saw that the differences and the derivative
is not the same concept. Despite of many similarities,
the polynomial of degree $n-1$ that generates 
the differences of a given polynomial of degree $n$
is not necessarily the derivative of that polynomial.
There is a class of polynomials, however, for which
derivative, well, a very special kind of derivative,
and differences are actually the same.
Those are the \term{factorial polynomials}.

A factorial polynomial $x^{(n)}$ is a polynomial of the form

\begin{equation}
x^{(n)} = x(x-1)(x-2) \dots (x-n+1). 
\end{equation}

A factorial polynomial, hence, is generated by the
\term{falling factorial} of $x$.
The simplest factorial polynomial $x^{(1)}$ is

\begin{equation}
x^{(1)} = x. 
\end{equation}

The, arguably, even simpler than simplest factorial polynomial
$x^{(0)}$ is, according to the definition of the
factorials, 1.

Here is a Haskell function that shows the
factors of the $n^{th}$ factorial polynomial:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{fpfacs}\mathbin{::}(\Conid{Integral}\;\Varid{a})\Rightarrow \Varid{a}\to [\mskip1.5mu \Conid{Poly}\;\Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{fpfacs}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{fpfacs}\;\Varid{n}\mathrel{=}[\mskip1.5mu \Varid{poly}\;[\mskip1.5mu \mathbin{-}\Varid{k},\mathrm{1}\mskip1.5mu]\mid \Varid{k}\leftarrow [\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{n}\mathbin{-}\mathrm{1}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Let us look at the first factorial polynomials:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{fpfacs}\;\mathrm{0}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{1}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{2}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{3}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{2},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{4}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{2},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{3},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{5}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{2},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{3},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{6}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{2},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{3},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{5},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{fpfacs}\;\mathrm{7}}: \ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{2},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{3},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{5},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{6},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}
\end{minipage}

This suggests that the factorial polynomials,
just as the factorials, can be defined recursively.
The following equation describes the recursive formula:

\begin{equation}
x^{(n+1)} = (x-n)x^{(n)},
\end{equation}

which we can translate to Haskell as

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{rfacpoly}\mathbin{::}(\Conid{Integral}\;\Varid{a})\Rightarrow \Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{rfacpoly}\;\mathrm{0}\mathrel{=}\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{rfacpoly}\;\Varid{n}\mathrel{=}\Varid{mul}\;(\Varid{rfacpoly}\;(\Varid{n}\mathbin{-}\mathrm{1}))\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}(\Varid{n}\mathbin{-}\mathrm{1}),\mathrm{1}\mskip1.5mu]){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The recursive formula is, of course, not an efficient
computing tool. For the factorial polynomial $x^{(n)}$,
we would need $n$ recursive steps, namely
$x^{(n-1)}(x-n+1)$,
$x^{(n-2)}(x-n+2)$, \dots,
$x^{(0)}x$.
To compute, for instance, $n=3$,
we need to compute:

\[
\begin{array}{lclcl}
x^{(1)} & = & x^{(0)}x &=& x\\
x^{(2)} & = & x(x-1) &=& (x^2 - x)\\
x^{(3)} & = & (x^2-x)(x-2) &=& (x^3 - 3x^2 + 2x)
\end{array}
\]

A better way to compute the polynomial,
once we have its factors, is to just multiply
them out, like: \ensuremath{\Varid{prodp}\;\Varid{mul}}.
The following implementation first creates
the factors and then builds their product:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{facpoly}\mathbin{::}(\Conid{Integral}\;\Varid{a})\Rightarrow \Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{facpoly}\mathrel{=}\Varid{prodp}\;\Varid{mul}\mathbin{\circ}\Varid{fpfacs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The two functions, \ensuremath{\Varid{rfacpoly}} and \ensuremath{\Varid{facpoly}},
create exactly the same result.
When we apply one of them to \ensuremath{[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{7}\mskip1.5mu]} 
as above we get

\begin{minipage}{\textwidth}
\ensuremath{\Varid{facpoly}\;\mathrm{1}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{facpoly}\;\mathrm{2}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathbin{-}\mathrm{1},\mathrm{1}\mskip1.5mu]}\\ 
\ensuremath{\Varid{facpoly}\;\mathrm{3}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{2},\mathbin{-}\mathrm{3},\mathrm{1}\mskip1.5mu]}\\ 
\ensuremath{\Varid{facpoly}\;\mathrm{4}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathbin{-}\mathrm{6},\mathrm{11},\mathbin{-}\mathrm{6},\mathrm{1}\mskip1.5mu]}\\ 
\ensuremath{\Varid{facpoly}\;\mathrm{5}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{24},\mathbin{-}\mathrm{50},\mathrm{35},\mathbin{-}\mathrm{10},\mathrm{1}\mskip1.5mu]}\\ 
\ensuremath{\Varid{facpoly}\;\mathrm{6}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathbin{-}\mathrm{120},\mathrm{274},\mathbin{-}\mathrm{225},\mathrm{85},\mathbin{-}\mathrm{15},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{facpoly}\;\mathrm{7}}: \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{720},\mathbin{-}\mathrm{1764},\mathrm{1624},\mathbin{-}\mathrm{735},\mathrm{175},\mathbin{-}\mathrm{21},\mathrm{1}\mskip1.5mu]} 
\end{minipage}

which corresponds to the polynomials (in mathematical notation):

\begin{center}
\begin{tabular}{c}
$x$ \\
$x^2 - x$ \\
$x^3 - 3x^2 + 2x$ \\
$x^4 - 6x^3 + 11x^2 - 6x$ \\
$x^5 - 10x^4 + 35x^3 -50x^2 + 24x$ \\
$x^6 - 15x^5 + 85x^4 - 225x^3 + 274x^2 -120x$ \\
$ x^7 - 21x^6 + 175x^5 - 735x^4 + 1624x^3 - 1764x^2 + 720x$  
\end{tabular}
\end{center}

Note, by the way, the last coefficient in each polynomial.
Those are factorials. More precisely, the last coefficient
of $x^{(n)}$ is $(n-1)!$
Does this pattern remind you of something?
Not? Don't worry, we will look into it later.

Let us now turn to differences.
Instead of just applying the polynomial to a sequence
of numbers and then compute the differences,
we could try to find a formula that expresses
the differences for a given polynomial.
When we take a formula like $x^{(3)}$,
we can compute its differences by
applying two consecutive values
and compute the difference of the results, \eg:

\[
\begin{array}{ccl}
&   & 3^{(3)} - 2^{(3)} \\
& = & (3^3 - 3\times 3^2 + 2\times 3) -
    (2^3 - 3\times 2^2 + 2\times 2)\\
& = & (27 - 27 + 6) - (8 - 12 + 4) \\
& = & 6 - 0\\
& = & 6.
\end{array}
\]

Instead of using concrete numbers, we can use
a placeholder like $a$:

\[
\begin{array}{ccl}
&   & (a+1)^{(3)} - a^{(3)}\\
& = & ((a+1)^3 - 3(a+1)^2 + 2(a+1)) -
      (a^3 - 3a^2 + 2a)\\
& = & ((a^3 +3a^2 + 3a + 1) - (3a^2+6a+3) + (2a+2)) - 
      (a^3 - 3a^2 + 2a)\\
& = & (a^3 - a) - 
      (a^3 - 3a^2 + 2a)\\
& = & 3a^2 - 3a
\end{array}
\]

Let us test this result.
We first apply $x^{(3)}$ on a sequence
and compute the differences:
\ensuremath{\Varid{diffs}\;(\Varid{mapply}\;(\Varid{facpoly}\;\mathrm{3})\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{11}\mskip1.5mu]}.
From this we get

0, 0, 6, 18, 36, 60, 90, 126, 168, 216, 270.

Now we apply the polynomial $3x^2 - 3x$
on the same sequence (minus one,
because \ensuremath{\Varid{diffs}} has one element less
than the sequence it is applied to): 
\ensuremath{\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathbin{-}\mathrm{3},\mathrm{3}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{10}\mskip1.5mu]}
and get

0, 0, 6, 18, 36, 60, 90, 126, 168, 216, 270.

The same sequence.

But what is so special about the result
$3x^2 - 3x$ in the first place?
Well, we can factor 3 out and get
$3(x^2 - x)$, whose second part is $x^{(2)}$
and whose first part is $n=3$. In other words,
what we see here is that the differences
of $x^{(n)}$ can be computed by
the polynomial $nx^{(n-1)}$ and that formula
is very similar to the concept of the derivative.
Of course, it is not really the derivative,
since the derivative of a polynomial
deals with powers. The derivative 
of the polynomial $x^n$ is,
according to the power rule, $nx^{n-1}$.
We see the same pattern here, but the
exponent is not really an exponent,
but a falling factorial.

A system that establishes a calculus
that follows the same rules as the
\term{infinitesimal} calculus, to which
the derivative belongs, is often called
\term{umbral calculus}. Most typical 
\term{umbral calculi} are systems of computations
based on \term{Bernoulli polynomials} and 
\term{Bernoulli numbers}.
But factorial polynomials, too, establish
an \term{umbral calculus}.

Here is a Haskell function to compute
the umbral derivative of the factorial polynomial
$x^{(n)}$:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{uderivative}\mathbin{::}(\Conid{Integral}\;\Varid{a})\Rightarrow \Varid{a}\to \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{uderivative}\;\Varid{n}\mathrel{=}\Varid{scale}\;\Varid{n}\;(\Varid{facpoly}\;(\Varid{n}\mathbin{-}\mathrm{1})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

But we are moving fast. We have just looked at
one special case, namely the differences of
$x^{(3)}$. To be sure that the equation

\begin{equation}
\Delta_{x^{(n)}} = nx^{(n-1)},
\end{equation}

holds for all factorial polynomials,
\ie\ that the differences of $x^{(n)}$
equal $nx^{(n-1)}$,
we first need to show it for the general case.

To do this, we start as above. We plug in
the ``value'' $a$ and compute the difference
$(a+1)^{(n)} - a^{(n)}$. When we expand the
formula for the falling factorial, we get

\[
\begin{array}{lcclll}
\Delta_{a^{(n)}} & = &   & (a+1) & a(a-1)\dots(a-n+2) & \\
                 &   & - &       & a(a-1)\dots(a-n+2) & (a-n+1)\\
\end{array}
\]

\ignore{
\begin{align*}
\Delta_{a^{(n)}} & = & (a+1) & & & a(a-1)\dots(a-n+2) & \\
                 & - &       & & & a(a-1)\dots(a-n+2) & (a-n+1)\\
\end{align*}
}

On the right-hand side of this equation
we see a middle part that is identical
in both lines, namely $a(a-1)\dots(a-n+2)$,
which is composed of the common factors of
$(a+1)^{(n)}$ and $a^{(n)}$.

We zoom out to get a better overview of the equation
by setting $b=a(a-1)\dots(a-n+2)$ and obtain:

\begin{equation}
  \Delta_{a^{(n)}} = (a+1)b - (a-n+1)b.
\end{equation}

By regrouping, we get $(a+1-a+n-1)b$.
In the sum, we have $a$ and $-a$ as well as 1 and $-1$.
These terms, hence, cancel out and we are left with
$\Delta_{a^{(n)}} = nb$.
But $b$ is $a(a-1)\dots(a-n+2)$, \ie\ the same
as the second line, but with one factor removed, namely
$(a-n+1)$. That, however, is $a^{(n-1)}$ and, thus,
we have

\begin{equation}
  \Delta_{a^{(n)}} = na^{(n-1)}.\qed
\end{equation}

This rule can be used to provide an elegant proof
for Pascal's rule, which, as you may remember,
states that

\begin{equation}
\binom{k+1}{n+1} = \binom{k}{n+1} + \binom{k}{n}.
\end{equation}

We start by subtracting $\binom{k}{n+1}$
from both sides, obtaining

\begin{equation}
\binom{k+1}{n+1} - \binom{k}{n+1} = \binom{k}{n}.
\end{equation}

This corresponds to

\begin{equation}
\frac{(k+1)^{(n+1)}}{(n+1)!} - \frac{k^{(n+1)}}{(n+1)!} = \binom{k}{n}.
\end{equation}

When we join the fractions on the left-hand side,
we get in the numerator the formula to compute 
the differences of $k^{(n+1)}$:

\[
\frac{(k+1)^{(n+1)} - k^{(n+1)}}{(n+1)!} =
\frac{\Delta_{k^{(n+1)}}}{(n+1)!}.
\]

We have shown that $\Delta_{k^{(n+1)}} = (n+1)k^{(n)}$.
If we substitute this back into the original equation,
we see

\begin{equation}
\binom{k+1}{n+1} - \binom{k}{n+1} =
\frac{(n+1){k^{(n)}}}{(n+1)!}.
\end{equation}

We now see in the fraction on the right-hand side 
that there is one factor
that appears in numerator and denominator,
namely $n+1$. When we cancel $n+1$ out
we need to
reduce $(n+1)!$ in the denominator by this factor.
$(n+1)!$, however, is $(n+1)n!$.
We therefore get:

\begin{equation}
\binom{k+1}{n+1} - \binom{k}{n+1} =
\frac{{k^{(n)}}}{n!} = 
\binom{k}{n}.\qed
\end{equation}

The difference between $x^{(n)}$ and $x^n$ is,
as already stated above, that the former is
a falling factorial, while the latter is
a power. Those are distinct concepts.
For instance, $x^2$ is $xx$, while
$x^{(2)}$ is $x(x-1)$. The falling factorial of $n$,
hence, is smaller than the corresponding power $n$.
We can even say precisely how much smaller it is.
We just have to look at the list of factorial
polynomials we have created above:

\begin{equation}
x^{(2)} = x(x-1) = x^2 - x.
\end{equation}

So, we could express $x^2$ as
$x^{(2)} + x$ adding the part that
we subtract from $x^2$ to get $x^{(2)}$.
If we wanted to express $x^2$ strictly
in terms of falling factorials, we could say: 

\begin{equation}
x^2 = x^{(2)} + x^{(1)}.
\end{equation}

With the same technique, we can establish 
what $x^3$ is in terms of factorial polynomials.
Since

\begin{equation}
x^{(3)} = x^3 - 3x^2 + 2x,
\end{equation}

we have

\begin{equation}
x^3 = x^{(3)} + 3x^2 - 2x.
\end{equation}

Using the previous result, we arrive at

\begin{equation}
x^3 = x^{(3)} + 3(x^{(2)} + x^{(1)}) - 2x^{(1)} =
      x^{(3)} + 3x^{(2)} + x^{(1)}.
\end{equation}

For $x^4$, we have

\begin{equation}
x^4 = x^{(4)} + 6x^3 - 11x^2 + 6x 
\end{equation}

and, hence,

\[
\begin{array}{lcl}
x^4 & = & x^{(4)} + 6(x^{(3)} + 3x^{(2)} + x^{(1)}) - 11(x^{(2)} + x^{(1)}) + 6x^{(1)} \\
    & = & x^{(4)} + 6x^{(3)} + 7x^{(2)} + x^{(1)}.
\end{array}
\]

In this way, we can go on and create formulas for all powers
(and, once we have shown that we can express powers by
factorial polynomials, we can show that we can represent
polynomials as factorial polynomials).
We can even show that each power has a unique representation
as sum of factorial polynomials, just as each number
has a unique representation as product of prime numbers.

To prove this, suppose that, for a power $x^n$,
there were two different representations as sums of
factorial polynomials, such that

\begin{equation}
\begin{array}{lcl}
x^n & = & A_1x^{(1)} + A_2x^{(2)} + \dots + A_nx^{(n)}\\
    & = & B_1x^{(1)} + B_2x^{(2)} + \dots + B_nx^{(n)}.
\end{array}
\end{equation}

When we subtract one representation from the other,
the result shall be zero, since both represent the
same value $x^n$. So, we have:

\begin{equation}
A_1x^{(1)} + A_2x^{(2)} + \dots + A_nx^{(n)} -
B_1x^{(1)} + B_2x^{(2)} + \dots + B_nx^{(n)} = 0.
\end{equation}

Regrouping we get

\begin{equation}
(A_1-B_1)x^{(1)} + (A_2-B_2)x^{(2)} + \dots + (A_n-B_n)x^{(n)} = 0.
\end{equation}

There are two ways for a sum to become zero.
Either all terms are zero or, for each term
that is not zero, the additive inverse
appears somewhere in the sum, either as
one term or as the sum of several terms.
But we are here dealing with a polynomial.
Whether a term is the additive inverse of another
depends on the value we fill in for $x$.
But the formula requires that the left-hand side
is zero for any value we may fill in for $x$.
We are therefore left with the first option:
all terms must be zero.

We look at the term $(A_n-B_n)x^{(n)}$.
When does this expression become zero?
Either when $x^{(n)}$ becomes zero or 
when $(A_n-B_n)$ becomes zero.
We know exactly when $x^{(n)}$ is zero
and when it is not, namely when
$x=0, x=1, x=2, \dots, x=n-1$.
In any other case, it is not zero.
In those cases, $(A_n-B_n)$ must be zero
to make the term zero. But that can only happen,
if $A_n = B_n$.
For this term, the first and the second
representations must therefore be equal.
We, thus, have to remove this term from
the list of candidates of terms
that make the two representations distinct.

We go on to the last but one term and
apply the same reasoning.
In consequence, we need to remove it
from the list of candidates too.
We go on to the next term and apply
the reasoning once again.
This way, we remove one term after the other,
until there are no more terms left.
That shows that the two representations are equal.\qed

We have proved that powers can be represented
uniquely by factorial polynomials. Here is a list
of such representations starting from $x^1$:

\begin{center}
\begin{tabular}{c}
$x^{(1)}$ \\
$x^{(2)} + x^{(1)}$ \\
$x^{(3)} + 3x^{(2)} + x^{(1)}$\\
$x^{(4)} + 6x^{(3)} + 7x^{(2)} + x^{(1)}$\\
$x^{(5)} + 10x^{(4)} + 25x^{(3)} +15x^{(2)} + x^{(1)}$ \\
$x^{(6)} + 15x^{(5)} + 65x^{(4)} + 90x^{(3)} + 31x^{(2)} + x^{(1)}$ \\
$x^{(7)} + 21x^{(6)} + 140x^{(5)} + 350x^{(4)} + 301x^{(3)} + 63x^{(2)} + x^{(1)}$  
\end{tabular}
\end{center}

Those of you who still suffer from triangle paranoia:
you have probably realised that this is already the second
triangle appearing in this section.
When you scroll back to certain triangle-intense chapters,
you will recognise the coefficients above as 
\term{Stirling numbers of the second kind}.
Of course the table above is inverted, because we start
with the largest $k$ in $x^{(k)}$ going down to $k=1$,
while the triangle for the Stirling numbers shows
the coefficients in the order 
$\stirlingTwo{n}{1} \dots \stirlingTwo{n}{n}$.
As a reminder, here they are:

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
1 &   &   &   &   &    &    &    &     &     &   1 &     &     &    &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &    &    &     &   1 &     &   1 &     &    &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &    &    &   1 &     &   3 &     &   1 &    &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &    &  1 &     &   7 &     &   6 &     &  1 &    &    &   &   &   &   &  \\
5 &   &   &   &   &    &  1 &    &  15 &     &  25 &     &  10 &    &  1 &    &   &   &   &   &  \\
6 &   &   &   &   &  1 &    & 31 &     &  90 &     &  65 &     & 15 &    &  1 &   &   &   &   &  \\   
7 &   &   &   & 1 &    & 63 &    & 301 &     & 350 &     & 140 &    & 21 &    & 1 &   &   &   &  
\end{tabular}

Well, we see for some cases that the coefficients
of the factorial polynomials that sum up to powers are
Stirling numbers. Can we prove it?

Let's give it a try with a proof by induction.
Any of the examples above serves as base case that shows that

\begin{equation}
x^n = \stirlingTwo{n}{n}x^{(n)} + \stirlingTwo{n}{n-1}x^{(n-1)} + \dots + \stirlingTwo{n}{1}x^{(1)}.
\end{equation}

We need to show that, if this equation holds for $x^n$, it holds for $x^{n+1}$ that

\begin{equation}
x^{n+1} = \stirlingTwo{n+1}{n+1}x^{(n+1)}
          \stirlingTwo{n+1}{n}x^{(n)} + \dots + 
          \stirlingTwo{n+1}{1}x^{(1)}
\end{equation}

We start with the base case and multiply $x$ on both sides.
On the left-hand side, we get $x^{n+1}$. But what do we get
on the right-hand side?
Well, for each term $x^{(k)}$, we get $xx^{(k)}$.
We have never really thought about what the result of $xx^{(k)}$ is.
We only know that $(x-k)x^{(k)} = x^{(k+1)}$.
So, let us stick to what we know and try to get it in.
A simple way is to express $x$ as an expression with a cameo of $x-k$,
for instance: $x = x-k+k$.
With this expression, we have $(x-k+k)x^{(k)}$.
We distribute $x^{(k)}$ over the sum and get

\[
(x-k)x^{(k)} + kx^{(k)} = x^{(k+1)} + kx^{(k)}.
\]

On the right-hand side, we, hence, get such a sum for each term:

\[
\stirlingTwo{n+1}{n+1}\left(x^{(n+1)} + nx^{(n)}\right) + 
\stirlingTwo{n+1}{n}\left(x^{(n)} + (n-1)x^{(n-1)}\right) + \dots + 
\stirlingTwo{n+1}{1}\left(x^{(2)} + 1 x^{(1)}\right)
\]

We can now regroup the terms, so that the elements with equal
``exponents'' appear together. This yields pairs composed
of the $x^{(k)}$ that was already there and the new one
that we generated by multiplying by $x$:

\[
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{array}{lc}
\stirlingTwo{n}{n}x^{(n+1)} & + \\
n\stirlingTwo{n}{n}x^{(n)}  + 
\stirlingTwo{n}{n-1}x^{(n)} & + \\
(n-1)\stirlingTwo{n}{n-1}x^{(n-1)} + 
\stirlingTwo{n}{n-2}x^{(n-1)} & + \\
\dots & + \\
\stirlingTwo{n}{1}x^{(1)} &
\end{array}
\endgroup
\]

\ignore{
\begin{align*}
\stirlingTwo{n}{n}x^{(n+1)} & + \\
n\stirlingTwo{n}{n}x^{(n)}  + 
\stirlingTwo{n}{n-1}x^{(n)} & + \\
(n-1)\stirlingTwo{n}{n-1}x^{(n-1)} + 
\stirlingTwo{n}{n-2}x^{(n-1)} & + \\
\dots & + \\
\stirlingTwo{n}{1}x^{(1)} &
\end{align*}
}

We regroup a bit more, in particular, we
factor $x^{(k)}$ out, so that we obtain
factors that consist only of expressions containing
Stirling numbers in front of the $x$es:

\[
\begingroup
\renewcommand{\arraystretch}{2}
\begin{array}{rlc}
\stirlingTwo{n}{n} & x^{(n+1)} & + \\
\left(n\stirlingTwo{n}{n} + \stirlingTwo{n}{n-1}\right) & x^{(n)} & + \\
\left((n-1)\stirlingTwo{n}{n-1} + \stirlingTwo{n}{n-2}\right) & x^{(n-1)} & + \\ 
\dots & & + \\
\left(2\stirlingTwo{n}{2} + \stirlingTwo{n}{1}\right) & x^{2} & + \\ 
\stirlingTwo{n}{1} & x^{(1)} &
\end{array}
\endgroup
\]

\ignore{
\begin{align*}
\stirlingTwo{n}{n}x^{(n+1)} & + \\
\left(n\stirlingTwo{n}{n} + \stirlingTwo{n}{n-1}\right)x^{(n)} & + \\
\left((n-1)\stirlingTwo{n}{n-1} + \stirlingTwo{n}{n-2}\right)x^{(n-1)} & + \\ 
\dots & + \\
\stirlingTwo{n}{1}x^{(1)} &
\end{align*}
}

You might remember the identity

\begin{equation}
\stirlingTwo{n+1}{k+1} = k\stirlingTwo{n}{k+1} + \stirlingTwo{n}{k},
\end{equation}

which is ``Pascal's rule'' for Stirling numbers of the second kind.
This is exactly what we see in each group! Compare the factors
in front of the first Stirling number that read $n$, $n-1$ and so on
with what you see in the Stirling number in the place of $k$ (\ie\ in the bottom).
For instance, in the formula

\[
\left((n-1)\stirlingTwo{n}{n-1} + \stirlingTwo{n}{n-2}\right)x^{(n-1)}
\]

we have $k = n-1$.

Now, all terms that show this pattern,
can be simplified to

\[
\stirlingTwo{n+1}{k+1}
\]

leaving only the first and the last term.
But since the first and the last are $\stirlingTwo{n}{n}$ and
$\stirlingTwo{n}{1}$ respectively, which are both just 1,
that is not a problem. We get as desired

\begin{equation}
x^{n+1} = \stirlingTwo{n+1}{n+1}x^{(n+1)} +
          \stirlingTwo{n+1}{n}x^{(n)} + \dots + 
          \stirlingTwo{n+1}{1}x^{(1)}\qed
\end{equation}

and that completes the proof.

The following function exploits Stirling numbers
to compute powers by means of factorial polynomials:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{stirpow}\mathbin{::}\Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{stirpow}\;\Varid{n}\mathrel{=}\Varid{sump}\;[\mskip1.5mu \Varid{scale}\;(\Varid{\Conid{Perm}.stirling2}\;\Varid{n}\;\Varid{k})\;(\Varid{facpoly}\;\Varid{k})\mid \Varid{k}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

This is a lame function, of course.
Powers are not difficult to compute at all,
so why using factorial polynomials in the first place?
More interesting, at least from theoretical perspective,
is the opposite function that, for a given power,
shows the factorial polynomials and the coefficients that
indicate how often each factorial polynomial appears:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{fpPowTerms}\mathbin{::}\Conid{Natural}\to [\mskip1.5mu (\Conid{Natural},\Conid{Poly}\;\Conid{Natural})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{fpPowTerms}\;\mathrm{0}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}[\mskip1.5mu (\mathrm{1},\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu])\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{fpPowTerms}\;\Varid{n}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}[\mskip1.5mu (\Varid{\Conid{Perm}.stirling2}\;\Varid{n}\;\Varid{k},\Varid{facpoly}\;\Varid{k})\mid \Varid{k}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function, just like the previous one,
makes use of the \ensuremath{\Varid{stirling2}} function that we defined
in the first chapter and so we are obliged to use the concrete type
\ensuremath{\Conid{Natural}}.

Here is a function to test the results:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{sumFpPolyTerms}\mathbin{::}[\mskip1.5mu (\Conid{Integer},\Conid{Poly}\;\Conid{Integer})\mskip1.5mu]\to \Conid{Poly}\;\Conid{Integer}{}\<[E]%
\\
\>[3]{}\Varid{sumFpPolyTerms}\mathrel{=}\Varid{sump}\mathbin{\circ}\Varid{map}\;(\Varid{uncurry}\;\Varid{scale}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function, basically, just sums up the list
we pass in scaling the polynomials by their coefficient.
Here is a test for the first 7 powers,\\
\ensuremath{\Varid{map}\;(\Varid{sumFpPolyTerms}\mathbin{\circ}\Varid{fpPowTerms})\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{6}\mskip1.5mu]}:

\begin{minipage}{\textwidth}
\begin{center}
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}
\end{center}
\end{minipage}

Once we can represent powers by factorial polynomials,
we are able to represent any polynomial by factorial polynomials,
since polynomials are just sums of scaled powers of $x$.
Here is a function that does that:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{58}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{fpPolyTerms}\mathbin{::}\Conid{Poly}\;\Conid{Natural}\to [\mskip1.5mu (\Conid{Natural},\Conid{Poly}\;\Conid{Natural})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{fpPolyTerms}\;(\Conid{P}\;\Varid{cs}){}\<[23]%
\>[23]{}\mathrel{=}[\mskip1.5mu \Varid{foldl}\;\Varid{ab}\;\Varid{p0}\;\Varid{p}\mid \Varid{p}\leftarrow \Varid{p2}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{p0}{}\<[23]%
\>[23]{}\mathrel{=}(\mathrm{0},\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]){}\<[E]%
\\
\>[12]{}\Varid{p1}{}\<[23]%
\>[23]{}\mathrel{=}\Varid{concat}\;[\mskip1.5mu \Varid{map}\;(\Varid{s}\;\Varid{c})\;(\Varid{fpPowTerms}\;\Varid{k})\mid (\Varid{c},\Varid{k})\leftarrow \Varid{zip}\;\Varid{cs}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{p2}{}\<[23]%
\>[23]{}\mathrel{=}\Varid{groupBy}\;((\equiv )\mathbin{`\Varid{on}`}\Varid{snd})\;(\Varid{sortOn}\;{}\<[58]%
\>[58]{}(\Varid{degree}\mathbin{\circ}\Varid{snd})\;\Varid{p1}){}\<[E]%
\\
\>[12]{}\Varid{ab}\;\Varid{a}\;\Varid{b}{}\<[23]%
\>[23]{}\mathrel{=}(\Varid{fst}\;\Varid{a}\mathbin{+}\Varid{fst}\;\Varid{b},\Varid{snd}\;\Varid{b}){}\<[E]%
\\
\>[12]{}\Varid{s}\;\Varid{c}\;(\Varid{n},\Varid{p}){}\<[23]%
\>[23]{}\mathrel{=}(\Varid{c}\mathbin{*}\Varid{n},\Varid{p}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function looks a bit confusing on the first sight.
It is not too horrible, though.
We start by computing $p1$.
We apply \ensuremath{\Varid{fpPowTerms}} on the exponents
of the original polynomial (\ensuremath{[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]})
and multiply the 
coefficients of the original 
(\ensuremath{\Varid{cs}})
and the coefficients
that tell us how often each factorial polynomial
occurs in the respective power.
The latter is done by function $s$ which is mapped
on the result of \ensuremath{\Varid{fpPowTerms}}.
The result is a list of lists of pairs $(n,p)$,
where $n$ is a \ensuremath{\Conid{Natural}} and $p$ a polynomial.
We concat this list, so we obtain a flat list
of polynomials.

In the next step, we compute $p2$ by sorting
and then grouping
this flat list by the degree of the polynomials.
The result is a list of lists of polynomials 
of equal degree with
differing coefficients.

In the final step we sum the coefficients of
each such groups starting with zero
\ensuremath{\Varid{p0}\mathrel{=}(\mathrm{0},\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu])}. 

We test this function by factoring
arbitrary polynomials into their terms and
summing the result together again:

\begin{minipage}{\textwidth}
\ensuremath{\Varid{sumFpPolyTerms}\;(\Varid{fpPolyTerms}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]))}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{sumFpPolyTerms}\;(\Varid{fpPolyTerms}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]))}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{sumFpPolyTerms}\;(\Varid{fpPolyTerms}\;(\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{4},\mathrm{3},\mathrm{2},\mathrm{1}\mskip1.5mu]))}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{4},\mathrm{3},\mathrm{2},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Varid{sumFpPolyTerms}\;(\Varid{fpPolyTerms}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]))}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]}
\end{minipage}

In the next experiment we retrieve the coefficients
for polynomials of the form 

\[
x^n + x^{n-1} + \dots + 1,
\]

\ie\ polynomials with all coefficient equal to 1.

We apply
\ensuremath{\Varid{map}\;(\Varid{map}\;\Varid{fst}\mathbin{\circ}\Varid{fpPolyTerms})}
to the first 7 polynomials of that form,
\ie\ $1$, $x + 1$, $x^2 + x + 1$ and so on
and get

\begin{minipage}{\textwidth}
\begin{center}
\ensuremath{[\mskip1.5mu \mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{4},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{4},\mathrm{11},\mathrm{7},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{26},\mathrm{32},\mathrm{11},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{6},\mathrm{57},\mathrm{122},\mathrm{76},\mathrm{16},\mathrm{1}\mskip1.5mu]}
\end{center}
\end{minipage}

This again is a triangle and it is the simplest
that we can obtain this way, since the input
coefficients are all 1.
One could think that other polynomials could now
be generated by means of these coefficients
just multiplying the coeficients of the polynomial
with these ones.
Unfortunately, that is too simple.
The coefficients here indicate only
how often each factorial polynomial appears
in the respective polynomial;
they are not coefficients of that polynomial
(which are all 1 anyway).

The sequence as such is the result of a matrix
multiplication (a topic we will study soon) with
one matrix being a lower-left triangle of ones
and the other a lower-left triangle containing
the Stirling numbers of the second kind:

\begin{equation}
\begin{pmatrix}
1 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 \\
1 & 1 & 1 & 0 & 0 \\
1 & 1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1 & 1 
\end{pmatrix}
\times
\begin{pmatrix}
1 &  0 &  0 &  0 &  0 \\
1 &  1 &  0 &  0 &  0 \\
1 &  3 &  1 &  0 &  0 \\
1 &  7 &  6 &  1 &  0 \\
1 & 15 & 25 & 10 &  1 
\end{pmatrix}
=
\begin{pmatrix}
1 &  0 &  0 &  0 &  0 \\
2 &  1 &  0 &  0 &  0 \\
3 &  4 &  1 &  0 &  0 \\
4 & 11 &  7 &  1 &  0 \\
5 & 26 & 32 & 11 &  1 
\end{pmatrix}
\end{equation}

\ignore{
with the first column of the triangle presented above missing.
why is it missing???
}

Meanwhile, you may have guessed or even verified
that the coefficients of factorial polynomials,
those appearing in the very first triangle in
this section are Stirling numbers of the first kind.
But they are special: some are negative.
Indeed, there are two variants of
the Stirling numbers of the first kind:
signed and unsigned. Since we were discussing
combinatorial problems related to permutations,
when we first introduced Stirling numbers,
we did not consider the signed variety.
Here are the signed Stirling numbers 
of the first kind:

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
1 &   &   &   &   &    &      &      &       &     &   1 &      &      &     &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &      &      &       &  -1 &     &    1 &      &     &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &      &      &     2 &     &  -3 &      &    1 &     &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &      &   -6 &       &  11 &     &   -6 &      &   1 &    &    &   &   &   &   &  \\
5 &   &   &   &   &    &   24 &      &   -50 &     &  35 &      &  -10 &     &  1 &    &   &   &   &   &  \\
6 &   &   &   &   &-120&      &  274 &       &-225 &     &   85 &      & -15 &    &  1 &   &   &   &   &  \\   
7 &   &   &   &720&    & -1764&      &  1624 &     &-735 &      &  175 &     &-21 &    & 1 &   &   &   &  
\end{tabular}

The recursive formula to compute these numbers is

\begin{equation}
\stirlingOne{n+1}{k+1} = -n\stirlingOne{n}{k+1} + \stirlingOne{n}{k}.
\end{equation}

Note that, when the first Stirling number, $\stirlingOne{n}{k}$,
on the right-hand side is positive,
then the second, $\stirlingOne{n}{k-1}$, 
is negative. Since we multiply the first by a negative
number, the first term becomes positive, when the Stirling number is
negative and negative otherwise. Therefore, both terms are either
negative or positive and the absolute value of the whole expression
does not change compared
to the unsigned Stirling number.

So, can we prove that the coefficients of factorial polynomials
are Stirling numbers of the first kind?
We prove by induction with any of the above given polynomials
as base case

\begin{equation}\label{eq:stir1fac1}
x^{(n)} = \stirlingOne{n}{n}x^n + 
          \stirlingOne{n}{n-1}x^{n-1} + \dots + 
          \stirlingOne{n}{1}x,
\end{equation}

where the Stirling numbers, here, are to be understood as signed.

We need to prove that, if that equation holds, 
then the following holds as well:

\begin{equation}\label{eq:stir1fac2}
x^{(n+1)} = \stirlingOne{n+1}{n+1}x^{n+1} + 
            \stirlingOne{n+1}{n}x^{n} + \dots + 
            \stirlingOne{n+1}{1}x.
\end{equation}

We start with the observation that

\begin{equation}
x^{(n+1)} = (x-n)x^{(n)}.
\end{equation}

So, we can go from \ref{eq:stir1fac1} to \ref{eq:stir1fac2}
by multiplying both sides of \ref{eq:stir1fac1} by $x-n$.
The right-hand side would then become:

\[
(x-n)\stirlingOne{n}{n}x^n + 
(x-n)\stirlingOne{n}{n-1}x^{n-1} +
      \dots + 
(x-n)\stirlingOne{n}{1}x.
\]

For each term, we distribute the factors over the sum $x-n$:

\[
\stirlingOne{n}{n}x^{n+1} - n\stirlingOne{n}{n}x^n        + \\
\stirlingOne{n}{n-1}x^n   - n\stirlingOne{n}{n-1}x^{n-1}  + \\
\dots                                                     + \\
\stirlingOne{n}{1}x^2     - n\stirlingOne{n}{1}x         
\]

and regroup so that we get pairs of terms 
with equal $x$es:

\[
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{array}{lclcclc}
  &\stirlingOne{n}{n}  &x^{n+1}& &                    &       & + \\
-n&\stirlingOne{n}{n}  &x^n    &+&\stirlingOne{n}{n-1}&x^n    & + \\
-n&\stirlingOne{n}{n-1}&x^{n-1}&+&\stirlingOne{n}{n-2}&x^{n-1}& + \\
  & \dots              &       & &                    &       & + \\
-n&\stirlingOne{n}{2}  &x^{2}  &+&\stirlingOne{n}{1}  &x^{2}  & + \\
-n&\stirlingOne{n}{1}  &x      & &                    &       & 
\end{array}
\endgroup
\]

When we factor the $x$es out again, we get

\[
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{array}{crlc}
  &\stirlingOne{n}{n}                                      &x^{n+1}& + \\
  &\left(-n\stirlingOne{n}{n} + \stirlingOne{n}{n-1}\right)&x^n    & + \\
  &\left(-n\stirlingOne{n}{n-1}+\stirlingOne{n}{n-2}\right)&x^{n-1}& + \\
  & \dots                                                  &       & + \\
  &\left(-n\stirlingOne{n}{2} + \stirlingOne{n}{1}\right)  &x^{2}  & + \\
  &-n\stirlingOne{n}{1}                                    &x      & 
\end{array}
\endgroup
\]

In each line but the first and the last, we now have
the formula to compute $\stirlingOne{n+1}{k+1}$ and
can simplify all these lines accordingly:

\[
\stirlingOne{n}{n}x^{n+1} + 
\stirlingOne{n+1}{n}x^n  + 
\stirlingOne{n+1}{n-1}x^{n-1} + 
\dots + 
\stirlingOne{n+1}{2}x^{2} 
-n \stirlingOne{n}{1}x
\]

For the first term, the same argument 
we already used before still holds:
$\stirlingOne{n}{n} = \stirlingOne{n+1}{n+1} = 1$.

For the last term, we know that 
$\stirlingOne{n}{1} = \pm(n-1)!$.
In the last term, we hence see the product 
$(-n)(\pm((n-1)!))$, which is $-(\pm(n!))$.
If, for $n$, the factorial was positive,
it will now be negative. If it was negative,
it will now be positive.
This complies with the signed Stirling numbers
of the first kind and completes the proof.$\qed$

What have we learnt in the last sections?
Well, factorial polynomials have coefficients
that count the number of permutations
that can be expressed by a given number of cycles.
When factorial polynomials are used to represent
powers, we need to scale them by factors
that count the number of ways to partition a set
into a given number of distinct subsets.

Furthermore, we can express any polynomial
by combinations of scaled factorial polynomials
and the coefficients of those
are products of the differences and
the binomial coefficients which count
the number of ways to choose $k$ out of $n$.
``The Lord is subtle'' said Einstein,
``but he is not plain mean''.
That is a quantum of solace for feeble minded
like me. Let us go on to see
what is there more to discover.

\ignore{
=> recursive formula:
?> more identities?
?> Taylor's theorem and Taylor's series
}
\section{Roots}
\ignore{
module Roots
where
  import DMachine
}

In the previous sections, we looked at the results,
when applying polynomials to given values.
That is, we applied a polynomial $\pi(x)$ to
a given value (or sequence of values) for $x$ 
and studied the result $y = \pi(x)$.
Now we are turning this around. We will look at
a given $y$ and ask which value $x$ would
create that $y$. In other words, 
we look at polynomials as equations of the form:

\begin{equation}
a_nx^n + a_{n-1}x^{n-1} + \dots + a_0 = a
\end{equation}

and search for ways to solve such equations.
In the focus of this investigation is usually
the special case $a=0$, \ie\:

\begin{equation}
a_nx^n + a_{n-1}x^{n-1} + \dots + a_0 = 0.
\end{equation}

The values for $x$ fulfilling this equation
are called the \term{root}s of the polynomial.
A trivial example is $x^2$, whose root is 0.
A slightly less trivial example is $x^2 - 4$,
whose roots are $x_1 = -2$ and $x_2 = 2$, since

\[
(-2)^2 - 4 = 4 - 4 = 0
\]

and

\[
2^2 - 4 = 4 - 4 = 0.
\]

Note that these examples are polynomials of even degree.
Polynomials of even degree do not need to have any roots.
Since even powers are always positive (or zero), negative values
are turned into positive numbers and, since the term of highest
degree is even, the whole expression may always be positive.
This is true for the polynomial $x^2 + 1$. Since all negative values
are transformed into positive values by $x^2$, the smallest value
that we can reach is the result for $x=0$, which is $0+1=1$.

On the other hand, even polynomials may have negative values,
namely when they have terms with negative coefficients that, 
for smaller numbers,
result in numbers that are greater than those resulting from
the term of highest degree. 
The polynomial $x^2 - 4$, once again, is negative
in the interval $]-2\dots 2[$. It, therefore, must have two roots:
one at -2, where the polynomial results become negative, and the other at 2,
where the polynomial results become positive again.

Odd polynomials, by contrast, usually have negative values, because
the term with the highest degree may result in a negative or a
positive number depending on the signedness of the input value
and that of the coefficient.
The trivial polynomial $x^3$, for instance, is negative for
negative values and positive for positive values. The slightly
less trivial polynomial $x^3 + 27$ has a root at -3, while
$x^3 - 27$ has a root at 3.

In summary, we can say that even polynomials do not necessarily
have negative values and, hence, do not need to have a root.
Odd polynomials, on the other hand, usually have both, negative
and positive values, and, hence, must have a root.

Those are strong claims. They are true, because polynomials
belong to a specific class of \term{functions}, 
namely \term{continuous} functions.
That, basically, means that they have no \emph{holes}, \ie\
for any value $x$ of a certain number type there is a result $y$
of that number type. For instance, when the coefficients of the
polynomial are all integers and the $x$-value is an integer,
then the result is an integer, too. When the polynomial is defined
over a field (all coefficients are part of that field and
the values to which we apply the polynomial lie in that field),
then the result is in that field, too. Rational polynomials,
for instance, have rational results. 
Real polynomials have real results.

Furthermore, the function does not ``jump'', \ie\ the results
grow with the input values -- not necessarily
at the same rate, in fact, for polynomials of degree greater than 1,
the result grows much faster than the input -- but the growth
is regular.

These properties appear to be ``natural'' at the first sight.
But there are functions that do not fulfil these criteria.
In the next chapter, when we properly define the term \term{function},
we will actually see functions with holes and jumps.

The reason that polynomials behave regularily is that we only
use basic arithmetic functions in their definition: we add, multiply
and raise to powers. 
All those operations are closed, \ie\ their results lie
in the same fields as their inputs. 

Furthermore, the form of polynomials guarantees that they develop
in a certain way. For very large numbers (negative or positive), 
it is the term with the greatest exponent, \ie\ the degree 
of the polynomial, that most significantly determines the
outcome, that is, the result for very large numbers
approaches the result for the term with the largest exponent. 
For smaller values, however, the terms of lower degree have
stronger impact. The terms ``large'' and ``small'', here, 
must be understood relative to the coefficients. If the coefficients
are very large, the values to which the polynomial is applied
must be even larger to approach the result for the first term.

There are polynomials with a quite confusing behaviour that
make it hard to guess the roots, for instance, \term{Wilkinson's polynomial}
named for James Hardy Wilkinson (1919 -- 1986), an American mathematician
and computer scientist. The Wilkinson polynomial is defined as

\begin{equation}
w(x) = \prod_{i=1}^{20}{(x-i)}.
\end{equation}

It is thus a factorial polynomial, namely $x^{(21)}$.
We can generate it in terms of our polynomial type as

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{wilkinson}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Enum}\;\Varid{a},\Conid{Show}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{wilkinson}\mathrel{=}\Varid{prodp}\;\Varid{mul}\;[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathbin{-}\Varid{i},\mathrm{1}\mskip1.5mu]\mid \Varid{i}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{20}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

It looks like this:

\begin{minipage}{\textwidth}
\ensuremath{\Conid{P}\;[\mskip1.5mu }\\
\ensuremath{\mathrm{2432902008176640000},\mathbin{-}\mathrm{8752948036761600000},\mathrm{13803759753640704000},}\\
\ensuremath{\mathbin{-}\mathrm{12870931245150988800},\mathrm{8037811822645051776},\mathbin{-}\mathrm{3599979517947607200},}\\
\ensuremath{\mathrm{1206647803780373360},\mathbin{-}\mathrm{311333643161390640},\mathrm{63030812099294896},}\\
\ensuremath{\mathbin{-}\mathrm{10142299865511450},\mathrm{1307535010540395},\mathbin{-}\mathrm{135585182899530},\mathrm{11310276995381},}\\
\ensuremath{\mathbin{-}\mathrm{756111184500},\mathrm{40171771630},\mathbin{-}\mathrm{1672280820},\mathrm{53327946},\mathbin{-}\mathrm{1256850},\mathrm{20615},\mathbin{-}\mathrm{210},\mathrm{1}\mskip1.5mu]}
\end{minipage}

The first terms are

\[
x^{20} - 210x^{19} + 20615x^{18} - 1256850x^{17} \dots
\]

When we apply Wilkinson's polynomial to the integers $1\dots 25$, we see:

\ensuremath{\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{0},\mathrm{2432902008176640000},}\\
\ensuremath{\mathrm{51090942171709440000},\mathrm{562000363888803840000},\mathrm{4308669456480829440000},}\\
\ensuremath{\mathrm{25852016738884976640000}},

which looks very confusing. When we try non-integers, we see

\ensuremath{\Varid{apply}\;\Varid{wilkinson}\;\mathrm{0.9}} is $1.7213\dots$\\
\ensuremath{\Varid{apply}\;\Varid{wilkinson}\;\mathrm{1.1}} is $-8.4600\dots$\\
\ensuremath{\Varid{apply}\;\Varid{wilkinson}\;\mathrm{1.9}} is $-8.1111\dots$\\
\ensuremath{\Varid{apply}\;\Varid{wilkinson}\;\mathrm{2.1}} is $4.9238\dots$\\

As we see, the results switch sign at the integers or,
more precisely, at the integers in the interval $[1\dots 20]$,
which are the roots of Wilkinson's polynomial.
Looking at the factors of the polynomial

\[
(x-1)(x-2)\dots (x-20),
\]

this result is much less surprising, since, obviously,
when any of these factors becomes 0, then the whole
expression becomes 0. So, for the value $x=3$, we would have

\[
2 \times 1 \times 0 \times \dots \times -17 = 0.
\]

When we look at the coefficients, however,
the results
look quite irregular and, on the first sight,
completely unrelated.
When we say that polynomials show a regular behaviour,
that must be taken with a grain of salt.
Anyway, that they behave like this 
gives rise to a number of simple
methods to find roots based on approximation,
at least when we start with a fair guess,
which requires some knowledge about the rough shape
of the polynomial in the first place.

These methods can be split into two major groups:
\term{bracketing} methods and \term{open} methods.
Bracketing methods start with two distinct values
somewhere on the ``left'' and the ``right'' of
the root. Bracketing methods, hence, require a
pre-knowledge about where, more or less, a root
is located. We then choose two values that limit
this interval on the lower and on the upper side.

The simplest variant of bracketing is the \term{bisect}
algorithm. It is very similar to Heron's method
to find the square root of a given number.
We start with two values $a$ and $b$ and, on each step,
we compute the average $(a+b)/2$ and substitute
either $a$ or $b$ by this value depending on the side
the value is located relative to the root.
Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{47}{@{}>{\hspre}c<{\hspost}@{}}%
\column{47E}{@{}l@{}}%
\column{50}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{bisect}\mathbin{::}{}\<[14]%
\>[14]{}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Ord}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Show}\;\Varid{a}){}\<[E]%
\\
\>[14]{}\Rightarrow \Conid{Poly}\;\Varid{a}\to \Varid{a}\to \Varid{a}\to \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{bisect}\;\Varid{p}\;\Varid{t}\;\Varid{a}\;\Varid{b}{}\<[19]%
\>[19]{}\mid {}\<[22]%
\>[22]{}\Varid{abs}\;\Varid{fc}\mathbin{<}\Varid{abs}\;\Varid{t}{}\<[47]%
\>[47]{}\mathrel{=}{}\<[47E]%
\>[50]{}\Varid{c}{}\<[E]%
\\
\>[19]{}\mid {}\<[22]%
\>[22]{}\Varid{signum}\;\Varid{fc}\equiv \Varid{signum}\;\Varid{fa}{}\<[47]%
\>[47]{}\mathrel{=}{}\<[47E]%
\>[50]{}\Varid{bisect}\;\Varid{p}\;\Varid{t}\;\Varid{c}\;\Varid{b}{}\<[E]%
\\
\>[19]{}\mid {}\<[22]%
\>[22]{}\Varid{otherwise}{}\<[47]%
\>[47]{}\mathrel{=}{}\<[47E]%
\>[50]{}\Varid{bisect}\;\Varid{p}\;\Varid{t}\;\Varid{a}\;\Varid{c}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{fa}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{apply}\;\Varid{p}\;\Varid{a}{}\<[E]%
\\
\>[12]{}\Varid{fb}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{apply}\;\Varid{p}\;\Varid{b}{}\<[E]%
\\
\>[12]{}\Varid{fc}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{apply}\;\Varid{p}\;\Varid{c}{}\<[E]%
\\
\>[12]{}\Varid{c}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}(\Varid{a}\mathbin{+}\Varid{b})\mathbin{/}\mathrm{2}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives four arguments.
The first is the polynomial.
The second is a tolerance.
When, on applying the polynomial,
we get a result that is smaller
than the tolerance, we return the obtained $x$ value.
$a$ and $b$ are the starting values.

\begin{minipage}{\textwidth}
We distinguish three cases:
\begin{itemize}
\item The result for the new value, $c$, 
      is below the tolerance threshold.
      In this case, $c$ is sufficiently close
      to the root and we yield this value.
\item the sign of the result for the new value
      equals the sign of $a$. Then we replace
      $a$ by $c$.
\item the sign of the result for the new value
      equals the sign of $b$. In this case,
      we replace $b$ by $c$.
\end{itemize}
\end{minipage}

We try \ensuremath{\Varid{bisect}} on the polynomial $x^2$ with
the initial guess $a=-1$ and $b=1$ (because we
assume that the root should be close to 0) and
a tolerance of 0.1:

\ensuremath{\Varid{bisect}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])\;\mathrm{0.1}\;(\mathbin{-}\mathrm{1})\;\mathrm{1}}

and see the correct result \ensuremath{\mathrm{0.0}}.

For the polynomial $x^2 - 4$, which has two roots,
we try 

\ensuremath{\Varid{bisect}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])\;\mathrm{0.1}\;(\mathbin{-}\mathrm{3})\;(\mathbin{-}\mathrm{1})},

which yields \ensuremath{\mathbin{-}\mathrm{2}} and 

\ensuremath{\Varid{bisect}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])\;\mathrm{0.1}\;\mathrm{1}\;\mathrm{3}},

which yields \ensuremath{\mathrm{2}}.

With Wilkinson's polynomial, however,
we get a surprise:

\ensuremath{\Varid{bisect}\;\Varid{wilkinson}\;\mathrm{0.1}\;\mathrm{0.5}\;\mathrm{1.5}},

for which we expect to find the root 1.
But the function does not return.
Indeed, when we try \ensuremath{\Varid{apply}\;\Varid{wilkinson}\;\mathrm{1.0}}, we see

\ensuremath{\mathrm{1148.0}},

a somewhat surprising result.
Wilkinson used this polynomial to demonstrate
the sensivity of coefficients to small differences
in the input values. Using Haskell real numbers,
The computation leads to a loss of precision
in representing the terms. Indeed, considering
terms raised to the $20^{th}$ power and multiplied
by large coefficients, the number 1148 appears to
be a tiny imprecision.

We can work around this, using rational numbers:

\ensuremath{\Varid{apply}\;\Varid{wilkinson}\;(\mathrm{1}\mathbin{\%}\mathrm{1})}

gives without any surprise \ensuremath{\mathrm{0}\mathbin{\%}\mathrm{1}}.
So, we try

\ensuremath{\Varid{bisect}\;\Varid{wilkinson}\;(\mathrm{1}\mathbin{\%}\mathrm{10})\;(\mathrm{1}\mathbin{\%}\mathrm{2})\;(\mathrm{3}\mathbin{\%}\mathrm{2})}

and get the correct result \ensuremath{\mathrm{1}\mathbin{\%}\mathrm{1}}.
The function with this parameters
returns almost instantly. That is
because the average of 0.5 and 1.5 is already 1.
The function finds the root in the first step.
A more serious challenge is

\ensuremath{\Varid{bisect}\;\Varid{wilkinson}\;(\mathrm{1}\mathbin{\%}\mathrm{10})\;(\mathrm{1}\mathbin{\%}\mathrm{3})\;(\mathrm{3}\mathbin{\%}\mathrm{2})},

which needs more than one recursion.
The function, now, runs for a short while and
comes up with the result

\ensuremath{\mathrm{1729382256910270463}\mathbin{\%}\mathrm{1729382256910270464}},

which is pretty close to 1 and, hence,
the correct result.

The most widely known open method is Newton's method,
also called Newton-Raphson method.
It was first developed by Newton in about 1670
and then, in 1690, again by Joseph Raphson.
Newton's version was probably not known to Raphson,
since Newton did not publish his work.
Raphson's version, on the other hand, is
simpler and, therefore, usually preferred.

Anyway, the method starts with only one approximation
and is therefore not a bracketing method.
The approximation is then applied to the polynomial $\pi$
and the derivative of that polynomial, $\pi'$.
Then, the quotient of the results, $\frac{\pi(x)}{\pi'(x)}$ 
is computed and subtracted from the initial guess.
Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}c<{\hspost}@{}}%
\column{21E}{@{}l@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{36}{@{}>{\hspre}c<{\hspost}@{}}%
\column{36E}{@{}l@{}}%
\column{39}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{newguess}\mathbin{::}{}\<[16]%
\>[16]{}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a},\Conid{Ord}\;\Varid{a},\Conid{Enum}\;\Varid{a},\Conid{Fractional}\;\Varid{a}){}\<[E]%
\\
\>[16]{}\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Natural}\to \Varid{a}\to \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{newguess}\;\Varid{p}\;\Varid{m}\;\Varid{t}\;\Varid{a}{}\<[21]%
\>[21]{}\mid {}\<[21E]%
\>[24]{}\Varid{abs}\;\Varid{pa}\mathbin{<}\Varid{t}{}\<[36]%
\>[36]{}\mathrel{=}{}\<[36E]%
\>[39]{}\Varid{a}{}\<[E]%
\\
\>[21]{}\mid {}\<[21E]%
\>[24]{}\Varid{m}\leq \mathrm{0}{}\<[36]%
\>[36]{}\mathrel{=}{}\<[36E]%
\>[39]{}\Varid{a}{}\<[E]%
\\
\>[21]{}\mid {}\<[21E]%
\>[24]{}\Varid{otherwise}{}\<[36]%
\>[36]{}\mathrel{=}{}\<[36E]%
\>[39]{}\Varid{newguess}\;\Varid{p}\;(\Varid{m}\mathbin{-}\mathrm{1})\;\Varid{t}\;(\Varid{a}\mathbin{-}\Varid{pa}\mathbin{/}\Varid{p'a}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{p'}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{derivative}\;(\mathbin{*})\;\Varid{p}{}\<[E]%
\\
\>[12]{}\Varid{pa}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{apply}\;\Varid{p}\;\Varid{a}{}\<[E]%
\\
\>[12]{}\Varid{p'a}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{apply}\;\Varid{p'}\;\Varid{a}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives four parameters.
The polynomial \ensuremath{\Varid{p}}, the natural number \ensuremath{\Varid{m}},
the tolerance \ensuremath{\Varid{t}} and the initial guess \ensuremath{\Varid{a}}.
The natural number \ensuremath{\Varid{m}} is a delimiter.
It is not guaranteed that the value increases
in precision with always more repetitions.
It may get worse at some point.
It is therefore useful to restrict the number
of iterations.

The function terminates when we have 
reached either the intended precision or 
the number of repetitions, $m$.
Otherwise, we repeat with $m-1$ and
$a - \frac{\pi(a)}{\pi'(a)}$.

For the polynomial $x^2 - 4$, we call first

\ensuremath{\Varid{newguess}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])\;\mathrm{10}\;\mathrm{0.1}\;\mathrm{1}}

and get $2.00069\dots$, which is very close
to the known root 2.
For the other root we call

\ensuremath{\Varid{newguess}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])\;\mathrm{10}\;\mathrm{0.1}\;(\mathbin{-}\mathrm{1})}

and get the equally close result $-2.00069\dots$
For the Wilkinson polynomial, we call

\ensuremath{\Varid{newguess}\;\Varid{wilkinson}\;\mathrm{10}\;(\mathrm{0.0001})\;\mathrm{1.5}}

and get $1.99999\dots$, which is very close
to the real root 2. We can further improve
precision by increasing the number of iterations:

\ensuremath{\Varid{newguess}\;\Varid{wilkinson}\;\mathrm{20}\;(\mathrm{0.0001})\;\mathrm{1.5}}

The difference is at the $12^{th}$ decimal digit.

Note that the Newton-Raphson method
is not only more precise (that is: converges earlier
with a good result), but also more robust against
real representation imprecision.

To understand why this method works at all,
we need to better understand what the derivative is.
We will come back to this issue in the next chapter.
In the strict sense, the derivative does not belong
here anyway, since the concept of derivative is
analysis, not algebra. Both kinds of methods,
the bracketing and the open methods, in fact, come
from numerical analysis.
They do not have the ``look and feel'' of algebraic
methods. So, how would an algebraist tackle the
problem of finding the roots of a polynomial?

One possibility is factoring.
Polynomials may be represented as the product
of their factors (just like integers).
We have experienced with Wilkinson's polynomial
that the factor representation may be much more
convenient than the usual representation with
coefficients. Wilkinson's polynomial expressed
as a product was just

\begin{equation}
w(x) = \prod_{i=1}^{20}{(x-i)},
\end{equation}

\ie: $(x-1)(x-2)\dots (x-20)$.

As for all products, when one of the factors
is zero, then the whole product becomes zero.
For the root problem, this means that, 
when we have the factors, we can find a value
for $x$, so that any of the factors becomes zero
and this value is then a root.
Any integer in the range $[1\dots 20]$ would make
one of the factors of Wilkinson's polynomial zero.
The integers $[1\dots 20]$ are therefore the roots
of this polynomial.

Factoring polynomials, however, is an advanced
problem in its own right and we will dedicate
some of the next sections to its study. Anyway, what
algebraists did for centuries was to find
formulas that would yield the roots for any kind
of polynomials. In some cases they succeeded,
in particular for polynomials of degrees less
than 5. For higher degrees, there are no such
formulas. This discovery is perhaps much more
important than the single formulas developed
over the centuries for polynomials of the first
four degrees. In fact, the concepts that led
to the discovery are the foundations of modern
(and postmodern) algebra. 

But first things first. To understand why there
cannot be general formulas for solving polynomials
of higher degrees, we need to understand polynomials
much better. First, we will look at the formula
to solve polynomials of the second degree.

Polynomials of the first degree are just 
linear equations of the form

\begin{equation}
ax + b = 0.
\end{equation}

We can easily solve by subtracting $b$ and dividing
by $a$:

\begin{equation}
x = -\frac{b}{a}.
\end{equation}

In Haskell, this is just:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{solve1}\mathbin{::}(\Conid{Fractional}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{solve1}\;(\Conid{P}\;[\mskip1.5mu \Varid{b},\Varid{a}\mskip1.5mu])\mathrel{=}[\mskip1.5mu \mathbin{-}\Varid{b}\mathbin{/}\Varid{a}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Note the order of $a$ and $b$ in the definition of the polynomial.
This is consistent with the equation we gave above,
since, in our definition of polynomials in Haskell,
the head of the list of the coefficients is the coefficient
of $x^0$.

Polynomials of the second degree can be solved with a technique
we already used in the previous chapter, namely
\term{completing the square}. We will now apply this technique
on symbols and, as a result, will obtain a formula that can
be applied on any polynomial of second degree.
We start with the equation

\begin{equation}
ax^2 + bx + c = 0.
\end{equation}

We subtract $c$ and divide by $a$ obtaining:

\begin{equation}
x^2 + \frac{b}{a}x = -\frac{c}{a}.
\end{equation}

Now, we want to get a binomial formula on the left-hand side
of the equation. A binomial formula has the form:

\begin{equation}
(\alpha + \beta)^2 = \alpha^2 + 2\alpha\beta + \beta^2.
\end{equation}

When we set $\alpha = x$, we have on the right-hand side:

\[
x^2 + 2\beta x + \beta^2.
\]

In our equation, we see the term $\frac{b}{a}x$ at the position
where, here, we have $2\beta x$.
We, therefore, have $\frac{b}{a} = 2\beta$ 
and $\beta = \frac{b}{2a}$.
The missing term, hence, is 
$(\frac{b}{2a})^2 = \frac{b^2}{4a^2}$.
We add this term to both sides of the equation:

\begin{equation}
x^2 + \frac{b}{a}x + \frac{b^2}{4a^2} = -\frac{c}{a} + \frac{b^2}{4a^2}.
\end{equation}

We can simplify the right-hand side of the equation a bit:

\begin{equation}
x^2 + \frac{b}{a}x + \frac{b^2}{4a^2} = \frac{b^2-4ac}{4a^2}.
\end{equation}

To get rid of all the squares, we now take the square root
on both sides of the equation. Since we have a binomial
formula on the left-hand side, we get:

\begin{equation}
x + \frac{b}{2a} = \frac{\pm\sqrt{b^2-4ac}}{2a}.
\end{equation}

When we solve this equation for $x$, we get

\begin{equation}
x = \frac{-b \pm\sqrt{b^2-4ac}}{2a}.
\end{equation}

Voil, this is the formula for solving polynomials of
the second degree.

We immediately see that polynomials with rational coefficients
may have irrational roots, because the solution involves
a square root, which leads either to an integer or
an irrational number.

We also see that polynomials of the second degree
may have two roots, namely the result of the expression
on the right-hand side, when we take the positive root,
\ie\ 

\[
\frac{-b+\sqrt{b^2-4ac}}{2a},
\]

and the one, when we take the negative root, \ie

\[
\frac{-b-\sqrt{b^2-4ac}}{2a}.
\]

However, when the square root is zero
then it makes no difference whether we add
or subtract. The square root becomes zero, when
the expression
$b^2-4ac$ is zero. So, when this expression
is zero, there is only one root.

But there is one more thing:
When the expression $b^2-4ac$ is negative,
then we will try to take a square root from
a negative term and that is not defined,
since a number multiplied by itself is always
positive, independent of that number itself being
positive or negative.

Well, it is not defined for \emph{real} numbers.
When we assume that $\sqrt{-1}$
is actually a legal expression, we could
\term{extend} the field of the real numbers
to another, more complex field that
includes this beast. 
We have already looked at how to extend fields
in the previous chapter and we will indeed
do this extension for $\mathbb{R}$ to create
the \term{complex numbers}, $\mathbb{C}$.
In that field, polynomials of the second degree
always have 1 or 2 solutions.

For instance the polynomial $x^2 + 1$ is never
negative and, therefore, has no roots in $\mathbb{R}$.
But when we assume that there is a number, say, $i$,
for which $i^2=-1$, then this value $i$ would
make the polynomial zero: $i^2 + 1 = -1 + 1 = 0$.

But, again, first things first.
The expression $b^2-4ac$ is called the
\term{discriminant} of the polynomial,
because it determines how many roots
there are: 2, 1 or (in $\mathbb{R}$) none.
The discriminant for polynomials
with real coefficients
may be implemented in Haskell as follows:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{dis}\mathbin{::}(\Conid{Num}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{dis}\;(\Conid{P}\;[\mskip1.5mu \Varid{c},\Varid{b},\Varid{a}\mskip1.5mu])\mathrel{=}\Varid{b}\mathbin{\uparrow}\mathrm{2}\mathbin{-}\mathrm{4}\mathbin{*}\Varid{a}\mathbin{*}\Varid{c}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

On top of this we implement a \term{root counter}:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{countRoots}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Ord}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to \Conid{Int}{}\<[E]%
\\
\>[3]{}\Varid{countRoots}\;\Varid{p}{}\<[17]%
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{dis}\;\Varid{p}\mathbin{>}\mathrm{0}{}\<[31]%
\>[31]{}\mathrel{=}\mathrm{2}{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{dis}\;\Varid{p}\mathbin{<}\mathrm{0}{}\<[31]%
\>[31]{}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}\mathrm{1}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The polynomial $x^2 + 4$, for instance,
has no roots in $\mathbb{R}$, since

\ensuremath{\Varid{countRoots}\;(\Conid{P}\;[\mskip1.5mu \mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])}

gives 0.
Indeed $0^2 - 4\times 1\times 4$ is negative.

The polynomial $x^2 - 4$, by contrast has

\ensuremath{\Varid{countRoots}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])},

2 roots.
Indeed, $0^2 - 4\times 1\times -4$ is
$0 + 16$ and, hence, positive.

The polynomial $x^2$ has 1 root,
since

\ensuremath{\Varid{countRoots}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])}

is 1.
Indeed, $0^2 - 4\times 1 \times 0$ is 0.

We finally implement the solution for
polynomials of the second degree:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}c<{\hspost}@{}}%
\column{25E}{@{}l@{}}%
\column{28}{@{}>{\hspre}l<{\hspost}@{}}%
\column{39}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{solve2}\mathbin{::}(\Conid{Floating}\;\Varid{a},\Conid{Fractional}\;\Varid{a},\Conid{Real}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{solve2}\;\Varid{p}\mathord{@}(\Conid{P}\;[\mskip1.5mu \Varid{c},\Varid{b},\Varid{a}\mskip1.5mu]){}\<[25]%
\>[25]{}\mid {}\<[25E]%
\>[28]{}\Varid{dis}\;\Varid{p}\mathbin{<}\mathrm{0}{}\<[39]%
\>[39]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[25]{}\mid {}\<[25E]%
\>[28]{}\Varid{x1}\not\equiv \Varid{x2}{}\<[39]%
\>[39]{}\mathrel{=}[\mskip1.5mu \Varid{x1},\Varid{x2}\mskip1.5mu]{}\<[E]%
\\
\>[25]{}\mid {}\<[25E]%
\>[28]{}\Varid{otherwise}{}\<[39]%
\>[39]{}\mathrel{=}[\mskip1.5mu \Varid{x1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{d}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{sqrt}\;(\Varid{dis}\;\Varid{p}){}\<[E]%
\\
\>[12]{}\Varid{x1}{}\<[16]%
\>[16]{}\mathrel{=}(\mathbin{-}\Varid{b}\mathbin{+}\Varid{d})\mathbin{/}\mathrm{2}\mathbin{*}\Varid{a}{}\<[E]%
\\
\>[12]{}\Varid{x2}{}\<[16]%
\>[16]{}\mathrel{=}(\mathbin{-}\Varid{b}\mathbin{-}\Varid{d})\mathbin{/}\mathrm{2}\mathbin{*}\Varid{a}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

When we call \ensuremath{\Varid{solve2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu])},
that is we solve the polynomial $x^2$,
we get the root \ensuremath{[\mskip1.5mu \mathrm{0}\mskip1.5mu]}, which is one root
as predicted.

To solve the polynomial $x^2 + 4$, we call
\ensuremath{\Varid{solve2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])} and get \ensuremath{[\mskip1.5mu \mskip1.5mu]}; as predicted,
this polynomials has no roots. 
It is everywhere positive.

The polynomial $x^2 - 4$, by contrast,
shall have two roots. We call
\ensuremath{\Varid{solve2}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu])} and get \ensuremath{[\mskip1.5mu \mathrm{2},\mathbin{-}\mathrm{2}\mskip1.5mu]}.
When we check this by applying the polynomial
to 2 and -2 like \ensuremath{\Varid{map}\;(\Varid{apply}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{4},\mathrm{0},\mathrm{1}\mskip1.5mu]))\;[\mskip1.5mu \mathrm{2},\mathbin{-}\mathrm{2}\mskip1.5mu]},
we get \ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{0}\mskip1.5mu]}.

What about the polynomial $-x^2 - x + 1$, which
we factored in the previous chapter?
We try \ensuremath{\Varid{solve2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathbin{-}\mathrm{1},\mathbin{-}\mathrm{1}\mskip1.5mu])} and get

\ensuremath{[\mskip1.5mu \mathbin{-}\mathrm{1.618033988749895},\mathrm{0.6180339887498949}\mskip1.5mu]},

which is $-\Phi$ and $-\Psi$, just as we saw before.

Which polynomial has the roots $\Phi$ and $\Psi$?
Well, let us try:

\ensuremath{\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\Varid{phi},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\Varid{psi},\mathrm{1}\mskip1.5mu])}

yields: 

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1.0},\mathbin{-}\mathrm{2.23606797749979},\mathrm{1.0}\mskip1.5mu]},

which corresponds to $x^2 -\sqrt{5} + 1$.
The coefficients are
1 for $x^2$, $-\sqrt{5}$ for $(-\Phi-\Psi)x$
and 1 for $(-\Phi)(-\Psi)$.

What is the result for the ``simple''
polynomial $x^2 + x + 1$?
We try with \ensuremath{\Varid{solve2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu])} and get
\ensuremath{[\mskip1.5mu \mskip1.5mu]} -- the empty list.
Indeed, $1^2 - 4\times 1\times 1$ is negative!

Let us pretend to be optimistic like the ``reckoning masters''
in the 15 and 16 hundreds. We already have a formula to compute
the roots for polynomials of the first two degrees.
It will certainly be easy to find formulas for the remaining
(infinitely many) degrees. We can then define a function
of the form:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}c<{\hspost}@{}}%
\column{12E}{@{}l@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{solve}\mathbin{::}(\Conid{Fractional}\;\Varid{a},\Conid{Floating}\;\Varid{a},\Conid{Real}\;\Varid{a})\Rightarrow \Conid{Poly}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{0}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{1}\mathrel{=}\Varid{solve1}\;\Varid{p}{}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{2}\mathrel{=}\Varid{solve2}\;\Varid{p}{}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{3}\mathrel{=}\bot {}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{4}\mathrel{=}\bot {}\<[E]%
\\
\>[3]{}\Varid{solve}\;\Varid{p}{}\<[12]%
\>[12]{}\mid {}\<[12E]%
\>[15]{}\Varid{degree}\;\Varid{p}\equiv \mathrm{5}\mathrel{=}\bot {}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

and so on.
With this optimism, our goal is to replace
the \ensuremath{\bot } implementations by functions
of the form \ensuremath{\Varid{solve3}}, \ensuremath{\Varid{solve4}}, \etc\
We come back to this endevour in a later chapter.

\section{Vieta's Formulas}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Vieta}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Roots}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

The binomial theorem describes regularities 
in the coefficients that turn up
when multiplying a polynomial (repeatedly) by it itself.
For the simple case $(a+b)(a+b)$, we get the result
$(a^2 + 2ab + b^2)$. The linear factors of polynomials
have a similar structure: sums of two numbers that are
multiplied with each other, \eg:

\begin{equation}
x^2 - 1 = (x+1)(x-1).
\end{equation}

Should we not expect similar regularities
with the coefficients of the resulting polynomials
in those cases? When we look at this in an algebraic
way, we would see:

\begin{equation}
(x+a)(x+b) = x^2+xb+xa+ab = x^2 + (a+b)x + ab.
\end{equation}

The coefficients of the resulting polynomial are
1, $a+b$ and $ab$. We immediately see the relation
to the binomial theorem:
if $a=b$, we would have $2a$ and $a^2$, where,
in the binomial theorem, the final coefficent is
interpreted as 1, for the number of occurrences of
$a^2$. We, hence, get $1, 2, 1$.

Let us check the theoretic result against
the concrete example $(x+1)(x-1)$. We set
$a=1$ and $b=-1$ and see:

\begin{equation}
x^2 + (1-1)x + (1\times (-1)) = x^2 - 1.
\end{equation}

That appears to be correct. But who are 
those $a$ and $b$ guys that appear in
the formula? Well, those are the additive
inverses of the roots of the polynomial in
question, since, if $(x+a)(x+b)\dots$ are the linear
factors, then the polynomial becomes 0 if any
of those factors becomes 0. The factor $(x+a)$,
obviously, becomes 0 if $x=-a$. $-a$ is therefore
a root of the polynomial. It follows that we
have a direct relation between the roots and
the coefficients.

As a first approximation (which is wrong!),
we could describe a second degree polynomial 
with the roots
$\alpha$ and $\beta$ as:

\[
x^2 + (-\alpha-\beta)x + \alpha\beta,
\]

We check again with $\alpha = -1$ and $\beta=1$:

\begin{equation}
x^2 + (1-1)x + (-1\times 1) = x^2 - 1.
\end{equation}

Correct until here.
What about other examples, for instance: 
$x^2 + x - 1$.
We already know the roots are $-\Phi$ and $-\Psi$.
So, we set $\alpha=-\Phi$ and $\beta=-\Psi$:

\begin{equation}
x^2 + (\Phi+\Psi)x + ((-\Phi)\times(-\Psi)) = x^2 + x - 1.
\end{equation}

The polynomial $x^2 - 4$ has the roots 2 and -2:

\begin{equation}
x^2 + (-2+2)x + (2\times(-2)) = x^2 - 4.
\end{equation}

The polynomial $x^2 + 5 + 6$ has the roots
-2 and -3:

\begin{equation}
x^2 + (2+3)x + (-2\times(-3)) = x^2 + 5x + 6.
\end{equation}

Note, by the way, the multiplication $12\times 13 = 156$.
Once again, this is a nice illustration of the similarity 
of numbers and polynomials.

Now, what about the polynomial
$-x^2 - x + 1$.
We know it has the same roots as
the polynomial
$x^2 + x - 1$.
But how can we get the coefficients from the roots
with the same formula? Something seems to be wrong\dots

Well, until now, we have looked only at \term{monic}
polynomials, that is polynomials with the first coefficient
being 1. But the polynomial $-x^2 - x + 1$ is not monic.
The first coefficient is -1. In fact, the complete
factorisation of this polynomial is

\[
-1(x+\Phi)(x+\Psi).
\]

We have to adjust our formula above to this case,
\ie:

\[
ax^2 + \frac{-\alpha-\beta}{a}x + \frac{\alpha\beta}{a},
\]

which can be simplified to

\[
ax^2 - \frac{\alpha+\beta}{a}x + \frac{\alpha\beta}{a}.
\]

When we now test with roots $\alpha=-\Phi$ and $\beta=-\Psi$
and coefficient $a=-1$,
we get

\[
-x^2 - \frac{-\Phi-\Psi}{-1}x + \frac{(-\Phi)(-\Psi)}{-1} = 
-x^2 - \frac{-1}{-1}x + \frac{-1}{-1} =
-x^2 - x + 1
\]

and everything appears to be in joint again.

When we advance beyond degree 2,
how should these formulas evolve?
Let us look at roots in terms of linear factors.
For a polynomial of degree $n$, we have up to $n$ factors
of the form

\[
(x+\alpha)(x+\beta)(x+\gamma)\dots
\]

When we multiply that out, we get combinations
as products and sums of products
of the coefficients of the linear factors
$\alpha, \beta, \gamma, \dots$
(which are the inverses of the roots 
of the resulting polynomial):

\[
(x^2 + \beta x + \alpha x + \alpha\beta)(x+\gamma),
\]

which is

\[
x^3 + 
(\alpha + \beta + \gamma) x^2 + 
(\alpha\beta + \alpha\gamma + \beta\gamma)x +
\alpha\beta\gamma.
\]

This already begins to reveal a pattern.
The first coefficient of the resulting polynomial
(counting without the coefficient of $x^3$)
is the sum of all the linear coefficients;
the second coefficient is the sum of all their
tuple products; the third is a triple product.
We could suspect that the third, in a 
four-degree polynomial, would be the sum
of all triple products and the fourth a
single quadruple product. Let us check:
we compute

\[
(x^3 + 
(\alpha + \beta + \gamma) x^2 + 
(\alpha\beta + \alpha\gamma + \beta\gamma)x +
\alpha\beta\gamma)(x + \delta)
\]

and get

\[
\begin{array}{cll}
    &  & x^4   \\
  + & (\alpha + \beta + \gamma + \delta) & x^3   \\
  + & (\alpha\beta + \alpha\gamma + \alpha\delta +
     \beta\gamma + \beta\delta + \gamma\delta) & x^2   \\
  + & (\alpha\beta\gamma + \alpha\beta\delta + \alpha\gamma\delta +
     \beta\gamma\delta) & x   \\
  + & \alpha\beta\gamma\delta. &
\end{array}
\]

The result, indeed, continues the pattern we saw above.
For the first coefficient we see the simple sum 
of all the linear coefficients;
for the second one, we see the sum of all tuple products;
for the third one, we see the sum of all triple products
and then we see a single quadruple product.

When we now bring the negative sign of the roots in
(we used their additive inverses) and
the first coefficient, the we get the following
sequence of formulas:

\begin{subequations}\label{eq:vieta1}
\begin{align}
x_1 + x_2 + \dots + x_n & = & -\frac{a_{n-1}}{a_n}\\
(x_1x_2 + \dots + x_1x_n) + 
(x_2x_3 + \dots + x_2x_n) + \dots + 
x_{n-1}x_n & = & \frac{a_{n-2}}{a_n}\\
\dots & = & \dots\\
(x_1x_2 \dots x_n) & = & (-1)^n\frac{a_0}{a_n}
\end{align}
\end{subequations}

to describe the relation of roots and coefficients
of a polynomial of the form

\[
a_nx^n + a_{n-1}x^{n-1} + \dots + a_0
\]

with roots $x_1, x_2, \dots, x_n$.

The equations \ref{eq:vieta1} are known as
\term{Vieta's formulas}, after the French lawyer
and mathematician Franois Vite (1540 -- 1603)
whom we already know as author of an elegant
formula to express $\pi$.

But what are those constructs on the left-hand
side of the formulas? One answer is:
those are \term{elementary symmetric polynomials},
which are building blocks for \term{symmetric polynomials}.
Symmetric polynomials will be very important for us
further down the road. At the moment, they only delay
a good answer to the question\dots

A better answer at this stage is
that those beasts are the sums of
all \emph{distinct} combinations
of the roots in 1-tuples, 2-tuples, 3-tuples
and so on.
For the first case, the `1-tuples',
that is just the sum of all the roots;
for the second case, the `2-tuples',
we have all combinations of 
\emph{2 elements out of $n$}, where $n$
is the number of roots;
for the third case, we have all combinations
of \emph{3 elements out of $n$} and so on.

You probably guess where this is leading us.
When we have four roots,
the first coefficient,
the one in front of $x^{n-1}$,
is basically the sum
of $\binom{4}{1} = 4$ terms;
the second coefficient is the sum of
$\binom{4}{2} = 6$ terms;
the third coefficient is the sum of
$\binom{4}{3} = 4$ terms and
the last coefficient,
the one without an $x$, is the sum of
only $\binom{4}{4} = 1$ term.

In general, for $n$ roots, we get,
for the $k^{th}$ coefficient,
$\binom{n}{k}$ terms
of products of $k$ roots.
Those are $\sum_{k=0}^n{\binom{n}{k}} = 2^n$
terms in total 
(including the coefficient in front of $x^n$,
which corresponds to $\binom{n}{0}=1$).
Once again, algebra boils down to combinatorial
problems induced by the distributive law.

Let us devise a function 
that gives us the right-hand sides of Vieta's formula,
when we provide the left-hand sides.
That is, we write a function that receives the list
of roots of the polynomial and that returns the list
of the coefficients divided by the first coefficient.

On the first sight, it seems to be tricky to get
the sums of products right. But, in fact, we already
know everything we need. 
What we want to do is to generate
all possible $k$-combinations for $k=1\dots n$
of the $n$ elements, but without duplicates, \ie\
$ab$ is the same as $ba$ (since multiplication
is commutative).
This, however, is the structure of the powerset,
which, for a set with $n$ elements, contains indeed
$2^n$ subsets -- just the number of all possibilities
to choose $k$ out of $n$ for $k=0\dots n$.

For instance, the set of roots 
$\lbrace\alpha,\beta,\gamma,\delta\rbrace$
has the powerset (ordered according
to the size of the subsets):

\begin{minipage}{\textwidth}
\begin{gather*}
\lbrace
\varnothing,\\
\lbrace\alpha\rbrace,
\lbrace\beta \rbrace,
\lbrace\gamma\rbrace,
\lbrace\delta\rbrace,\\
\lbrace\alpha,\beta\rbrace,
\lbrace\alpha,\gamma\rbrace,
\lbrace\alpha,\delta\rbrace,
\lbrace\beta,\gamma\rbrace,
\lbrace\beta,\delta\rbrace,
\lbrace\delta,\gamma\rbrace,\\
\lbrace\alpha,\beta,\gamma\rbrace,
\lbrace\alpha,\beta,\delta\rbrace,
\lbrace\alpha,\gamma,\delta\rbrace,
\lbrace\beta,\gamma,\delta\rbrace,\\
\lbrace\alpha,\beta,\gamma,\delta\rbrace
\rbrace
\end{gather*}
\end{minipage}

We can transform the powerset into 
the coefficients by
dropping $\varnothing$ (which 
represents $a$ in a monic polynomial)
and then adding up the products of
the subsets of the same size. 
The following function
does that:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{vieta}\mathbin{::}(\Conid{Real}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{vieta}\mathrel{=}\Varid{c}\mathbin{\circ}\Varid{g}\mathbin{\circ}\Varid{d}\mathbin{\circ}\Varid{s}\mathbin{\circ}\Varid{\Conid{Perm}.ps}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{d}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{drop}\;\mathrm{1}{}\<[E]%
\\
\>[12]{}\Varid{g}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{groupBy}\;((\equiv )\mathbin{`\Varid{on}`}\Varid{length}){}\<[E]%
\\
\>[12]{}\Varid{s}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{sortOn}\;\Varid{length}{}\<[E]%
\\
\>[12]{}\Varid{c}\;\Varid{p}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}[\mskip1.5mu (\mathbin{-}\mathrm{1})\mathbin{\uparrow}\Varid{n}\mathbin{*}\Varid{sum}\;(\Varid{map}\;\Varid{product}\;\Varid{x})\mid (\Varid{x},\Varid{n})\leftarrow \Varid{zip}\;\Varid{p}\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We first create the powerset (\ensuremath{\Varid{\Conid{Perm}.ps}}).
We then sort it by the lengths of the subsets (that is
the \term{cardinalities} in set theory jargon) and
drop the first one (the empty set).
We then introduce one more level of separation,
\ie\ we group the subsets by their size.
From this result, we create a new set
by zipping the result with the natural numbers
starting from 1 so that each
group of equal length gets paired with a
number $n$.
We, then, map \ensuremath{\Varid{product}} on these lists and 
add the resulting products together.
Finally, we multiply this number by -1
raised to the power of $n$. 

This last step
takes care of signedness.
Since, in the linear factors, we use the
additive inverses of the roots,
the effect of the signs of the roots
must be flipped around.
Therefore, we turn
the sign of every second result, namely
those with an odd number of factors.
The negative signs of the roots
that enter products with an even number of factors
cancel out by themselves.

Let us look at some examples.
We start with our favourite: $x^2 + x - 1$.
We call \ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathbin{-}\Varid{phi},\mathbin{-}\Varid{psi}\mskip1.5mu]} and get

\ensuremath{[\mskip1.5mu \mathrm{1.0},\mathbin{-}\mathrm{1.0}\mskip1.5mu]}.

That are the coefficient of $x$ and the constant -1.
To complicate, we check some variants of
those roots:

\begin{itemize}
\item \ensuremath{\Varid{vieta}\;[\mskip1.5mu \Varid{phi},\mathbin{-}\Varid{psi}\mskip1.5mu]} gives $[-\sqrt{5}, 1]$ and, hence, the polynomial
$x^2 - \sqrt{5} + 1$, whose roots are indeed $\Phi$ and $-\Psi$.

\item \ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathbin{-}\Varid{phi},\Varid{psi}\mskip1.5mu]} gives $[\sqrt{5}, 1]$ and that is the polynomial
$x^2 + \sqrt{5} + 1$, whose roots are $-\Phi$ and $\Psi$.

\item \ensuremath{\Varid{vieta}\;[\mskip1.5mu \Varid{phi},\Varid{psi}\mskip1.5mu]} gives $[-1, -1]$, the polynomial
$x^2 - x - 1$, whose roots are $\Phi$ and $\Psi$.
\end{itemize}

A simpler example that shows the signedness of roots and coefficients
is $x^2 - 1$. \ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathrm{1},\mathbin{-}\mathrm{1}\mskip1.5mu]} gives \ensuremath{[\mskip1.5mu \mathrm{0},\mathbin{-}\mathrm{1}\mskip1.5mu]}, which, indeed,
corresponds to $x^2  - 1$.

What about a third-degree polynomial,
\eg\ $(x+1)(x+1)(x+1) = x^3 + 3x^2 + 3x + 1$?
We call \ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathbin{-}\mathrm{1},\mathbin{-}\mathrm{1}\mskip1.5mu]} and see \ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{3},\mathrm{1}\mskip1.5mu]}.

Another experiment: we compute
\ensuremath{\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu])\;(\Varid{mul}\;(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{1}\mskip1.5mu])\;(\Conid{P}\;[\mskip1.5mu \mathrm{3},\mathrm{1}\mskip1.5mu]))},
which is $(x+1)(x+2)(x+3)$ and get
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{11},\mathrm{6},\mathrm{1}\mskip1.5mu]}, which represents the polynomial
$x^3 + 6x^2 + 11x + 6$.
We call \ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathbin{-}\mathrm{2},\mathbin{-}\mathrm{3}\mskip1.5mu]} and get \ensuremath{[\mskip1.5mu \mathrm{6},\mathrm{11},\mathrm{6}\mskip1.5mu]}.
(You may have realised that the coefficients are 
unsigned Stirling numbers of the first kind and
now might want to contemplate 
why those guys show up again\dots)

A fifth-degree polynomial:
\ensuremath{\Varid{prodp}\;\Varid{mul}\;[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{3},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{4},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}:
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{120},\mathrm{274},\mathrm{225},\mathrm{85},\mathrm{15},\mathrm{1}\mskip1.5mu]}, that is

\[
x^5 + 15x^4 + 85x^3 + 225x^2 + 274x + 120.
\]

\ensuremath{\Varid{vieta}\;[\mskip1.5mu \mathbin{-}\mathrm{1},\mathbin{-}\mathrm{2},\mathbin{-}\mathrm{3},\mathbin{-}\mathrm{4},\mathbin{-}\mathrm{5}\mskip1.5mu]}: \ensuremath{[\mskip1.5mu \mathrm{15},\mathrm{85},\mathrm{225},\mathrm{274},\mathrm{120}\mskip1.5mu]}.

Well, we can go on playing around like this forever.
The point of Vieta's formulas, however, is not so
much practical. It is not an efficient way to compute
roots from coefficients or coefficients from roots.
That should be clear immediately, when we look at
the Haskell function \ensuremath{\Varid{vieta}}. It generates the
powerset of the set of roots -- and that cannot be
efficient at least for large (or better worded perhaps:
unsmall) numbers.
Vieta's formulas, instead, are a theoretical device.
They help to understand the relation between
coefficients and roots and they will play an important
role in our further investigations.

\section{Factoring Polynomials}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{PolyFactor}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Roots}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}
Polynomials can be factored in different contexts,
for instance a field or the integers (which, as
you may remember, do not form a field, but a ring).
These contexts can be generalised to what is called
a \term{unique factorisation domain}.
A unique factorisation domain is a commutative ring $R$,
where

\begin{itemize}
\item $uv \neq 0$, whenever $u,v \in R$ and 
      $u \neq 0$ and $v \neq 0$;
\item every nonzero element is a \term{unit},
      a \term{prime} or can be uniquely represented 
      as a product of primes;
\item every unit $u$ has an inverse $v$, such that
      $uv = 1$.
\item a prime $p$ is a nonunit element for which an
      equation of the form $p = qr$ is true, only 
      if either $q$ or $r$ is a unit.
\end{itemize} 

The integers form a unique factorisation domain,
with the units 1 and -1 and the primes 
$\pm 2, \pm 3, \pm 5, \pm 7, \dots$
We can easily verify that 1 and -1 obey to
the definition of unit, 
when we assume that each one is its own inverse.
We can also agree that the primes are primes
in the sense of the above definition: for any prime
in $p \in \mathbb{Z}$, if $p = qr$, then either 
$q$ or $r$ must be a unit and the other must equal $p$.
That is the definition of primes.

A field is trivially a unique factorisation domain
without primes where all elements are units.

The simplest notion of factoring in such a domain
is the factoring into \term{primitive part} and
\term{content}. This, basically, splits a polynomial
into a number (in the domain we are dealing with) and
a \term{primitive polynomial}. 

With the integers, the content is the \acronym{gcd} of the
coefficients. For instance, the \acronym{gcd} 
of the coefficients of the polynomial 
$9x^5 + 27x^2 + 81$ is 9. 
When we divide the polynomial by 9 we get
$x^5 + 3x^2 + 9$.

For rational numbers, we would choose a fraction
that turns all coefficients into integers that do
not share divisors. The polynomial

\[
\frac{1}{3}x^5 + \frac{7}{2}x^2 + 2x + 1,
\]

for instance, can be factored dividing 
all coefficients by $\frac{1}{6}$:

\[
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{array}{rcrcr}
\frac{1}{3} & \times & 6 & = & 2\\
\frac{7}{2} & \times & 6 & = & 21\\
2 & \times & 6 & = & 12\\
1 & \times & 6 & = & 6
\end{array}
\endgroup
\]

We, hence, get the product
$\frac{1}{6}(2x^5 + 21x^2 + 12x + 6)$.

This, however, is not the end of the story.
Consider the polynomial 

\[
3x^2 - 27.
\]

We can factor this one into
$3(x^2 - 9)$, with the second part being
primitve: the \acronym{gcd} of its coefficients
is 1. But we can factor it further.
Obviously, we have

\begin{equation}
x^2 - 9 = (x - 3)(x + 3).
\end{equation}

The complete factorisation of the polynomial
$3x^2 - 27$, hence, is $3(x-3)(x+3)$.

For factoring primitive polynomials manually, there
are many different methods (most of which have a
video on youtube). They share one property:
they are highly inefficient, when it comes
to polynomials of larger degrees or with big
coefficients. They, basically, all use integer
factorisation of which we know that it is extremely
expensive in terms of computation complexity.
Instead of going through all of them, we will here
present a typical classical method, namely Kronecker's
method.

Kronecker's method is a distinct-degree approach. 
That is, it searches for the factors of a given degree.
We start by applying the polynomial to $n$ distinct values,
for $n$ the degree of the factors plus 1.
That is because, to represent a polynomial of degree $d$,
we need $d+1$ coefficients, \eg\ \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu]} has three
coefficients and represents the polynomial $x^2$, which is
of degree 2.

The rationale of applying the polynomial is the following:
When the polynomial we want to factor generates
a given set of values, then the product of the factors
of that polynomial must generate the same values.
Any factor must, hence, consist of divisors of those values. 
The number of integer divisors
of those values, however, is limited.
We can therefore afford, at least for small polynomials
with small coefficients, trying all the combinations
of the divisors.

We have already defined a function
to find the divisors of a given number,
when we discussed Euler's totient function.
However, that function dealt with natural numbers
only. We now need a variant that is able to
compute negative divisors.
It would be also nice if that function
could give us not only the divisors,
but additionally the additive inverse,
\ie\ the negation of the divisors, because,
in many cases, we need to look at the negative
alternatives too. Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}c<{\hspost}@{}}%
\column{11E}{@{}l@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{divs}\mathbin{::}\Conid{Zahl}\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{divs}\;\Varid{i}{}\<[11]%
\>[11]{}\mid {}\<[11E]%
\>[14]{}\Varid{i}\mathbin{<}\mathrm{0}{}\<[25]%
\>[25]{}\mathrel{=}\Varid{divs}\;(\mathbin{-}\Varid{i}){}\<[E]%
\\
\>[11]{}\mid {}\<[11E]%
\>[14]{}\Varid{otherwise}{}\<[25]%
\>[25]{}\mathrel{=}\Varid{ds}\plus \Varid{map}\;\Varid{negate}\;\Varid{ds}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{ds}\mathrel{=}[\mskip1.5mu \Varid{d}\mid \Varid{d}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{i}\mskip1.5mu],\Varid{rem}\;\Varid{i}\;\Varid{d}\equiv \mathrm{0}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The divisors are now combined to yield
$n$-tuples with $n$ still the degree of the factor
plus one and each divisor representing
one coefficient of the resulting polynomial.
But before we can convert the $n$-tuples
into polynomials, we need to create all
possible permutations, since the polynomial
\ensuremath{\Conid{P}\;[\mskip1.5mu \Varid{a},\Varid{b}\mskip1.5mu]} is not the same as \ensuremath{\Conid{P}\;[\mskip1.5mu \Varid{b},\Varid{a}\mskip1.5mu]} if
$a \neq b$.
From this we obtain a (potentially very large)
list of $n$-tuples that we then convert
into polynomials. From that list,
we finally filter those polynomials
for which \ensuremath{\Varid{p}\mathbin{`\Varid{divp}`}\Varid{k}\equiv \mathrm{0}}, where $p$ is the
input polynomial and $k$ the candidate in 
the list of polynomials. Here is an implementation
(using lists instead of $n$-tuples):

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{kronecker}\mathbin{::}\Conid{Poly}\;\Conid{Zahl}\to [\mskip1.5mu \Conid{Zahl}\mskip1.5mu]\to [\mskip1.5mu \Conid{Poly}\;\Conid{Quoz}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{kronecker}\;(\Conid{P}\;\Varid{cs})\;\Varid{is}\mathrel{=}\Varid{nub}\;[\mskip1.5mu \Varid{a}\mid \Varid{a}\leftarrow \Varid{as},\Varid{snd}\;(\Varid{r}\mathbin{`\Varid{divp}`}\Varid{a})\equiv \Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{ds}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{map}\;\Varid{divs}\;\Varid{is}{}\<[E]%
\\
\>[12]{}\Varid{ps}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{concatMap}\;\Varid{perms}\;(\Varid{listcombine}\;\Varid{ds}){}\<[E]%
\\
\>[12]{}\Varid{as}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{map}\;(\Conid{P}\mathbin{\circ}\Varid{map}\;\Varid{fromInteger})\;\Varid{ps}{}\<[E]%
\\
\>[12]{}\Varid{r}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Conid{P}\;[\mskip1.5mu \Varid{c}\mathbin{\%}\mathrm{1}\mid \Varid{c}\leftarrow \Varid{cs}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function takes two arguments. The first is
the polynomial we want to factor and the second
is the list of results obtained by applying the polynomial.
We then get the divisors \ensuremath{\Varid{ds}}, create all possible
combinations of the divisors and all possible
permutations of the resulting lists.
We then convert the coefficients to rational numbers
(since we later use \ensuremath{\Varid{divp}}). Finally,
we filter all polynomials that leave no remainder
when the input polynomial is divided by any one of them.

There are two combinatorial functions, \ensuremath{\Varid{perms}} and \ensuremath{\Varid{listcombine}}.
We have already defined \ensuremath{\Varid{perms}}, when discussing
permutations. The function generates all permutations
of a given list. The other function, \ensuremath{\Varid{listcombine}},
however, is new. It creates all possible combinations
of a list of lists. Here is a possible implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}c<{\hspost}@{}}%
\column{24E}{@{}l@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{listcombine}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{listcombine}\;{}\<[16]%
\>[16]{}[\mskip1.5mu \mskip1.5mu]{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{listcombine}\;{}\<[16]%
\>[16]{}([\mskip1.5mu \mskip1.5mu]\mathbin{:\char95 }){}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{listcombine}\;{}\<[16]%
\>[16]{}(\Varid{x}\mathbin{:}\Varid{xs}){}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\Varid{inshead}\;(\Varid{head}\;\Varid{x})\;(\Varid{listcombine}\;\Varid{xs})\plus {}\<[E]%
\\
\>[27]{}\Varid{listcombine}\;((\Varid{tail}\;\Varid{x})\mathbin{:}\Varid{xs}){}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{inshead}\mathbin{::}\Varid{a}\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{inshead}\;\Varid{x}\;[\mskip1.5mu \mskip1.5mu]{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}[\mskip1.5mu [\mskip1.5mu \Varid{x}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{inshead}\;\Varid{x}\;\Varid{zs}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\Varid{map}\;(\Varid{x}\mathbin{:})\;\Varid{zs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Let us try \ensuremath{\Varid{kronecker}} on some polynomials.
First, we need to apply the input polynomial
to get $n$ results. For instance, we know
that the polynomial $x^2 - 9$ has factors
of first degree. We, therefore, apply it on
two values: \ensuremath{\mathbf{let}\;\Varid{vs}\mathrel{=}\Varid{mapply}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{9},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]}
and get for \ensuremath{\Varid{vs}}: \ensuremath{[\mskip1.5mu \mathbin{-}\mathrm{9},\mathbin{-}\mathrm{8}\mskip1.5mu]}.
Now we call \ensuremath{\Varid{kronecker}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{9},\mathrm{0},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathbin{-}\mathrm{9},\mathbin{-}\mathrm{8}\mskip1.5mu]}
and get:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{3}\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{3}\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{1})\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu (\mathbin{-}\mathrm{3})\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu (\mathbin{-}\mathrm{3})\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{1})\mathbin{\%}\mathrm{1}\mskip1.5mu]}

Those are the polynomials $x+3$, $-x+3$, $x-3$ and $-x-3$.
By convention, we exclude the polynomials starting with
a negative coefficient by factoring -1 out.
However, we can easily see that all of them are actually
factors of $x^2 - 9$, since

\begin{equation}
(x+3)(x-3) = (x^2-9)
\end{equation}

and

\begin{equation}
(-x+3)(-x-3) = (x^2-9).
\end{equation}

Here is another example: $x^5 + x^4 + x^2 + x + 2$.
We want to find a factor of degree 2,
so we apply the polynomial to three values, say,
\ensuremath{[\mskip1.5mu \mathbin{-}\mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu]}. The result is \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{2},\mathrm{6}\mskip1.5mu]}.
We run \ensuremath{\Varid{kronecker}\;(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{1},\mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu])\;[\mskip1.5mu \mathrm{2},\mathrm{2},\mathrm{6}\mskip1.5mu]} 
and, after a short while, we get:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1},\mathrm{1}\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{2}\mathbin{\%}\mathrm{1},\mathrm{2}\mathbin{\%}\mathrm{1},\mathrm{2}\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu (\mathbin{-}\mathrm{1})\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{1})\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{1})\mathbin{\%}\mathrm{1}\mskip1.5mu]}\\
\ensuremath{\Conid{P}\;[\mskip1.5mu (\mathbin{-}\mathrm{2})\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{2})\mathbin{\%}\mathrm{1},(\mathbin{-}\mathrm{2})\mathbin{\%}\mathrm{1}\mskip1.5mu]},

which corresponds to the polynomials
$x^2+x+1$, $2x^2+2x+2$,
$-x^2-x -1$ and $-2x^2-2x-2$.
Ony the first one is a primitive polynomial.
We can factor out 2 from the second one,
leaving just the first one;
polynomials three and four, simply, are 
the negative counterparts of one and two,
so we can factor out -1 and -2, respectively,
to obtain again the first one.

To check if the first one is really a factor
of the input polynomial we divide:

\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{1},\mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu]\mathbin{`\Varid{divp}`}\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\
 and get
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathbin{-}\mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu]}, which corresponds to 
$x^3 - x + 2$. Indeed:

\begin{equation}
(x^2 + x + 1)(x^3 - x + 2) = x^5 + x^4 + x^2 + x + 2.
\end{equation}

Kronecker's method is just a brute force search.
It is obvious that it is not efficient and will fail
with growing degrees and coefficients.
Modern methods to factor polynomials use
much more sophisticated techniques.

They are, in particular, based on modular arithmetic
and make use of theorems that we have
already discussed in the ring of integers.
Polynomials with coefficients in a ring (or field)
form a ring too, a polynomial ring.
Theorems that hold in any ring, hence, hold also
in a polynomial ring. We, therefore, do not need
to prove them here again.

We will discuss the methods for factorising
polynomials in a finite field in the next section.
Let us here assume that we already knew such a method.
We could then call it to factor a given polynomial
in a finite field and then reinterpret the result
in the domain we started with.
\section{Factoring Polynomials in a finite Field}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{PolyModFactor}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{PolyFactor}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Famous factorisation algorithms using modular
arithmetic are \term{Berlekamp's algorithm}
developed by the American mathematician 
Elwyn Berlekamp in the late Sixties
and the \term{Cantor-Zassenhaus algorithm}
developed in the late Seventies and early Eighties
by David Cantor, an American methematician,
not to be confused with Georg Cantor, and
Hans Zassenhaus (1912 -- 1991), a German-American
mathematician.
We will here focus on Cantor-Zassenhaus,
which is by today probably the most-used algorithm
implemented in many computer algebra systems.

The contribution of Cantor-Zassenhaus, strictly speaking,
is just one of several pieces. The whole approach is
based on Euler's theorem,
which, as you may remember, states that

\begin{equation}
a^{\varphi(n)} \equiv 1 \pmod{n},
\end{equation}

where $\varphi(n)$ is the totient function
of $n$ counting the numbers $1\dots n-1$ that
are coprime to $n$, \ie\ that share no divisors with $n$.

Euler's theorem is defined as theorem over the ring
of integers, which, by modular arithmetic, transforms
into the finite field of the integers $0\dots n-1$.
Polynomial rings can be seen as extensions
of the underlying ring (of integers).
When we introduce modular arithmetic,
that is, when we build polynomials on 
a finite field, they still
constitute a ring, but now a ring built on top of a finite field.
Notationally, this is usually expressed as $K[x]$,
where $K$ is a field and $K[x]$ the polynomial ring
defined on top of $K$.

When we now take polynomials modulo a polynomial,
we again get a finite field, this time a polynomial field
of the form $K[x]/m$ (pronounced ``over'' $m$),
where $m$ is a polynomial.
The point in doing this is that many properties of
the original field $K$ are preserved in $K[x]/m$ and
Euler's theorem is one of them.

However, we need to redefine Euler's theorem
to make clear what is meant by it in the new context.
We are now dealing with the polynomial ring $K[x]$
and a polynomial $m \in K[x]$.
Based on this, we can define the totient function as

\[
\varphi(m) = |\lbrace f \in K[x] : 0 \le f \le m \wedge \gcd(m,f) = 1\rbrace|,
\]

\ie\ the cardinality of the set of all polynomials $f$
less or equal than $m$ that do not share
divisors with $m$. For any such ring $K[x]$
and any $f \in K[x] : \gcd(m,f) = 1$,
the following holds:

\begin{equation}
f^{\varphi(m)} \equiv 1 \pmod{m}.
\end{equation}

This is easy to prove.
The resulting structure $K[x]/(m)$
has a multiplicative
group $K_m^*$ (just as the integers $\pmod{n}$).
The members of this group are all polynomials
that do not share divisors with $m$ and $\varphi(m)$
is the cardinality of this group.
From $gcd(m,f) = 1$, it follows that
$f_m = f \mod{m} \in K_m^*$.
We, for sure, have 
$f_m^{\varphi(m)} \equiv 1 \pmod{m}$, since 
$\varphi(m)$ is the size of the group.
This equivalence may hold also for other numbers, $a$,
such that $f^a \equiv 1 \pmod{m}$, but
according to Lagrange's theorem 
(that the cardinality of subgroups of $G$ divides
the cardinality of $G$), all these numbers $a$
must divide $\varphi(m)$, the size of the group,
and we unmistakenly have
$f^{\varphi(m)} \equiv 1 \pmod{m}\qed$.

From this theorem, Fermat's little theorem
follows. Let $K$ be a field with $q$ elements; when using
arithmetic modulo a prime $p$, then $K_m^*$ is the group
of numbers $1\dots p-1$, which has $q=p-1$ elements.
Note that, when we refer to the multiplicative group
of this field, we usually refer only to the numbers
$1\dots p-1$, \ie\ $p-1$ numbers.
Now, let $g$ be an \term{irreducible} polynomial,
\ie\ a non-constant polynomial that cannot be 
further factored and, hence, a ``prime'' in our polynomial ring,
with degree $d$, $d>0$. Then it holds for any polynomial $f$
from this field

\begin{equation}\label{eq:polyFacFermat}
f^{q^d} \equiv f \pmod{g}.
\end{equation}

We can prove this easily:
We know that $K$ has $q$ elements.
From this $q$ elements we can create 
a limited number of polynomials.
When you look at our Haskell representation of polynomials,
you will easily convince yourself that the number of valid
polynomials of a given degree $d$ equals the number of valid
numbers that can be presented in the numeral system base $q$
with $d+1$ digits. If, for instance, $q=2$, then we have
(without the zero-polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]})

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|r|r|c|}\hline
degree & size & polynomials\\\hline\hline
0      & 1    & \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]}\\\hline
1      & 2    & \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]}\\\hline
2      & 4    & \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\\hline
3      & 8    & \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}\\
       &      & \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{0},\mathrm{1}\mskip1.5mu]}\\\hline
$\dots$&$\dots$&$\dots$\\\hline
\end{tabular}
\endgroup
\end{center}

We, hence, can precisely say how many polynomials 
of degree $<d$ there are, namely $r=q^d$.
For the example $q=2$, we see that there are 16 polynomials
with degree less than 4, which is $2^4$.
One of those polynomials, however, is \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]},
which we must exclude, when asking for $\varphi(g)$,
since, for this polynomial, 
division is not defined.
For the irreducible polynomial $g$, we therefore
have $r-1$ elements that do not share divisors with $g$,
\ie\ $\varphi(g) = r-1$. So, according to Euler's theorem,
we have 

\begin{equation}
f^{r-1} \equiv 1 \pmod{g}. 
\end{equation}

Multiplying both sides by $f$, we get

\begin{equation}
f^{r} \equiv f \pmod{g}. 
\end{equation}

Since $r=q^d$, this is equivalent to \ref{eq:polyFacFermat}
and this concludes the proof.$\qed$

From Fermat's theorem, we can derive a nice and useful corollary.
Note that, when we subtract $f$ from both sides of the equivalence,
we would get 0 on the right-hand side, which means that
$g$ divides the expression on the left-hand side.
Set $x=f$, then we have:

\begin{equation}\label{eq:polyFacIrrTest}
x^{q^d} - x \equiv 0 \pmod{g}. 
\end{equation}

This is the basis for 
a nice test for irreducibility.
Since the group established by a non-irreducible
polynomial of degree $d$ has less than $p^d - 1$ elements,
it will divide $x^{p^c} - x$ for some $c<d$, but an irreducible
polynomial will not.
\ignore{TODO: why are there no subgroups?}
Here is a Haskell implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{48}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{irreducible}\mathbin{::}\Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}\to \Conid{Bool}{}\<[E]%
\\
\>[3]{}\Varid{irreducible}\;\Varid{p}\;\Varid{u}{}\<[20]%
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{d}\mathbin{<}\mathrm{2}{}\<[34]%
\>[34]{}\mathrel{=}\Conid{False}{}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{otherwise}{}\<[34]%
\>[34]{}\mathrel{=}\Varid{go}\;\mathrm{1}\;\Varid{x}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{d}{}\<[20]%
\>[20]{}\mathrel{=}{}\<[20E]%
\>[23]{}\Varid{degree}\;\Varid{u}{}\<[E]%
\\
\>[12]{}\Varid{x}{}\<[20]%
\>[20]{}\mathrel{=}{}\<[20E]%
\>[23]{}\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{i}\;\Varid{z}{}\<[20]%
\>[20]{}\mathrel{=}{}\<[20E]%
\>[23]{}\mathbf{let}\;\Varid{z'}\mathrel{=}\Varid{powmp}\;\Varid{p}\;\Varid{p}\;\Varid{z}{}\<[E]%
\\
\>[23]{}\mathbf{in}\;{}\<[27]%
\>[27]{}\mathbf{case}\;\Varid{pmmod}\;\Varid{p}\;(\Varid{addp}\;\Varid{p}\;\Varid{z'}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\Varid{p}\mathbin{-}\mathrm{1}\mskip1.5mu]))\;\Varid{u}\;\mathbf{of}{}\<[E]%
\\
\>[27]{}\Conid{P}\;[\mskip1.5mu \anonymous \mskip1.5mu]{}\<[34]%
\>[34]{}\to {}\<[38]%
\>[38]{}\Varid{i}\equiv \Varid{d}{}\<[E]%
\\
\>[27]{}\Varid{g}{}\<[34]%
\>[34]{}\to {}\<[38]%
\>[38]{}\mathbf{if}\;\Varid{i}\mathbin{<}\Varid{d}\;{}\<[48]%
\>[48]{}\mathbf{then}\;\Varid{go}\;(\Varid{i}\mathbin{+}\mathrm{1})\;(\Varid{pmmod}\;\Varid{p}\;\Varid{z'}\;\Varid{u}){}\<[E]%
\\
\>[48]{}\mathbf{else}\;\Conid{False}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives two arguments: the modulus and 
the polynomial we want to check.
First, we compute the degree of the polynomial.
When the polynomial is of degree 0 or 1, 
there are by definition only trivial, \ie\ constant factors.
It is, hence, not irreducible (it is not reducible either, 
it is just uninteresting).
Then we start the algorithm beginning with values 1 and
$x$, where $x$ is the simple polynomial $x$.
In \ensuremath{\Varid{go}}, we raise this polynomial to the power of $p$,
and subtract it from the result.
Note that we add $p-1$, which,
in modular arithmetic, is the same as subtracting 1. 
We take the result modulo the input polynomial \ensuremath{\Varid{u}}. 
This corresponds to
$x^{p^d} - x$ for degree $d=1$.

If the result is a constant polynomial 
and the degree counter $i$ equals $d$,
then equation \ref{eq:polyFacIrrTest} is fulfilled.
(Note that we consider any constant polynomial as zero,
since a constant polynomial is just the content,
which usually should have been removed before we start
to search factors.)
Otherwise, if the degree counter does not equal $d$,
this polynomial fulfils the equation with a ``wrong'' degree.
This is possible only if the input was not irreducible
in the first place.

Finally, if we have a remainder that is not constant,
we either continue (if we have not yet reached the degree
in question) or, if we had already reached the final degree,
we return with False, since the polynomial 
is certainly not irreducible.

Note that we continue with \ensuremath{\Varid{pmmod}\;\Varid{p}\;\Varid{z'}\;\Varid{u}}, that is,
with the previous power modulo $u$. This is an important
optimisation measure. If we did not do that,
we would create gigantic polynomials. Imagine
a polynomial of degree 8 modulo 11. To check that polynomial
we would need to raise $x$ to the power of $11^8$,
which would result in a polynomial of degree \num{214358881}.
Since the only thing we want to know is
a value modulo $u$, we can reduce the overhead of taking powers
by taking them modulo $u$ in the first place.

Let us look at an example.
We generate a random polynomial of degree 3 modulo 7:

\ensuremath{\Varid{g}\leftarrow \Varid{randomPoly}\;\mathrm{7}\;\mathrm{4}}\\

I get the polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{3},\mathrm{3},\mathrm{3},\mathrm{4}\mskip1.5mu]}.
(Note that you may get another one!)
Calling \ensuremath{\Varid{irreducible}\;\mathrm{7}\;\Varid{g}} says: \ensuremath{\Conid{False}}.

When we raise the polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]} 
to the power of $7^3 = 343$,
we get a polynomial of degree 343
with the leading coefficient 1.
When we subtract \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]} from it,
it will have -1, which is 6 in this case,
as last but one coefficient.
Taking this modulo to the random polynomial $g$,
we get the polynomial \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{3},\mathrm{6}\mskip1.5mu]}, which is 
$6x^2 + 3x$ and certainly not constant.
$g$ is therefore not irreducible.

Let us try another one:

\ensuremath{\Varid{g}\leftarrow \Varid{randomPoly}\;\mathrm{7}\;\mathrm{4}}\\

This time, I get \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{3},\mathrm{1},\mathrm{4},\mathrm{4}\mskip1.5mu]}.
Calling \ensuremath{\Varid{irreducible}\;\mathrm{7}\;\Varid{g}} says: \ensuremath{\Conid{True}}.
When we take $x^{7^3} - x$ modulo $g$,
we get \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu]}. But we do not get a 
constant polynomial for $x^7 - x$ or
$x^{7^2} -x $. \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{3},\mathrm{1},\mathrm{4},\mathrm{4}\mskip1.5mu]}, hence,
is irreducible.

The formula, however, is not only interesting
for testing irreducibility.
What the formula states is in fact that
all irreducible polynomials of degree $d$
are factors of $x^{q^d} - x$.
Whenever we construct this expression,
we have created the product of all 
irreducible polynomials of degree $d$.
The irreducible factors of the polynomial we want to factor
are part of this product and we can get them out
just by asking for the greatest common divisor
of $x^{p^d} - x$ and the polynomial we want to factor.
This would give us the product of all factors
of our polynomial of a given degree.

We need to add one more qualification however.
Since we are searching for a \term{unique}
factorisation, we should make sure that
we always make the polynomial \term{monic},
that is, we should remove the leading coefficient
by dividing all coefficients by it.
This corresponds to content-and-primitive-part factorisation
as already discussed above, but in the case of modular
arithmetic it is much simpler. Whatever the leading coefficients
is, we can just multiply all coefficients by its inverse
without worrying about coefficients becoming fractions.
Here is an implementation:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{monicp}\mathbin{::}\Conid{Integer}\to \Conid{Poly}\;\Conid{Integer}\to \Conid{Poly}\;\Conid{Integer}{}\<[E]%
\\
\>[3]{}\Varid{monicp}\;\Varid{p}\;\Varid{u}\mathrel{=}{}\<[17]%
\>[17]{}\mathbf{let}\;{}\<[22]%
\>[22]{}\Varid{cs}{}\<[26]%
\>[26]{}\mathrel{=}\Varid{coeffs}\;\Varid{u}{}\<[E]%
\\
\>[22]{}\Varid{k}{}\<[26]%
\>[26]{}\mathrel{=}\Varid{last}\;\Varid{cs}\mathbin{`\Varid{\Conid{M}.inverse}`}\Varid{p}{}\<[E]%
\\
\>[17]{}\mathbf{in}\;\Conid{P}\;(\Varid{map}\;(\Varid{modmul}\;\Varid{p}\;\Varid{k})\;\Varid{cs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The following function, obtains the products of the factors
of a given (monic) polynomial degree by degree. Note
that we give the result back as a monic
polynomial again. Each result is a tuple of the degree and
the corresponding factor product.

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}c<{\hspost}@{}}%
\column{22E}{@{}l@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{36}{@{}>{\hspre}l<{\hspost}@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{44}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{ddfac}\mathbin{::}\Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}\to [\mskip1.5mu (\Conid{Int},\Conid{Poly}\;\Conid{Natural})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{ddfac}\;\Varid{p}\;\Varid{u}{}\<[15]%
\>[15]{}\mathrel{=}\Varid{go}\;\mathrm{1}\;\Varid{u}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu]){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{n}{}\<[15]%
\>[15]{}\mathrel{=}\Varid{degree}\;\Varid{u}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{d}\;\Varid{v}\;\Varid{x}{}\<[22]%
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{degree}\;\Varid{v}\leq \mathrm{0}{}\<[40]%
\>[40]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{otherwise}{}\<[40]%
\>[40]{}\mathrel{=}{}\<[E]%
\\
\>[25]{}\mathbf{let}\;{}\<[30]%
\>[30]{}\Varid{x'}{}\<[40]%
\>[40]{}\mathrel{=}\Varid{powmp}\;\Varid{p}\;\Varid{p}\;\Varid{x}{}\<[E]%
\\
\>[30]{}\Varid{t}{}\<[40]%
\>[40]{}\mathrel{=}\Varid{addp}\;\Varid{p}\;\Varid{x'}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\Varid{p}\mathbin{-}\mathrm{1}\mskip1.5mu]){}\<[E]%
\\
\>[30]{}\Varid{g}{}\<[40]%
\>[40]{}\mathrel{=}\Varid{gcdmp}\;\Varid{p}\;\Varid{t}\;\Varid{v}{}\<[E]%
\\
\>[30]{}(\Varid{v'},\anonymous ){}\<[40]%
\>[40]{}\mathrel{=}\Varid{divmp}\;\Varid{p}\;\Varid{v}\;\Varid{g}{}\<[E]%
\\
\>[30]{}\Varid{r}{}\<[40]%
\>[40]{}\mathrel{=}(\Varid{d},\Varid{monicp}\;\Varid{p}\;\Varid{g}){}\<[E]%
\\
\>[25]{}\mathbf{in}\;{}\<[29]%
\>[29]{}\mathbf{case}\;\Varid{g}\;\mathbf{of}{}\<[E]%
\\
\>[29]{}\Conid{P}\;[\mskip1.5mu \anonymous \mskip1.5mu]{}\<[36]%
\>[36]{}\to {}\<[44]%
\>[44]{}\Varid{go}\;(\Varid{d}\mathbin{+}\mathrm{1})\;\Varid{v'}\;(\Varid{pmmod}\;\Varid{p}\;\Varid{x'}\;\Varid{u}){}\<[E]%
\\
\>[29]{}\anonymous {}\<[36]%
\>[36]{}\to \Varid{r}\mathbin{:}{}\<[44]%
\>[44]{}\Varid{go}\;(\Varid{d}\mathbin{+}\mathrm{1})\;\Varid{v'}\;(\Varid{pmmod}\;\Varid{p}\;\Varid{x'}\;\Varid{u}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The real work is done by function \ensuremath{\Varid{go}}.
It starts with degree $d=1$, the polynomial $u$
we want to factor and, again, the simple polynomial $x$.
We then raise $x$ to the power $p^1$ for the first degree,
subtract $x$ from the result and compute the \ensuremath{\Varid{gcd}}.
If the result is a constant polynomial,
there are no non-trivial factors of this degree and we continue.
Otherwise, we store the result with the degree,
making $g$ monic. 

We continue with the next degree, $d+1$,
the quotient of the polynomial we started with and
the factor product $g$ we obtained and
the power of \ensuremath{\Varid{x'}} reduced to the modulo $u$.
The latter is again an optimisation.
The former, however, is essential to avoid
generating the same factor product over and over again.
By dividing the input polynomial by $g$, we make sure
that the factors we have already found are taken out.
This works only if the polynomial is squarefree of course.
(You might remember the discussion of squarefree
numbers in the context of Euler's theorem where we found
that, if $n$ is squarefree, then 
$\varphi(n) = \prod_{p|n}{p-1}$, \ie\ the totient number
of $n$ is the product of the primes in the factorisation
of $n$ all reduced by 1.)
We need to come back to this topic and, for the moment,
make sure that we only apply polynomials that are
squarefree and monic.

We try \ensuremath{\Varid{ddfac}} on the 4-degree polynomial 
$u(x) = x^4 + x^3 + 3x^2 + 4 x + 5$ modulo 7 and call
\ensuremath{\Varid{ddfac}\;\mathrm{7}\;\Varid{u}} and obtain the result

\ensuremath{[\mskip1.5mu (\mathrm{1},\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu]),(\mathrm{2},\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu])\mskip1.5mu]},

\ie\ the factor product $x^2 + 4x + 2$ for degree 1
and the factor product $x^2 + 4x + 6$ for degree 2.
First, we make sure that these are really factors
of $u$ by calling \ensuremath{\Varid{divmp}\;\mathrm{7}\;(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu])}, which shows

\ensuremath{(\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{0}\mskip1.5mu])}.

We can conclude that these are indeed all the factors
of $u$. But, obviously, \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu]} or $x^2 + 4x + 2$ is
not irreducible, since it is a second-degree polynomial,
but it was obtained for the irreducible factors of degree 1.
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu]}, on the other hand, was obtained for degree 2
and is itself of degree 2. We can therefore assume that
it is already irreducible, but let us check: 
\ensuremath{\Varid{irreducible}\;\mathrm{7}\;(\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu])}, indeed, yields \ensuremath{\Conid{True}}.

But what about the other one? How can we get the irreducible
factors out of that one? Here Cantor and Zassenhaus come in.
They proposed a simple algorithm with the following logic.
We, again, use the magic polynomial $x^{p^d} - x$, but choose 
a specific polynomial for $x$, say $t$. We already have
that chunk of irreducible polynomials hidden in \ensuremath{(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu])},
let us call it $u$,
and know that those polynomials are factors of both,
$t^{p^d} - t$ and $u$. 
The approach of Cantor and Zassenhaus is to split the factors
so that the problem reduces significantly. We can split $t$
into three parts using the equality

\begin{equation}
t^{p^d} - t = t(t^{(p^d-1)/2} + 1)(t^{(p^d-1)/2} - 1).
\end{equation}

Make sure that the equality holds 
by multiplying the right-hand side out.
By a careful choice of $t$, we can make sure that the factors
are likely to be more or less equally distributed among the
latter two factors. That, indeed, would reduce the problem significantly.

Since $u$ and $t^{p^d} - t$ share factors, we can transform
the equality into the following variant:

\begin{equation}\label{eq:polyFac_CZ1}
u = \gcd(u,t)\times\gcd(u,(t^{(p^d-1)/2} + 1))\times\gcd(u,((t^{(p^d-1)/2} - 1))
\end{equation}

A reasonable choice for $t$ is a polynomial
of degree $2d-1$. 
With high probability, the factors are equally distributed
among the latter two factors of the equation and 
we indeed reduce the problem significantly.
To do so, we compute one of the $\gcd$s and continue splitting
this $\gcd$ and the quotient of $u$ and the $\gcd$ further.
Should we be unlucky (the $\gcd$ contains either no or all
of the factors), we just try again with another choice for $t$.
After some tries (less than three according to common wisdom), 
we will hit a common factor.

There is an issue, however, for $p=2$.
Because in that case, 
$t^{(p^d-1)/2} - 1 = t^{(p^d-1)/2} + 1$.
Consider, to illustrate that, a polynomial modulo 2, for instance
\ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu]} and $d=3$. Then we have 

\[
(p^d-1)/2 = (2^3-1)/2 = 7/2 = 3.
\]

We raise the polynomial to the power of 3 and get \ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}.
When we add \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]}, we get \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}. 
But what do we subtract? Let us try \ensuremath{\Varid{modp}\;\mathrm{2}\;(\Conid{P}\;[\mskip1.5mu \mathbin{-}\mathrm{1}\mskip1.5mu])}.
We get back \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{1}\mskip1.5mu]}. 
Adding and subtracting 1 is just the same thing here.

But that would mean that our formula would be much poorer.
We would not have three different factors, but only two, namely
$t$ and $t^{(p^d-1)/2} + 1$. Unfortunately, it is very likely
that all the factors end up in the second one and with this,
we would not simplify the problem.

The fact that we are now working modulo 2
may help. We first observe that, modulo 2, there is
no difference between the polynomials 
$t^{2^d} - t$ (the magic one with $p=2$) and 
$t^{2^d} + t$. 
The second one, however, is easy to split,
when we set 

\[
w = t + t^2 + t^4 + \dots + t^{2^{d-1}}.
\]

Then, $w^2$ would be 

\[
t^2 + t^4 + \dots + t^{2d}.
\]

This may shock you on the first sight.
But remember, we are still working modulo p 
and we have (\term{freshman's dream}):

\[
(a+b)^p \equiv a^p + b^p \pmod{p}.
\]

When multiplying $w$ by itself, we would get

\[
t^2 + 2t^3 + t^4 + 2t^5 + 2t^6 + t^8.
\]

Since we are working modulo 2,
all terms with even coefficients cancel out,
we, hence, get

\[
t^2 + t^4 + t^8.
\]

Now, observe that 

\[
w^2 + w = t^2 + t^4 + \dots + t^{2^d} + t + t^2 + \dots + t^{2^{d-1}},
\]

when we rearrange according to exponents, we again get pairs of equal terms:

\[
w^2 + w = t + 2t^2 + 2t^4 + \dots +  2t^{2^{d-1}} + t^{2^d}.
\]

When we compute this modulo 2, again all terms with even coefficients
fall away and we finally get

\begin{equation}
w^2 + w = t^{2^d} + t.
\end{equation}

The point of all this is that we can split the expression $w^2 + w$
into two more or less equal parts, just by factoring $w$ out:
$w(w+1)$. Now, it is again very probable that we find common divisors
in both of the factors, $w$ or $w+1$ making it likely that we can
reduce the problem by taking the $\gcd$ with one of them.
Here is an implementation of the Cantor-Zassenhaus algorithm:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{cz}\mathbin{::}\Conid{Natural}\to \Conid{Int}\to \Conid{Poly}\;\Conid{Natural}\to \Conid{IO}\;[\mskip1.5mu \Conid{Poly}\;\Conid{Natural}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cz}\;\Varid{p}\;\Varid{d}\;\Varid{u}{}\<[13]%
\>[13]{}\mid \Varid{n}\leq \Varid{d}{}\<[26]%
\>[26]{}\mathrel{=}\Varid{return}\;[\mskip1.5mu \Varid{monicp}\;\Varid{p}\;\Varid{u}\mskip1.5mu]{}\<[E]%
\\
\>[13]{}\mid \Varid{otherwise}{}\<[26]%
\>[26]{}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{x}\leftarrow \Varid{monicp}\;\Varid{p}\mathbin{<\$>}\Varid{randomPoly}\;\Varid{p}\;(\mathrm{2}\mathbin{*}\Varid{d}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{t}{}\<[12]%
\>[12]{}\mid \Varid{p}\equiv \mathrm{2}{}\<[25]%
\>[25]{}\mathrel{=}\Varid{addsquares}\;(\Varid{d}\mathbin{-}\mathrm{1})\;\Varid{p}\;\Varid{x}\;\Varid{u}{}\<[E]%
\\
\>[12]{}\mid \Varid{otherwise}{}\<[25]%
\>[25]{}\mathrel{=}\Varid{addp}\;\Varid{p}\;(\Varid{powmodp}\;\Varid{p}\;\Varid{m}\;\Varid{x}\;\Varid{u})\;(\Conid{P}\;[\mskip1.5mu \Varid{p}\mathbin{-}\mathrm{1}\mskip1.5mu]){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{r}\mathrel{=}\Varid{gcdmp}\;\Varid{p}\;\Varid{u}\;\Varid{t}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{if}\;\Varid{degree}\;\Varid{r}\equiv \mathrm{0}\mathrel{\vee}\Varid{degree}\;\Varid{r}\equiv \Varid{n}\;\mathbf{then}\;\Varid{cz}\;\Varid{p}\;\Varid{d}\;\Varid{u}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\mathbf{else}\;\mathbf{do}\;{}\<[16]%
\>[16]{}\Varid{r1}\leftarrow \Varid{cz}\;\Varid{p}\;\Varid{d}\;\Varid{r}{}\<[E]%
\\
\>[16]{}\Varid{r2}\leftarrow \Varid{cz}\;\Varid{p}\;\Varid{d}\;(\Varid{fst}\mathbin{\$}\Varid{divmp}\;\Varid{p}\;\Varid{u}\;\Varid{r}){}\<[E]%
\\
\>[16]{}\Varid{return}\;(\Varid{r1}\plus \Varid{r2}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{n}\mathrel{=}\Varid{degree}\;\Varid{u}{}\<[E]%
\\
\>[12]{}\Varid{m}\mathrel{=}(\Varid{p}\mathbin{\uparrow}\Varid{d}\mathbin{-}\mathrm{1})\mathbin{\Varid{`div`}}\mathrm{2}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function receives a natural number,
that is the modulus $p$, an \ensuremath{\Conid{Int}}, $d$, for the degree,
and the polynomial $u$, the factor product,
which we both obtained from \ensuremath{\Varid{ddfac}}.
When the degree is equal or greater than $n$,
the degree of $u$, we are done: we already have
a factor of the predicted degree.
Otherwise, we generate a random monic polynomial
of degree $2d-1$. Note that, since \ensuremath{\Varid{randomPoly}}
expects the number of coefficients, 
which is $d+1$, we just pass $2d$.

Then we calculate $t$. If $p$ is 2, we use \ensuremath{\Varid{addsquares}},
at which we will look in a moment. Otherwise,
we raise the random polynomial to the power of $(p^d-1)/2$
and subtract 1. That is the third factor of equation
\ref{eq:polyFac_CZ1}. We compute the $\gcd$ and,
if the result has either degree 0 (no factor was found)
or the same degree as $u$ (all factors are in this one),
we just try again with another random polynomial.
Otherwise, we continue with the $\gcd$ and the quotient
$u/\gcd$.

Let us try this for the result \ensuremath{(\mathrm{1},\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu])} we obtained
earlier from applying \ensuremath{\Varid{ddfac}} on \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{4},\mathrm{3},\mathrm{1},\mathrm{1}\mskip1.5mu]}.
\ensuremath{\Varid{cz}\;\mathrm{7}\;\mathrm{1}\;(\Conid{P}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{1}\mskip1.5mu])} gives 

\ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{1}\mskip1.5mu]\mskip1.5mu]},

two irreducible polynomials of degree 1.
The complete factorisation of \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{4},\mathrm{3},\mathrm{1},\mathrm{1}\mskip1.5mu]}, hence, is

\ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu]\mskip1.5mu]},

which we can test by calling 
\ensuremath{\Varid{prodp}\;(\Varid{mulmp}\;\mathrm{7})\;[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{6},\mathrm{4},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}
and we, indeed, get \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{5},\mathrm{4},\mathrm{3},\mathrm{1},\mathrm{1}\mskip1.5mu]} back.

For the case where $p=2$, we use the function addsquares:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}c<{\hspost}@{}}%
\column{34E}{@{}l@{}}%
\column{37}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{addsquares}\mathbin{::}\Conid{Int}\to \Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}\to \Conid{Poly}\;\Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{addsquares}\;\Varid{i}\;\Varid{p}\;\Varid{x}\;\Varid{u}\mathrel{=}\Varid{go}\;\Varid{i}\;\Varid{x}\;\Varid{x}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\mathrm{0}\;\Varid{w}\;\anonymous {}\<[22]%
\>[22]{}\mathrel{=}\Varid{w}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{k}\;\Varid{w}\;\Varid{t}{}\<[22]%
\>[22]{}\mathrel{=}{}\<[25]%
\>[25]{}\mathbf{let}\;{}\<[30]%
\>[30]{}\Varid{t'}{}\<[34]%
\>[34]{}\mathrel{=}{}\<[34E]%
\>[37]{}\Varid{pmmod}\;\Varid{p}\;(\Varid{powmp}\;\Varid{p}\;\Varid{p}\;\Varid{t})\;\Varid{u}{}\<[E]%
\\
\>[30]{}\Varid{w'}{}\<[34]%
\>[34]{}\mathrel{=}{}\<[34E]%
\>[37]{}\Varid{addp}\;\Varid{p}\;\Varid{w}\;\Varid{t'}{}\<[E]%
\\
\>[25]{}\mathbf{in}\;\Varid{go}\;(\Varid{k}\mathbin{-}\mathrm{1})\;\Varid{w'}\;\Varid{t'}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

which just computes $w$ as $t + t^2 + t^4 + \dots t^{2^{d-1}}$.

Let us try \ensuremath{\Varid{ddfac}} and \ensuremath{\Varid{cz}} with a polynomial modulo 2,
\eg\ \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]}, which is of degree 8 and
is squarefree (and, per definition, monic).
The call \ensuremath{\Varid{ddfac}\;\mathrm{2}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu])} gives
us three chunks of factors:

\ensuremath{[\mskip1.5mu (\mathrm{1},\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu]),(\mathrm{2},\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]),(\mathrm{4},\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu])\mskip1.5mu]}.

We see at once that the second and third polynomials
are already irreducible, since they already have 
the specified degree. The first one, however, is
of degree 2, but shall contain factors of degree 1.
So, let us see what \ensuremath{\Varid{cz}\;\mathrm{2}\;\mathrm{1}\;(\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1}\mskip1.5mu])} will yield:

\ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}.

The complete factorisation of \ensuremath{\Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{0},\mathrm{0},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]},
hence, is

\ensuremath{[\mskip1.5mu \Conid{P}\;[\mskip1.5mu \mathrm{0},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu],\Conid{P}\;[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}.

Now, we still have to solve the problem of polynomials
containing squared factors, \ie\ repeated roots.
There is in fact a method to find such factors
adopted from calculus and, again, related to the
derivative. The point is that a polynomial and
its derivative share only those divisors that
appear more than once in the factorisation.
We have not enough knowledge on derivatives yet to
prove that here rigorously, but we can give an
intuition.

Consider a polynomial with the factorisation

\[
(x+a)(x+b)\dots
\]

This is a product and, to find the derivative
of this polynomial, we need to apply the \term{product rule}
(which we will study in part 3). The product rule states that

\begin{equation}
(f \times g)' = fg' + f'g,
\end{equation}

\ie\ the derivative of the product of $f$ and $g$
is the sum of the product of $f$ and the derivative of $g$
and the product of the derivative of $f$ and $g$.

The derivatives of the individual factors $(x+a)(x+b)$
all reduce to 1, since for $f = x^1 + a$,
$f' = 1\times x^0 = 1$. The product of factors, hence,
turns into a sum of factors:

\[
1\times(x+a) + 1\times(x+b) = (x+a) + (x+b).
\]

Let us compute the polynomial
with the factors $(x+a)(x+b)$. The polynomial is
$x^2 + ax + bx + ab$. Its derivative is
$2x + a + b$. When we apply the product rule to
the factors, we get $(x+a)+(x+b)= x+a+x+b = 2x+a+b$,
which is indeed the same result.

It is intuitively clear that the sum of the factors
is not the same as the product of those same factors
and, even further, that none of the factors is preserved.
They all disappear in favour of others they do not
share divisors with, because, since the factors are
coprime to each other, they do not share divisors
with their sum either.

To elaborate on this, consider now polynomials
with more than two factors of the form

\[
abc\dots,
\]

where $a$, $b$ and $c$ stand for irreducible polynomials like
$(x+\alpha)$, $(x+\beta)$ and so on.

We apply the product rule on the first two factors
and get:

\[
(a'b + ab')\dots
\]

When we now apply the product rule once again,
we would multiply $c$ with the derivative of $ab$
(which is $a'b + ab'$) and the derivative of $c$,
$c'$, with the original $ab$ and get:

\[
(a'b + ab')c + abc' = a'bc + ab'c + abc'.
\]

We see that we end up with the sum of the products
of the original factors, with the current factor $i$
substituted by something else, namely the derivative
of this factor. This can be represented nicely as:

\begin{equation}\label{eq:polyFacProductRule}
\left(\prod_{i=0}^k{a_i}\right)' = 
\sum_{i=0}^k{\left(a_i'\prod_{j\neq i}{a_j}\right)}
\end{equation}

There is a remarkable similarity to the structure
we found in analysing the Chinese remainder theorem,
when we divided the product of all remainders by
the current remainder. Just as in the Chinese remainder
theorem, each of the terms resulting from the product
rule is coprime to the original factor at the same position,
since it is the product of irreducible factors 
(which, hence, are coprime to each other) and the derivative
of that factor, which, for sure, does not share
divisors with the original factor at that position.

When we have a repeated factor, however, as in
the following polynomial

\[
(x+a)(x+a)(x+b)\dots,
\]

then this factor is preserved. The product rule
will create the factor $x+a+x+a=2x+2a$, which
is a multiple of the original factor, which,
in its turn, is therefore preserved.

Suppose we want to compute the factorisation
of 

\begin{equation}
f = a_1a_2^2a_3^3\dots a_k^k,
\end{equation}

where the $a$s represent the products of all
the factors raised to the indicated exponent,
then, since the derivative preserves the factors,
the $\gcd$ of $f$ and its derivative $f'$
(whatever that looks like) is:

\begin{equation}\label{eq:polyFacYun1}
\gcd(f,f') = a_2^1a_3^2\dots a_k^{k-1},
\end{equation}

\ie\, the repeated factors with the exponent
decreased by one. Then $f$ divided by the $\gcd$
gives us

\begin{equation}\label{eq:polyFacYun2}
\frac{f}{\gcd(f,f')} = a_1a_2a_3\dots a_k,
\end{equation}

all the factors reduced to their first power.
Now, if we continue this scheme using
the $\gcd(f,f')$ and $f/\gcd(f,f')$ as input,
we would get \ref{eq:polyFacYun1} 
reduced one more ($a_3a_4^2\dots$) and
\ref{eq:polyFacYun2} with the head chopped off
($a_2a_3\dots$). The quotient of the two
versions of \ref{eq:polyFacYun2}, \ie\

\[
\frac{a_1a_2a_3\dots}{a_2a_3\dots},
\]

would give us the head. The head, however,
is the product of all factors with a given exponent.

In a finite field, this, unfortunately does not
work in all cases. Problematic are all coefficients
with exponents that are multiples of the modulus.
When we compute $nc^{n-1}$, for $n$ an exponent
in the original polynomial that is a multiple of
the modulus, the coefficient itself becomes zero.
If we are unlucky, the derivative \term{disappears},
\ie\ it becomes zero. A simple example is the polynomial
$x^4 \pmod{2}$. When we compute the derivative, we get
$4x^3$. Unfortunately, 4 is a multiple of 2 and,
therefore, the only nonzero coefficient we had
in the original polynomial becomes zero and the entire
derivative disappears.

What we can do, however, is to keep the coefficients
with exponents that are multiples of the modulus
separated from those that are not.
As in the algorithm for infinte fields, we would
iteratively compute two sequences of values,
namely $T_{k+1} = T_k/V_{k+1}$ with $T_1 = \gcd(f,f')$
and $V_{k+1} = \gcd(T_k,V_k)$ with $V_1 = f/T_1$.
But we would now deviate for all $k$ that are
multiples of $p$, \viz\: 

\[
V_{k+1} = \begin{cases}
            \gcd(T_k,V_k) & if~p\nmid k~\textrm{(as before)}\\
            V_k & if~p\mid k
          \end{cases}
\]

At each step, we have

\begin{equation}
V_k = \prod_{i\ge k, p\nmid i}{a_i},
\end{equation}

\ie, the product of all $a$s with exponents greater than those
that we have already processed and that do not divide $p$, and

\begin{equation}
T_k = \prod_{i\ge k, p\nmid i}{a_i^{i-k}}
      \prod_{i\ge k, p\mid i}{a_i^i},
\end{equation}

\ie, the product of the powers greater than those 
we have already processed for both cases $p\mid i$ and $p\nmid i$.
For the cases $p\nmid i$, everything is as before.
For the cases $p\mid i$, we will end up, when we have reduced
$V_k$ to a constant polynomial, with a product of all the powers
of the $a$s with exponents that are multiples of $p$.

To get these $a$s out, we divide all all exponents by $p$ 
and repeat the whole algorithm. For the return value, \ie\ the
factors, we need to remember the original exponent,
but that is easily done as shown below.

Note that for polynomials with many coefficients,
this recursion step will occur more than once.
The exponents that are multiples of $p$ 
in such a polynomial have the form

\[
0p,p,2p,3p,4p,\dots
\]

Dividing by $p$, we get 

\[
0,1,p,2p,3p,\dots
\]

So, we need to repeat, until there are no more multiples of $p$.
Here is the algorithm:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{36}{@{}>{\hspre}l<{\hspost}@{}}%
\column{39}{@{}>{\hspre}l<{\hspost}@{}}%
\column{42}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{47}{@{}>{\hspre}l<{\hspost}@{}}%
\column{48}{@{}>{\hspre}l<{\hspost}@{}}%
\column{53}{@{}>{\hspre}l<{\hspost}@{}}%
\column{54}{@{}>{\hspre}l<{\hspost}@{}}%
\column{58}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{sqmp}\mathbin{::}\Conid{Integer}\to \Conid{Integer}\to \Conid{Poly}\;\Conid{Integer}\to [\mskip1.5mu (\Conid{Integer},\Conid{Poly}\;\Conid{Integer})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{sqmp}\;\Varid{p}\;\Varid{e}\;\Varid{u}{}\<[15]%
\>[15]{}\mid \Varid{degree}\;\Varid{u}\mathbin{<}\mathrm{1}{}\<[31]%
\>[31]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[15]{}\mid \Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[34]%
\>[34]{}\mathbf{let}\;{}\<[39]%
\>[39]{}\Varid{u'}{}\<[43]%
\>[43]{}\mathrel{=}\Varid{derivative}\;(\Varid{modmul}\;\Varid{p})\;\Varid{u}{}\<[E]%
\\
\>[39]{}\Varid{t}{}\<[43]%
\>[43]{}\mathrel{=}\Varid{gcdmp}\;\Varid{p}\;\Varid{u}\;\Varid{u'}{}\<[E]%
\\
\>[39]{}\Varid{v}{}\<[43]%
\>[43]{}\mathrel{=}\Varid{fst}\;(\Varid{divmp}\;\Varid{p}\;\Varid{u}\;\Varid{t}){}\<[E]%
\\
\>[34]{}\mathbf{in}\;\Varid{go}\;\mathrm{1}\;\Varid{t}\;\Varid{v}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\Varid{k}\;\Varid{tk}\;\Varid{vk}\mathrel{=}{}\<[26]%
\>[26]{}\mathbf{let}\;{}\<[31]%
\>[31]{}\Varid{vk'}{}\<[36]%
\>[36]{}\mid \Varid{k}\mathbin{\Varid{`rem`}}\Varid{p}\not\equiv \mathrm{0}\mathrel{=}\Varid{gcdmp}\;\Varid{p}\;\Varid{tk}\;\Varid{vk}{}\<[E]%
\\
\>[36]{}\mid \Varid{otherwise}{}\<[53]%
\>[53]{}\mathrel{=}\Varid{vk}{}\<[E]%
\\
\>[31]{}\Varid{tk'}{}\<[36]%
\>[36]{}\mathrel{=}\Varid{fst}\;(\Varid{divmp}\;\Varid{p}\;\Varid{tk}\;\Varid{vk'}){}\<[E]%
\\
\>[31]{}\Varid{k'}{}\<[36]%
\>[36]{}\mathrel{=}\Varid{k}\mathbin{+}\mathrm{1}{}\<[E]%
\\
\>[26]{}\mathbf{in}\;\mathbf{case}\;\Varid{divmp}\;\Varid{p}\;\Varid{vk}\;\Varid{vk'}\;\mathbf{of}{}\<[E]%
\\
\>[26]{}\hsindent{5}{}\<[31]%
\>[31]{}(\Conid{P}\;[\mskip1.5mu \anonymous \mskip1.5mu],\anonymous ){}\<[42]%
\>[42]{}\to {}\<[58]%
\>[58]{}\Varid{nextStep}\;\Varid{k'}\;\Varid{tk'}\;\Varid{vk'}{}\<[E]%
\\
\>[26]{}\hsindent{5}{}\<[31]%
\>[31]{}(\Varid{f},\anonymous ){}\<[42]%
\>[42]{}\to (\Varid{k}\mathbin{*}\Varid{p}\mathbin{\uparrow}\Varid{e},\Varid{f})\mathbin{:}{}\<[58]%
\>[58]{}\Varid{nextStep}\;\Varid{k'}\;\Varid{tk'}\;\Varid{vk'}{}\<[E]%
\\
\>[12]{}\Varid{nextStep}\;\Varid{k}\;\Varid{tk}\;\Varid{vk}{}\<[30]%
\>[30]{}\mid {}\<[30E]%
\>[33]{}\Varid{degree}\;\Varid{vk}\mathbin{>}\mathrm{0}{}\<[48]%
\>[48]{}\mathrel{=}\Varid{go}\;\Varid{k}\;\Varid{tk}\;\Varid{vk}{}\<[E]%
\\
\>[30]{}\mid {}\<[30E]%
\>[33]{}\Varid{degree}\;\Varid{tk}\mathbin{>}\mathrm{0}{}\<[48]%
\>[48]{}\mathrel{=}\Varid{sqmp}\;\Varid{p}\;(\Varid{e}\mathbin{+}\mathrm{1})\;(\Varid{dividedTk}\;\Varid{tk}){}\<[E]%
\\
\>[30]{}\mid {}\<[30E]%
\>[33]{}\Varid{otherwise}{}\<[48]%
\>[48]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{dividedTk}\;\Varid{tk}\mathrel{=}\Varid{poly}\;(\Varid{divExp}\;\mathrm{0}\;(\Varid{coeffs}\;\Varid{tk})){}\<[E]%
\\
\>[12]{}\Varid{divExp}\;\anonymous \;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{divExp}\;\Varid{i}\;(\Varid{c}\mathbin{:}\Varid{cs}){}\<[29]%
\>[29]{}\mid \Varid{i}\mathbin{\Varid{`rem`}}\Varid{p}\equiv \mathrm{0}{}\<[47]%
\>[47]{}\mathrel{=}\Varid{c}\mathbin{:}{}\<[54]%
\>[54]{}\Varid{divExp}\;(\Varid{i}\mathbin{+}\mathrm{1})\;\Varid{cs}{}\<[E]%
\\
\>[29]{}\mid \Varid{otherwise}{}\<[47]%
\>[47]{}\mathrel{=}{}\<[54]%
\>[54]{}\Varid{divExp}\;(\Varid{i}\mathbin{+}\mathrm{1})\;\Varid{cs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

As usual, the hard work is done in the local function \ensuremath{\Varid{go}},
which takes three arguments, $k$, $t_k$ and $v_k$.
We initialise $k=1$, $t_k = \gcd(u,u')$ and $v_k = u/t_k$.
We set $v_{k+1} = \gcd(t_k,v_k)$, if $p\nmid k$,
and $v_{k+1}=v_k$, otherwise.
We further set $t_{k+1} = t_k/v_{k+1}$ and $k = k+1$.
If $v_k / v_{k+1}$ (this is the head)
is not constant (otherwise it is irrelevant),
we remember the result as the product of factors with this
exponent. Note that the overall result is a list of tuples,
where the first element represents the exponent and
the second the factor product. The exponent is calculated
as $k\times p^e$. The number $e$, here, is not the
Euler-Napier constant, but a variable passed in to \ensuremath{\Varid{sqmp}}.
We would start the algorithm with $e=0$. We, hence, get
$k\times p^0 = k\times 1 = k$ for the first recursion.

The function \ensuremath{\Varid{nextStep}} is just a convenient wrapper
for the decision of how to continue.
If $v_k$ is not yet constant, we continue with 
$go~(k+1)~t_{k+1}~v_{k+1}$.
Otherwise, if $t_k$ is not yet constant, we continue
with \ensuremath{\Varid{sqmp}} with $e+1$ and $t_k$ with exponents
that are multiples of $p$ divided by $p$.

For bootstrapping the algorithm, we can define
a simple function with a reasonable name that
calls \ensuremath{\Varid{sqmp}} with $e=0$:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{squarefactormod}\mathbin{::}\Conid{Integer}\to \Conid{Poly}\;\Conid{Integer}\to [\mskip1.5mu (\Conid{Integer},\Conid{Poly}\;\Conid{Integer})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{squarefactormod}\;\Varid{p}\mathrel{=}\Varid{sqmp}\;\Varid{p}\;\mathrm{0}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Finally, we are ready to put everything together:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{28}{@{}>{\hspre}c<{\hspost}@{}}%
\column{28E}{@{}l@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{44}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{cantorzassenhaus}\mathbin{::}\Conid{Integer}\to \Conid{Poly}\;\Conid{Integer}\to \Conid{IO}\;[\mskip1.5mu (\Conid{Integer},\Conid{Poly}\;\Conid{Integer})\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{cantorzassenhaus}\;\Varid{p}\;\Varid{u}{}\<[25]%
\>[25]{}\mid \Varid{irreducible}\;\Varid{p}\;\Varid{m}{}\<[44]%
\>[44]{}\mathrel{=}\Varid{return}\;[\mskip1.5mu (\mathrm{1},\Varid{m})\mskip1.5mu]{}\<[E]%
\\
\>[25]{}\mid \Varid{otherwise}{}\<[44]%
\>[44]{}\mathrel{=}{}\<[E]%
\\
\>[25]{}\hsindent{5}{}\<[30]%
\>[30]{}\Varid{concat}\mathbin{<\$>}{}\<[43]%
\>[43]{}\Varid{mapM}\;\Varid{mexpcz}\;[\mskip1.5mu (\Varid{e},\Varid{ddfac}\;\Varid{p}\;\Varid{f})\mid {}\<[E]%
\\
\>[43]{}(\Varid{e},\Varid{f})\leftarrow \Varid{squarefactormod}\;\Varid{p}\;\Varid{m}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{m}\mathrel{=}\Varid{monicp}\;\Varid{p}\;\Varid{u}{}\<[E]%
\\
\>[12]{}\Varid{expcz}\;\Varid{e}\;(\Varid{d},\Varid{v}){}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Varid{map}\;(\lambda \Varid{f}\to (\Varid{e},\Varid{f}))\mathbin{<\$>}\Varid{cz}\;\Varid{p}\;\Varid{d}\;\Varid{v}{}\<[E]%
\\
\>[12]{}\Varid{mexpcz}\;(\Varid{e},\Varid{dds}){}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Varid{concat}\mathbin{<\$>}\Varid{mapM}\;(\Varid{expcz}\;\Varid{e})\;\Varid{dds}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

Hans Zassenhaus worked most of his life as a computeralgeabrist and
was important for the development of this area of mathematics and
computer science. He was born in Germany before the second world war
and studied mathematics under Emil Artin, one of the founders of
modern algebra. Zassenhaus' father was strongly influenced by
Albert Schweitzer and, as such, opposed to Nazi ideology.
Hans shared this antipathy and, to avoid being drafted to
a significant war effort like, as it would appear natural
for an algebraist, cryptography, he left university and volonteered for 
the army weather forecast where he survived the war.
Later, he would follow invitations first to the UK and later to the USA,
where he remained until his death.

His sister Hiltgunt (who, after emigrating to the USA,
preferred to use her second name Margret) studied Scandinavistics.
During the war, she worked as translator for censorship in camps
for Norwegian and Danish prisoners. She undermined censorship in
this position, maintained contact between 
prisioners and helped smuggling medicine, tobacco and food
into the prisons. For her efforts during and after the war,
she was nominated for the Nobel Peace Prize in 1974. 
Unfortunately, the societal engagement of the Zassenhaus siblings
is hardly remembered today.

\ignore{
TODO:
- why does x^p^c, with c dividing d is not divided by g?
- application to integers: hensel's lemma
}

\section{Hensel Lifting}
\ignore{
- show examples for using p > coefficients, e.g.
  x^2 - 9
  cz  13: (x+3)(x+10)
  cz  17:  "   (x+14)
  cz  31:  "   (x+28)
  cz 101:  "   (x+98), etc., i.e.:
  (x+3)(x-3)

https://en.wikipedia.org/wiki/Factorization_of_polynomials#Modern_methods

- Taylor series
- Hensel's lemma
- Hensel Lifting for roots
=> find a good example
Examples:
  x^5 + x^4 + x^2 + 2
  = (x^2 + x + 1)(x^3 - x + 2)
  cz 17: (x^2 + x + 1)(x^3 + 16x + 2)

  P [3,0,2,6,1]
  x^4 + 6x^3 + 2x^2 + 3
  cz 7: [(1,P [2,1]),(1,P [1,1]),(1,P [5,3,1])]
  so, -2 (=5) and -1 (=6) are roots
  lift the roots to whatever you want using hlift
}
\section{Enumerating the Algebraic Numbers}
\ignore{
module Cantor3
where
  import PolyModFactor
}
\ignore{
https://en.wikipedia.org/wiki/Georg_Cantor%27s_first_set_theory_article
http://www.digizeitschriften.de/dms/img/?PID=GDZPPN002155583&physid=phys268#navi
}


\end{document}
