%% =======================================================
%% (c) Tobias Schoofs
%% =======================================================
%% Math 4 Programmers - Induction
%% =======================================================

% Plain Style
\documentclass{scrreprt}

% Springer Style
%\documentclass[envcountsame]{llncs}

%% ODER: format ==         = "\mathrel{==}"
%% ODER: format /=         = "\neq "
%
%
\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
  {\@namedef{lhs2tex.lhs2tex.sty.read}{}%
   \newcommand\SkipToFmtEnd{}%
   \newcommand\EndFmtInput{}%
   \long\def\SkipToFmtEnd#1\EndFmtInput{}%
  }\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}% suggested by Neil Mitchell
\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

%mathindent has to be defined
\@ifundefined{mathindent}%
  {\newdimen\mathindent\mathindent\leftmargini}%
  {}%

\def\resethooks{%
  \global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]%
  {\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{%
  \let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{%
  \let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}% default is fixed indentation
\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%

\include{cmds}

\begin{document}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\chapter{Induction, Series, Sets and Combinatorics} % c03
\section{Logistics}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Ex1}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{import}\;\Conid{\Conid{Data}.List}\;(\Varid{intercalate}){}\<[E]%
\\
\>[B]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\mathbf{import}\;\Conid{Numbers}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  more number operations}{}\<[E]%
\\
\>[B]{}\mbox{\onelinecomment  basic sums: 1 + 2 + ... + n}{}\<[E]%
\\
\>[B]{}\mbox{\onelinecomment              1^2 + ... n^2}{}\<[E]%
\\
\>[B]{}\mbox{\onelinecomment }{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Before we continue to investigate
the properties of natural numbers,
let us deviate from pure theory for a moment 
and have a look at a motivating example
from my professional practice.
It is quite a simple case, but, for me,
it was one of the starting points 
to get involved with sequences, series, combinatorics
and other things natural numbers can do.

I was working for a logistics service provider
for media, mainly \acronym{cd}s, \acronym{dvd}s,
video games, books and so on.
The company did all the merchandise management
for its customers, mainly retailers,
and, for this purpose,
ran a set of logistics centres.
We got involved, when one of those logistics centres
was completely renewed,
in particular a new sorter system was installed
that enabled the company to comfortably serve 
all their current customers
and those expected according to 
steep growth rates
in the near future.

A sorter is a machine that reorders things.
Goods enter the warehouse ordered by the suppliers
that actually sent the goods in lots of, for instance, 
$ \num{1,000}$ boxes of album A
+ $\num{450}$ boxes of album B
+ $\num{150}$ boxes of album C.
These lots would go onto the sorter
and the sorter would reorder them into
lots according to customer orders,
\eg: customer I ordered:
    $\num{150}$ boxes of album A
  + $\num{30}$  boxes of album B 
  + $\num{10}$  boxes of album C,
customer II ordered: 
    $\num{45}$  boxes of album A
  + $\num{99}$  boxes of album B
and so on. 

Mechanically, the sorter consisted
of a huge belt with 
carrier bins attached to it
that went around in circles.
Feeders would push goods onto the carrier bins
and, at certain positions,
the bins would drop goods into buckets
on the floor beneath the belt, so called endpoints.
At any time, endpoints were assigned
to customers, so that each endpoint
ended up with goods ordered by one specific customer.

Our software was responsible for the 
configuration of the machine.
It decided, which customers were assigned
to endpoints and how goods were related to customer orders.
The really tricky task was optimising the process,
but here I would like to focus on one single
issue that, in fact, was much simpler than all
that optimisation stuff, namely
the allocation of customers to endpoints.

At any given time, 
the sorter had a certain allocation,
that is an assignment of endpoints to customers.
There were very big customers
that received several lots per day and
others that would only receive lots on certain weekdays.
Only those customers that would still receive
a lot on the same day and within the current batch 
would actually have
an allocation. The goods for those, 
currently not on the sorter,
would fall in reserved endpoints, 
called ``ragmen'', for later batches
or other weekdays.
With this logic, the sorter was able to serve
much more customers than it had endpoints,
and what we wanted to know was
how many ragmen we would need
with respect to a given amount of customers. 

Our idea for attacking the problem was the following:
we started to assume na\"ively that we could
simply split the customers by some ratio 
in those currently \term{on} (assigned to an endpoint) and
those currently \term{off} (not assigned to an endpoint).
We would split, let us say, \num{1,000} customers 
into $500$ allocated to some endpoint
and $500$ currently not allocated.
But, unfortunately, we needed some endpoints
to catch all the merchandise intended
for those currently not \term{on}.
So, we had to reserve a certain amount
of endpoints as ragmen
and subtract this number from the amount
of endpoints available for allocated customers.
A key variable was the number of customers \term{off}
per ragman endpoint.
We wanted this number, of course, to be as high
as possible, because from this relation 
came the saving in endpoints that must be reserved
as ragmen and it finally determined,
how many customers the server could serve.
On the other hand, we could not throw the goods
for all customers currently not at the sorter
into one single endpoint. This would have caused
this endpoint to overflow every few minutes
causing permant work in getting the merchandise
to a waiting zone.
This special point turned out to be quite complicated:
small customers with small lots would need
less ragman capacity than big ones; 
the problem was solved with
a classification approach,
but that does not matter here at all.
For our purpose, it is just important
that there actually was some value
to determine this relation, let us say
it was $c = 10$, meaning that we needed
a ragman endpoint for every 10 customers
not on the sorter.

We will now use the na\"ive assumption
to compute the number of ragmen 
as $r = \left\lceil\frac{n-m}{c}\right\rceil$,
where $n$ is the number of customers
and $m$ the number of available endpoints.
For our example 
of $\num{1,000}$ customers and 500 endpoints,
$r$ is $\frac{1000 - 500}{10}$,
hence, $50$ ragman endpoints.

But this result cannot be true!
We na\"ivley assumed that we have 500 endpoints.
But in the very moment
we reserve 50 endpoints as ragmen
for customers not currently on the sorter,
this number reduces instantly to $m - r$,
that is $450$ endpoints.
We, therefore, have to reserve more ragmen,
that is to say for those 50 customers that, now,
have no endpoint on the sorter anymore.
Since we need one ragman per 10 customers,
this would give $50 + 5$ ragmen.
But would this not reproduce the problem
we wanted to solve in the first place?
In the very moment, we add 5 more endpoints
to the ragmen, we have to take away $5$
from the available endpoints,
reducing the number of available endpoints once again
to $450-5 = 445$.

We end up with something called a series:
the number of ragmen equals
the number of endpoints divided by $c$ 
plus this number divided by $c$ 
plus this number divided by $c$ 
and so on. We can represent this with a nice formula as:

\begin{equation}
r = \left\lceil\frac{n - m}{c}\right\rceil 
  + \left\lceil\frac{n - m}{c^2}\right\rceil 
  + \dots
\end{equation}

Or even nicer:
\begin{equation}\label{eq2}
r = \sum_{k=1}^{\infty}\left\lceil\frac{n - m}{c^k}\right\rceil 
\end{equation}

You can easily convince yourself
that dividing $n - m$ by $c^2$ is the same
as dividing $\frac{n - m}{c}$ by $c$,
because dividing a fraction by a natural number
is equivalent to multiplying it with the denominator
(we will look at this more carefully later).
In the sum in equation \ref{eq2},
the $k$ is therefore growing with each step.

But the equation, still, has a flaw.
The inner division in the summation formula
will leave smaller and smaller values
that, at some point, become infinitesimally small.
but, since we ceil the division result,
these tiny values will always be rounded up
to one, such that the formula produces
an endless tail of ones,
which is of course not what we want.
Therefore, we should use the opposite of ceiling,
floor, but should not forget to add one additional
ragman to cope with the remainders:

\begin{equation}\label{eq3}
r = 1 + \sum_{k=1}^{\infty}{\left\lfloor\frac{n - m}{c^k}\right\rfloor}
\end{equation}

Now, when $\frac{n - m}{c^k}$ becomes less than one,
the division result is rounded down to zero
and the overall result of the summation
converges to some integer value.
For \num{1000} customers, the series converges already
for $k = 3$; we, thus, need $50 + 5 + 1 = 56$ ragmen to cope
with \num{1000} customers and
will be able to serve 444 customers on the sorter.
For, say, \num{2,000} customers, the series converges
for $k = 4$, so we need 
$\left\lfloor\frac{1500}{10}\right\rfloor   + 
 \left\lfloor\frac{1500}{100}\right\rfloor  + 
 \left\lfloor\frac{1500}{1000}\right\rfloor + 1 
 = 167$
ragmen and will have 333 endpoints \term{on}.
For \num{5,000} customers, the series, again, converges
for $k = 4$ and we will need
$\left\lfloor\frac{4500}{10}\right\rfloor   + 
 \left\lfloor\frac{4500}{100}\right\rfloor  + 
 \left\lfloor\frac{4500}{1000}\right\rfloor + 1 
 = 500$,
which is just the amount of endpoints we have available in total.
We, thus, cannot serve \num{5,000} customers with 
this configuration. We would need to increase $c$
and accept more workload in moving
goods into wating zones.

Let us look at a possible implementation
of the above with our natural numbers.
First, the notion of \term{convergence},
as we have used it above,
appears to be interesting enough to 
define a function for it. The idea is
that we sum up the results of 
a function applied to an increasing value
until the result reaches zero and, 
in consequence, will not affect the 
cumulated result anymore:

% combinator?
% r `combine` converge l f (n+1)
% with this combinator
% always being (+),
% the limit must be 0
% (it is therefore much more an identity
%  than a limit)

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{converge1}\mathbin{::}(\Conid{Natural}\to \Conid{Natural})\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[B]{}\Varid{converge1}\;\Varid{f}\;\Varid{n}\mathrel{=}{}\<[18]%
\>[18]{}\mathbf{let}\;\Varid{r}\mathrel{=}\Varid{f}\;\Varid{n}{}\<[E]%
\\
\>[18]{}\hsindent{1}{}\<[19]%
\>[19]{}\mathbf{in}\;\mathbf{if}\;\Varid{r}\equiv \mathrm{0}\;{}\<[33]%
\>[33]{}\mathbf{then}\;\Varid{r}{}\<[E]%
\\
\>[33]{}\mathbf{else}\;\Varid{r}\mathbin{+}\Varid{converge1}\;\Varid{l}\;\Varid{f}\;(\Varid{n}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The function \term{converge} receives 
a function $f$ that transforms 
a natural number into another natural number
and the natural number $n$,
which is the starting point for the series. 
We compute the result $r$ of \ensuremath{\Varid{f}\;\Varid{n}}
and if this result equals zero,
we produce the result $r$,
otherwise, we continue with \ensuremath{\Varid{n}\mathbin{+}\mathrm{1}}.

We can generalise this function
so that it is also applicable to  products.
In this case, we would not stop,
when $f$ produces 0, but when it produces 1,
the neutral element with respect to multiplication.
The definition of the generalised convergence function
must hence include the stop signal
explicitly as one of its arguments:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{47}{@{}>{\hspre}l<{\hspost}@{}}%
\column{60}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{converge}\mathbin{::}\Conid{Natural}\to {}\<[25]%
\>[25]{}(\Conid{Natural}\to \Conid{Natural}{}\<[47]%
\>[47]{}\to \Conid{Natural}){}\<[60]%
\>[60]{}\to {}\<[E]%
\\
\>[25]{}(\Conid{Natural}\to \Conid{Natural}){}\<[47]%
\>[47]{}\to \Conid{Natural}{}\<[60]%
\>[60]{}\to \Conid{Natural}{}\<[E]%
\\
\>[B]{}\Varid{converge}\;\Varid{l}\;\Varid{con}\;\Varid{f}\;\Varid{n}\mathrel{=}{}\<[23]%
\>[23]{}\mathbf{let}\;\Varid{r}\mathrel{=}\Varid{f}\;\Varid{n}{}\<[E]%
\\
\>[23]{}\hsindent{1}{}\<[24]%
\>[24]{}\mathbf{in}\;\mathbf{if}\;\Varid{r}\equiv \Varid{l}\;{}\<[38]%
\>[38]{}\mathbf{then}\;\Varid{r}{}\<[E]%
\\
\>[38]{}\mathbf{else}\;\Varid{r}\mathbin{`\Varid{con}`}\Varid{converge}\;\Varid{l}\;\Varid{f}\;(\Varid{n}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This version is very similar to the previous one,
but it accepts two more arguments:
The first argument, $l$, 
is the neutral element with respect
to the combination function, $con$, passed in 
as the second argument (\ensuremath{\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}}),
\ie\ addition or multiplication.
The implementation of the function
differs in only two aspects:
We compare the result not explicitly with 0,
but with $l$, the limit passed in,
and, instead of \ensuremath{(\mathbin{+})}, we use \ensuremath{\mathbin{`\Varid{con}`}}
to combine results.

From here, we can very simply define two
derived functions \ensuremath{\Varid{convSum}} and \ensuremath{\Varid{convProduct}}:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{convSum}\mathbin{::}(\Conid{Natural}\to \Conid{Natural})\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[B]{}\Varid{convSum}\mathrel{=}\Varid{converge}\;\mathrm{0}\;(\mathbin{+}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{convProduct}\mathbin{::}(\Conid{Natural}\to \Conid{Natural})\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[B]{}\Varid{convProduct}\mathrel{=}\Varid{converge}\;\mathrm{1}\;(\mathbin{*}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Let us look at how to use the convergence function:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{10}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{ragmen}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[B]{}\Varid{ragmen}\;\Varid{n}\;\Varid{m}\;\Varid{c}\mathrel{=}\mathrm{1}\mathbin{+}\Varid{convSum}\;(\Varid{f}\;\Varid{n}\;\Varid{m}\;\Varid{c})\;\mathrm{1}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}\;{}\<[10]%
\>[10]{}\Varid{f}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[10]{}\Varid{f}\;\Varid{n}\;\Varid{m}\;\Varid{c}\;\Varid{k}\mathrel{=}(\Varid{n}\mathbin{-}\Varid{m})\mathbin{`\Varid{floorDiv}`}(\Varid{c}\mathbin{\uparrow}\Varid{k}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The \ensuremath{\Varid{ragmen}} function simply adds one to the result 
of a call to the \ensuremath{\Varid{convSum}} function defined above.
The function $f$ passed to \ensuremath{\Varid{convSum}}
and defined in the \ensuremath{\mathbf{where}} clause
can be easily recocnised as the ragman function
defined in the text above.
We pass $f$ with $n$, $m$ and $c$,
thas is the number of customers, the number of endpoints
and the constant $c$ to \ensuremath{\Varid{convSum}}.
We additionally pass 1 as the first value of $k$.

\section{Induction}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}
The series we looked at in the previous section
converge very soon,
for realistic values,
after 3 or 4 steps.
But this may be different and
then huge sums would arise that are costly to compute,
since many, perhaps unfeasibly many additions
had to be made. We already stumbled on such problems,
when we looked at multiplication.
It is therefore often desirable to find a \term{closed form}
that leads to the same result without the necessity
to go through all the single steps. % mention complexity classes?
Let us look at a very simple example.
We could be interested in the value 
of the $n$ first odd numbers summed up,
\ie\ for $n = 2$: $1 + 3 = 4$,
         $n = 3$: $1 + 3 + 5 = 9$,
         $n = 4$: $1 + 3 + 5 + 7 = 16$
and so on.
With large values of $n$, 
we would have to go through many steps, 
\viz\ $n - 1$ additions.

First, let us think about how to express this as a formula.
An odd number is a number that is not divisible by 2.
Even numbers could be expressed as $2k$
for all $ks$ from $1 \dots n$, for instance
the first even number, $n = 1$, is 2,
the first two even numbers, $n = 2$, are 2 and 4,
since $2 \times 2 = 4$,
the first three even numbers, $n = 3$, are 2, 4 and 6,
since $2 \times 3 = 6$ 
and so on.
Odd numbers, correspondingly, can be described as:
$2k - 1$.
The first odd number, hence, is $2 \times 1 - 1 = 1$,
the first two odd numbers, $n=2$, are 1 and 3,
since $2 \times 2 - 1 = 3$,
the first three odd numbers, $n=3$, are 1, 3 and 5,
since $2 \times 3 - 1 = 5$ and so on.
Correspondingly, the sum of the first $n$ odd numbers
can be properly described as:

\[ 
\sum_{k=1}^{n}{(2k - 1)}
\] 

To convince ourselves that this formula is correct,
let us go through some examples:
If $n=1$, then $2k - 1$ equals $1$,
for $n=2$, this is the result of $n = 1$
plus $4 - 1$, hence $1 + 3 = 4$,
for $n=3$, the formula leads to $4 + 6 - 1$ = $9$
and for $n=4$, the result is $9 + 8 - 1$ = $16$.
The formula appears to be correct.

We can implement this formula literally 
by a simple Haskell program:
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}c<{\hspost}@{}}%
\column{31E}{@{}l@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{oddSum1}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{oddSum1}\;\Varid{n}\mathrel{=}\Varid{go}\;\mathrm{1}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{k}{}\<[17]%
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{k}\mathbin{>}\Varid{n}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}\mathrm{0}{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}(\mathrm{2}\mathbin{*}\Varid{k}\mathbin{-}\mathrm{1})\mathbin{+}\Varid{go}\;(\Varid{k}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Now, is there a closed form that spares us
from going through all the additions in the $go$ function?
When we look at the results of the first $9$ numbers
calling oddSum1 as  

\begin{tabbing}\tt
~~~map~oddSum1~\char91{}1\char46{}\char46{}9\char93{}
\end{tabbing}

we see that all the numbers are perfect squares:
$1, 4, 9, 16, 25, 36, 49, 65, 81$.
Indeed, the results suggest that
the sum of the first $n$ odd numbers equals $n^2$.
But is this always true
or does it hold only for the first
nine numbers we just happened to look at?
Let us try a proof by \term{induction}.

Induction is an tremendously important technique,
since it enables us to prove that a property holds
for infinitely many numbers!
A proof by induction proves that a property $P$
holds for a base case and,
by advancing from the base case to the following number,
that it holds for all numbers we are interested in.
Formally, we prove, for example, that
$P(n) \rightarrow P(n+1)$,
where $+1$ is a very common way to advance.
With $+$, we actually prove that $P$ holds for all $x > n$.
But we can use induction also for 
with functions that advance at a different pace,
for instance, we might want to prove
that some property holds for even numbers,
we would then advance with $+2$.

Proofs by induction consist
of two parts:
First, the proof that the property 
is true for the base case
and, second, that it is still true when advancing
from a number, for which we know that it is true,
like the base case, to the next number.

For the example of the sum of the odd numbers,
the base case, $n = 1$, is trivially true,
since $1^2$ and $\sum_{k=1}^{n}{(2k-1)}$
are both $1$.
Now, if we assume that, for a number $n$, it is true
that the sum of the first $n$ odd numbers is $n^2$,
we have to show that this is also true
for the next number $n + 1$ or, more formally,
that 

\begin{equation}
\sum_{k=1}^{n+1}{(2k - 1)} = (n + 1)^2.
\end{equation}

We can decompose the sum on the left side
of the equal sign by taking the induction step
($n+1$) out and get the following equation:

\begin{equation}
\sum_{k=1}^{n}{(2k - 1)} + 2(n + 1) - 1 = (n+1)^2.
\end{equation}

Note that the part broken out of the sum
corresponds exactly to the formula within the sum
for the case that $k = n + 1$.
Since we already now that the first part is $n^2$,
we can simplify the expression 
on the left side of the equal sign
to $n^2 + 2(n + 1) - 1$,
which, again simplified, gives:

\begin{equation}
 n^2 + 2n + 1 = (n+1)^2\qed
\end{equation}

and, thus, concludes the proof.
If you do not see 
that both sides are equal,
multiply the right side out as
$(n + 1) (n + 1)$,
where $n \times n = \mathbf{n^2}$,
      $n \times 1 = \mathbf{n}$,
      $1 \times n = \mathbf{n}$ and
      $1 \times 1 = \mathbf{1}$.
Summing this up gives $n^2 + 2n + 1$.

The $oddSum$ function can thus be implemented
in much more efficient way:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{oddSum}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{oddSum}\mathrel{=}(\mathbin{\uparrow}\mathrm{2}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

For another example, let us look at
even numbers. Formally, the sum of the 
first $n$ even numbers corresponds to:
$\sum_{k=1}^{n}{2k}$. This is easily implemented
in Haskell as

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}c<{\hspost}@{}}%
\column{31E}{@{}l@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{evenSum1}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{evenSum1}\;\Varid{n}\mathrel{=}\Varid{go}\;\mathrm{1}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{k}{}\<[17]%
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{k}\mathbin{>}\Varid{n}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}\mathrm{0}{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}\mathrm{2}\mathbin{*}\Varid{k}\mathbin{+}\Varid{go}\;(\Varid{k}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Applying $evenSum1$ to the test set $[1..9]$
gives the sequence:
$2, 6, 12, 20, 30, 42, 56, 72, 90$.
These are obviously no perfect squares and,
compared to the odd numbers 
($1, 4, 9, 16, \dots$),
the results are slightly greater.
How much greater are they?
For $n = 1$, $oddSum$ is 1, $evenSum$ is 2,
$evenSum$ is hence $oddSum + 1$ for this case;
for $n = 2$, the difference between the results $4$ and $6$
is $2$;
for $n = 3$, the difference between $9$ and $12$ is $3$.
This suggests a pattern: 
the difference between $oddSum$ and $evenSum$ is exactly $n$.
This would suggest the closed form $n^2 + n$ 
or, which is the same, $n(n+1)$.
Can we prove this by induction?

For the base case $n = 1$, 
$\sum_{k=1}^{1}{2k}$ and $n(n + 1)$
are both $2$.
Now assume that for some $n$,
$\sum_{k=1}^{n}2k = n(n + 1)$
holds, as we have just seen for the base case $n = 1$,
then we have to show that 

\begin{equation}
\sum_{k=1}^{n+1}2k = (n + 1)(n + 2).
\end{equation}

Again, we decompose the sum on the left side of the equal sign:

\begin{equation}
\sum_{k=1}^{n}{(2k)} + 2(n + 1) = (n+1)(n+2).
\end{equation}

According to our assumption, the summation now equals $n(n+1)$:

\begin{equation}
n(n+1) + 2(n + 1) = (n+1)(n+2).
\end{equation}

The left side of the equation can be further simplified 
in two steps, first, to 
$n^2 + n + 2n + 2$ and, second, to
$n^2 + 3n + 2$,
which concludes the proof:

\begin{equation}
n^2 + 3n + 2 = (n+1)(n+2)\qed
\end{equation}

If you do not see the equality,
just multiply $(n+1)(n+2)$ out: 
$n \times n = \mathbf{n^2}$, 
$n \times 2 = \mathbf{2n}$;
$1 \times n = \mathbf{n}$, 
$1 \times 2 = \mathbf{2}$;
adding all this up gives $n^2 + 2n + n + 2 = n^2 + 3n + 2$.

We can now define an efficient version of $evenSum$:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{evenSum}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{evenSum}\;\Varid{n}\mathrel{=}\Varid{n}\mathbin{\uparrow}\mathrm{2}\mathbin{+}\Varid{n}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Now, of course, the question arises
to what number the first $n$ of both kinds of numbers, 
even and odd, sum up.
One might think that this must be something
like the sum of odd and even for $n$,
but that is not true.
Note that the sum of the first $n$ 
either odd or even numbers 
is in fact number greater than the first $n$ numbers,
since, when we leave out every second number,
then the result of counting $n$ numbers is much higher
than counting all numbers, \eg\
for $n = 3$, the odd numbers are $1, 3, 5$ and
             the even are $2, 4, 6$.
The first 3 numbers, however, are $1, 2, 3$. 

The answer jumps into the eye
when we look at the formula for the sum of even numbers:
$\sum_{k=1}^{n}2k$. This formula implies 
that, for each $n$, we take twice $n$.
The sum of all numbers, in consequence, 
should be the half of the sum of the even, \ie\ 
$\sum_{k=1}^{n}{(k)} = \frac{n(n+1)}{2}$,
a formula that is sometimes humorously called
\term{The Little Gauss}.

Once again, we prove by induction.
The base case, $n=1$, is trivially true:
$\sum_{k=1}^{1}{k} = 1$ and
$\frac{1 * (1 + 1)}{2} = \frac{2}{2} = 1$.
Now assume that 
$\sum_{k=1}^{n}{k} = \frac{n(n+1)}{2}$
holds for $n$;
then, we have to prove that

\begin{equation}
\sum_{k=1}^{n+1}{k} = \frac{(n+1)(n+2)}{2}.
\end{equation}

As in our previous exercises, 
we take the induction step out of the summation formula
and get $\sum_{k=1}^{n}{(k)} + (n + 1)$. 
According to our assumption, we can reformulate this as
$\frac{n(n+1)}{2} + (n + 1)$.
We have not yet discussed how to add fractions;
to do this, we have to present both values
as fractions with the same denominator,
which is $2$. 
To maintain the value of $n + 1$, 
when we divide it by $2$,
we have to multiply it with $2$ at the same time,
yielding the fraction $\frac{2(n+1)}{2} = \frac{2n + 2}{2}$:

\begin{equation}
\frac{n(n+1)}{2} + \frac{2n + 2}{2} = \frac{(n+1)(n+2)}{2}
\end{equation}

After multiplying the numerator of the first fraction
on the left side of the equation out 
($n^2 + n$)
and then adding the two numerators we
obtain, in the numerators, the formula
we already know from the even numbers: 

\begin{equation}
\frac{n^2 + 3n + 2}{2} = \frac{(n+1)(n+2)}{2}\qed
\end{equation}

The sum of the first $n$ natural numbers in Haskell, hence is:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{natSum}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{natSum}\mathrel{=}(\mathbin{\Varid{`div`}}\mathrm{2})\mathbin{\circ}\Varid{evenSum}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

\section{Arithmetic and Geometric Series}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Arigeo}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

\ignore{
Arithmetic
==========

1 .. n
Sn = 1 + (1 + 1) + (1 + 2) + (1 + 3) + ... + (1 + n-1)
Sn = (n - (n-1)) + (n - (n-2)) + ... + (n -1) + 1

Sn+Sn = n(1+n)
2Sn = n(1+n)
Sn = s(1 + n)/2

a .. a + n
Sn = a1 + (a1 + 1) + (a1 + 2) + (a1 + 3) + ... + (an + n-1)
Sn = (an - (n-1)) + (an - (n-2)) + ... + (an - 1) + an

Sn + Sn = n(a1 + an)
2Sn = n(a1 + an)
Sn = n(a1 + an)/2

a .. a + dn
Sn = a1 + (a1 + d) + (a1 + 2d) + (a1 + 3d) + ... + (an + (n-1)d)
Sn = (an - (n-d)) + (an - (n-2d)) + ... + (an - d) + an

Sn + Sn = n(a1+an)

Arithmetic progression: n(a1+an)/2

Geometric
=========
\sum_{k=0}^{n-1}{ar^k} // <-- connection to GF: \sum_{n=0}^{\infy}{a_nx^n}

Sn = a + ar + ar^2 + ar^3 + ... + ar^(n-1)
rSn = ar + ar^2 + ar^3 + ... + ar^n
s-rs = s(1-r) = a - ar^n
s = (a - ar^n) / (1-r)
for r < 1 and n growing:
a/(1-r)

for a = 1
1/(1-r)

}

\section{The Fibonacci Sequence}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\mathbf{import}\;\Conid{Prelude}\;\Varid{hiding}\;(\Varid{gcd}){}\<[E]%
\\
\>[3]{}\mathbf{import}\;\Conid{Natural}\;\Varid{hiding}\;(\Varid{gcd}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

We have already discussed 
and analysed the run time behaviour of $\gcd$.
Let us look at an intriguing example,
the $\gcd$ of, say, $89$ and $55$.
As a reminder here the definition 
of $\gcd$ once again:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}c<{\hspost}@{}}%
\column{12E}{@{}l@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{gcd}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{gcd}\;\Varid{a}\;\mathrm{0}{}\<[12]%
\>[12]{}\mathrel{=}{}\<[12E]%
\>[15]{}\Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{gcd}\;\Varid{a}\;\Varid{b}{}\<[12]%
\>[12]{}\mathrel{=}{}\<[12E]%
\>[15]{}\Varid{gcd}\;{}\<[20]%
\>[20]{}\Varid{b}\;(\Varid{a}\mathbin{\Varid{`rem`}}\Varid{b}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

We start with \ensuremath{\Varid{gcd}\;\mathrm{89}\;\mathrm{55}}, which is
\ensuremath{\Varid{gcd}\;\mathrm{55}\;(\mathrm{89}\mathbin{\Varid{`rem`}}\mathrm{55})} after one step.
What is the remainder of 89 and 55?
89 divided by 55 is 1 leaving the remainder 
$89 - 55 = 34$.
The next round, hence, is \ensuremath{\Varid{gcd}\;\mathrm{55}\;\mathrm{34}}.
The remainder of 55 and 34 is $55 - 34 = 21$.
We recurse once again, this time with \ensuremath{\Varid{gcd}\;\mathrm{34}\;\mathrm{21}}.
The remainder of 34 and 21 is $34 - 21 = 13$.
The next step, hence, is \ensuremath{\Varid{gcd}\;\mathrm{21}\;\mathrm{13}},
which leads to the remainder $21 - 13 = 8$.
As you see, this gets quite boring, 
but we are not done yet,
since the next round \ensuremath{\Varid{gcd}\;\mathrm{13}\;\mathrm{8}}
forces us to call the function again with 8 and $13 - 8 = 5$,
which then leads to \ensuremath{\Varid{gcd}\;\mathrm{5}\;\mathrm{3}}, subsequently to
\ensuremath{\Varid{gcd}\;\mathrm{3}\;\mathrm{2}} and then to \ensuremath{\Varid{gcd}\;\mathrm{2}\;\mathrm{1}}.
The division of 2 by 1 is 2 leaving no remainder
and, finally, we call \ensuremath{\Varid{gcd}\;\mathrm{1}\;\mathrm{0}},
which reduces immediately to 1.

Apparently, we got in some kind of trap.
The first pair of numbers, 89 and 55,
leads to a sequence of numbers, where 
every number is the sum of its two predecessors:
$1 + 2 = 3, 2 + 3 = 5, 3 + 5 = 8, 5 + 8 = 13,
 8 + 13 = 21, 13 + 21 = 34, 21 + 34 = 55, 34 + 55 = 89$.
We entered with 89 and 55 and 
computed the remainder.
Since the difference of 89 and 55
is less than 55,
the remainder between these two number
is just the difference $89-55$.
That way,
we got to the next pair, 55 and 34,
for which the same is true,
\viz\ that the remainder is just
the difference between the two and so 
we continued step for step 
until we finally reached $(2,1)$.

This sequence is well known.
It was used by the Italian mathematician
Leonardo Pisano, better known as Fibonacci
(\term{Filius}, that is, son of Bonaccio),
as an arithmetic exercise in his \term{Abacus}
(``calculating") book, which was published in 1202.
The sequence is the solution to an exercise
with the following wording:
``How many pairs of rabbits can be produced
  from a single pair in a year's time
  if every fertile pair produces a new pair
  of offsping per month and every pair
  becomes fertile in the age of one month?"
% check for a canonical solution
We start with 1 pair, which produces one offspring
after one month, yielding 2 pairs; in the second month,
the first pair produces one more offsping,
hence we have 3 pairs. In the third month,
we have 2 fertile pairs producing each 1 more pair
and we, hence, have 5 pairs. 
This, quickly, becomes confusing.
Here a table that gives an overview of what happens
during the first year:

\begin{center}
\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r|r|r}
month     & 1 & 2 & 3 & 4 &  5 &  6 &   7 &   8 &  9 &  10 &  11 &   12 \\\hline
new pairs & 1 & 1 & 2 & 3 &  5 &  8 &  13 &  21 & 34 &  55 &  89 &  144 \\\hline
total     & 1 & 2 & 4 & 7 & 12 & 20 &  33 &  54 & 88 & 143 & 232 &  376 
\end{tabular}
\end{center}

This means that,
in month 1, there is 1 new pair;
in month 2, there is another new pair;
in month 3, there are 2 new pairs;
in month 4, there are 3 new pairs;
in month 5, there are 5 new pairs;
$\dots$;
in month 12, there are 144 new pairs.
This is the Fibonacci sequence,
whose first 12 values are given in the second row. 
The answer to Fibonacci's question
consists in summing the sequence up:
$\sum_{k=2}^{12}{F_k} = 375$.
This can be seen in the third row of the table,
which shows the total number of rabbit pairs for each month.
Since this sum includes the first pair,
which was already there,
we must subtract one from the values in this row
to come to the correct result.

The Fibonacci function can be defined as:

\[
F_n = \begin{cases}
        0 & \textrm{if n = 0}\\
        1 & \textrm{if n = 1}\\
        F_{n-1} + F_{n-2} & \textrm{otherwise}
      \end{cases}
\]

The Fibonacci sequence
is explicitly defined for 0 and 1
(since, of course, 0 and 1 do not have
two predecessor from which they could be derived)
and for all other numbers recursively as
the sum of the Fibonacci numbers
of its two predecessors.
In Haskell this looks like:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{fib}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{fib}\;\mathrm{0}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{fib}\;\mathrm{1}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{fib}\;\Varid{n}\mathrel{=}\Varid{fib}\;(\Varid{n}\mathbin{-}\mathrm{1})\mathbin{+}\Varid{fib}\;(\Varid{n}\mathbin{-}\mathrm{2}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Many people have studied the Fibonacci sequence,
following Fibonacci,
but also independently 
and even before it was mentioned in the \term{Abacus Book}.
The sequence has the astonishing habit
of popping up in very different contexts
in the study of mathematics, nature, arts and music.
Many mathematical objects, however, have
this surprising -- or, for some at least, annoying --
property.

The first known practical application of the Fibonacci sequence
in Europe appeared in an article of the French mathematician
Gabriel Lamé in 1844 that identified the worst case
of the Euclidian $\gcd$ algorithm as any two subsequent numbers
of the Fibonacci sequence.
This remarkable paper is also considered as the earliest
study in computational complexity theory,
a discipline that would be established only 120 years later.\footnote{
There are of course always predecessors.
The relation between $\gcd$ and the Fibonacci sequence
was, according to Knuth, already discussed in a paper 
by a French mathematician called Léger in 1837,
and an analysis of the run time behaviour 
of the Euclidan algorithm
was already presented in a paper by another French mathematician
called Reynaud in 1811.}
Lamé gave the worst case of the algorithm as $n$
for $\gcd(F_{n+2}, F_{n+1})$.
Let us check if this is true for our example above.
We started with 89 and 55, which correspond
to $F_{11}$ and $F_{10}$.
According to Lamé, we would then need 9 steps
to terminate the algorithm.
The pairs of numbers we applied are:
$(89,55),(55,34),(34,21),(21,13),(13,8),$
$(8,5),(5,3),(3,2),(2,1),(1,0)$,
which are 10 pairs of numbers and indeed 9 steps.
But why is this so and is it always true
or just for the sequence, we happen to look at?

We can answer the first question by observing
the recursive nature of the Euclidian algorithm.
When there is a pair of subsequent Fibonacci numbers
that needs $n$ steps, then the pair of the next
subsequent Fibonacci numbers 
will reduce to the first pair after one round of $\gcd$
and, thus, needs $n + 1$ steps:
all the steps of the first pair plus the one for itself. 
This is just the structure of mathematical induction,
which leads us to the proof.
We choose $(2,1)$ as the base case,
which is $(F_3,F_2)$ and,
as we have seen,
needs one step to result 
in the trivial case $(1,0)$.
If the proposition that $\gcd(F_{n+2}, F_{n+1})$ 
needs $n$ steps is true, then,
for the case $n = 1$,
$F_{n+2}$ is $F_3$, which is 2,
and $F_{n+1}$ is $F_2$, which is 1.
Therefore, the base case $(2,1)$ fulfils the rule.

Now assume that we have a case for which it is true
that the number of steps of $\gcd ( F_{n+2}, F_{n+1})$ is $n$.
Then we have to show that 
the number of steps of 
$\gcd ( F_{n+3}, \\ F_{n+2})$ is $n + 1$.
According to its definition, 
$\gcd$ for a pair of numbers $(a,b)$ is $(b, a \bmod  b)$.
For subsequent Fibonacci numbers 
(as we have already shown informally above),
$a \bmod b$ is identical to $a - b$ 
(except for the case where $b = 1$).
After one step 
with $a=F_{n+3}$ and $b=F_{n+2}$, 
we therefore have:

\[
\gcd(F_{n+2}, F_{n+3} - F_{n+2}).
\]

We can substitute $F_{n+3}$ in this formula
according to the definition of the Fibonacci sequence,
$F_{n} = F_{n-2} + F_{n-1}$,
by $F_{n+1} + F_{n+2}$:

\[
\gcd(F_{n+2}, F_{n+1} + F_{n+2} - F_{n+2}),
\]

which, of course, simplifies to

\[
\gcd(F_{n+2}, F_{n+1}).
\]

This shows that we can reduce
$\gcd(F_{n+3}, F_{n+2})$ 
to 
$\gcd(F_{n+2}, F_{n+1})$
in one step
and that concludes the proof.

There is much more to say about 
this delightful sequence,
and we are even far away from the conclusions
of Lamé's paper.
Unfortunately, we have not yet
acquired the tools to talk
about these upcoming issues
in a meaningful way.
But, very soon, we will have.
In the meanwhile,
you might try to discover the Fibonacci sequence
in other objects we will meet on our way,
for example, in a certain very strange triangle.

\ignore{
proof: fib (gcd a b) = gcd (fib a) (fib b)
example: 
gcd 10 5 = 5
fib 5 = 5

gcd (fib 10 = 55) (fib 5 = 5) = 5

see http://www.cut-the-knot.org/arithmetic/algebra/Fibonacci\gcd.shtml
}

\ignore{
The German mathematician, astronomer and astrologer
Johannes Kepler
has realised that the quotient of two consecutive
Fibonacci numbers aproximates the \emph{golden ratio}
for sufficiently big numbers.
The golden ratio is very prominent
in aesthetic theory and practice
at least since the Renaissance.
Renaissance and humanistic scholars
date the use of the golden ratio
back to the antiquity
and, indeed, Euclide gives a description
of this ratio in the \term{Elements}.
Whether antique architecture and art
actually used the golden ratio is
contested among modern scholars.
However, the golden ratio describes a relation 
of two values $a$ and $b$ such that

\begin{equation}
\frac{a}{b} = \frac{a + b}{a}.
\end{equation}

In other words the greater of the two values,
$a$, has the same ratio to the smaller value, $b$,
as the sum of the two to $a$. 
The relation corresponds to

\[
\frac{1 + \sqrt{5}}{2} \approx 1.618.
\]

The golden ratio, indeed, looks very similar to the quotient
formed from two consecutive Fibonacci numbers:

\begin{equation}
\frac{F_{n}}{F_{n-1}} = \frac{F_{n-1} + F_{n-2}}{F_{n-1}}.
\end{equation}

From Kepler's observation, 
a closed form for the Fibonacci sequence can be devised,
which is desirable because
Fibonacci numbers are actually helpful in the analysis
of algorithms (as we have just seen for $\gcd$)
and the recursive computation of Fibonacci numbers
is quite expensive, as you may have experienced
if you have tried to to generate Fibonacci numbers
beyond $20$ with the \haskell{fib} implementation
given above.
I would very much like to present this closed form here,
but since its derivation requires some more math
than we have learnt so far,
I have to put you off for a future chapter.
This, of course,
is a somewhat anticlimactic finish to the current section.
On the other hand, with this postponement,
the very delightful Fibonacci sequence 
remains vivid in this textbook 
and we have something to look forward to. 
}

\ignore{
  - Kepler + golden ratio
  - closed form
  - yes, there is and it is based on Keplers observation.
    but there is more math in it than we can represent
    right now. we will come back to it later
}

\section{Factorial}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Fact}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{\Conid{Data}.List}\;(\Varid{concatMap}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

A fundamental concept in mathematics,
computer science and also real life
is the idea of \term{permutation},
variations in the order
of a sequence of objects.
Shuffling a card deck
would for instance create permutations
of the original arrangement of the cards.
The possible outcomes of a sports event,
the order in which the sprinters in a race arrive
or the final classification of a league 
where all teams play all others,
is another example.

For the list \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} (in Haskell notation),
the following permutations are possible:
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} (this is the identity),
\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{3}\mskip1.5mu]},
\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{3},\mathrm{1}\mskip1.5mu]},
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{2}\mskip1.5mu]},
\ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{1},\mathrm{2}\mskip1.5mu]} and
\ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{2},\mathrm{1}\mskip1.5mu]}.

Let us look at how to construct 
all permutations of a given sequence.
The simplest case is the empty list
that allows only one arrangement:
\ensuremath{\Varid{permutations}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]}. 
From this base case on, 
we can easily create permutations of longer lists,
simply inserting new elements at every possible
position within the permutations.
The permutations of a list with one elmenent, for instance,
would be constructed by inserting this element, say $x$,
in all possible positions of all possible permutations
of the empty list,
trivially yielding: \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{x}\mskip1.5mu]\mskip1.5mu]}.
Now, when we add one more element, we get:
\ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{y},\Varid{x}\mskip1.5mu],[\mskip1.5mu \Varid{x},\Varid{y}\mskip1.5mu]\mskip1.5mu]}, 
first adding the new element $y$ 
in front of the existing element $x$
and, second, adding it behind $x$.
We now easily create the permutations
of a list with three elements
by simply inserting the new element $z$
in all possible positions of these two sequences,
which, for the first, gives:
\ensuremath{[\mskip1.5mu \Varid{z},\Varid{y},\Varid{x}\mskip1.5mu],[\mskip1.5mu \Varid{y},\Varid{z},\Varid{x}\mskip1.5mu],[\mskip1.5mu \Varid{y},\Varid{x},\Varid{z}\mskip1.5mu]}
and for the second:
\ensuremath{[\mskip1.5mu \Varid{z},\Varid{x},\Varid{y}\mskip1.5mu],[\mskip1.5mu \Varid{x},\Varid{z},\Varid{y}\mskip1.5mu],[\mskip1.5mu \Varid{x},\Varid{y},\Varid{z}\mskip1.5mu]}.
Compare this pattern to the permutations
of the list \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} above
with $z=1,y=2$ and $x=3$.

Let us implement the process
of inserting a new element at any possible position
of a list in Haskell
using the $cons$ operator \ensuremath{(\mathbin{:})}:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}c<{\hspost}@{}}%
\column{21E}{@{}l@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{insall}\mathbin{::}\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{insall}\;\Varid{p}\;{}\<[13]%
\>[13]{}[\mskip1.5mu \mskip1.5mu]{}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}[\mskip1.5mu [\mskip1.5mu \Varid{p}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{insall}\;\Varid{p}\;{}\<[13]%
\>[13]{}(\Varid{x}\mathbin{:}\Varid{xs}){}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}(\Varid{p}\mathbin{:}\Varid{x}\mathbin{:}\Varid{xs})\mathbin{:}(\Varid{map}\;(\Varid{x}\mathbin{:})\mathbin{\$}\Varid{insall}\;\Varid{p}\;\Varid{xs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

As base case, we have $p$, the new element,
added to the empty list,
which trivially results in \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{p}\mskip1.5mu]\mskip1.5mu]}.
From here on, for any list of the form \ensuremath{\Varid{x}\mathbin{:}\Varid{xs}},
we add $p$ in front of the list (\ensuremath{\Varid{p}\mathbin{:}\Varid{x}\mathbin{:}\Varid{xs}})
and then repeat the process for all possible
reductions of the list until we reach the base case.
In each recursion step, we add $x$,
the head of the original list, 
in front of the resulting lists.
Imagine this for the case $p=1$, $x=2$ and $xs=\{3\}$:
We first create \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} by means of \ensuremath{\Varid{p}\mathbin{:}\Varid{x}\mathbin{:}\Varid{xs}};
we then enter $insall$ again with $p=1$, $x=3$ and $xs=\{\}$,
which creates \ensuremath{\mathrm{1}\mathbin{:}\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu]}, to which later,
when we return, $2$, the $x$ of the previous step, is inserted, 
yielding \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{3}\mskip1.5mu]}.
With the next step, 
we hit our base case \ensuremath{\Varid{insall}\;\mathrm{1}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]}.
Returning to the step with $x={3}$,
mapping \ensuremath{(\Varid{x}\mathbin{:})} gives \ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{1}\mskip1.5mu]} and,
one step further back, \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{3},\mathrm{1}\mskip1.5mu]}.
We, hence, have created three cases: 
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}
inserting $1$ in front of the list,
in the middle of the list
and at the end.

To generate all possible permutations 
we would need to apply $insall$
to \emph{all} permutations of the input list,
that is not only to \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]}
as above, but also to \ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{2}\mskip1.5mu]}.
This is done by the following function:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{perms}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{perms}\;[\mskip1.5mu \mskip1.5mu]{}\<[13]%
\>[13]{}\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{perms}\;(\Varid{x}\mathbin{:}\Varid{xs})\mathrel{=}\Varid{concatMap}\;(\Varid{insall}\;\Varid{x})\mathbin{\$}\Varid{perms}\;\Varid{xs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Called with \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]},
the function would map \ensuremath{\Varid{insall}\;\mathrm{1}}
on the result of \ensuremath{\Varid{perms}\;\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{3}\mskip1.5mu]}.
This, in its turn, would map \ensuremath{\Varid{insall}\;\mathrm{2}}
onto \ensuremath{\Varid{perms}\;\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu]}.
Finally, we get to the base case resulting in \ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]}.
Going back the call tree, 
\ensuremath{\Varid{insall}\;\mathrm{3}} would now be called on the empty set
yielding \ensuremath{[\mskip1.5mu \mathrm{3}\mskip1.5mu]}; 
one step further back, \ensuremath{\Varid{insall}\;\mathrm{2}} would 
now result in \ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{3},\mathrm{2}\mskip1.5mu]\mskip1.5mu]}.
Mapping \ensuremath{\Varid{insall}\;\mathrm{1}} finally on these two lists
leads to the desired result.

You will have noticed that we are using the function $concatMap$.
The reason is that each call of $insall$ creates a list of lists
(a subset of the possible permutations). 
Mapping \ensuremath{\Varid{insall}\;\mathrm{1}} on the permutations of 
\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]}, for instance, creates two lists,
one for each permutation (\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]} and \ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{2}\mskip1.5mu]}):
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3},\mathrm{1}\mskip1.5mu]\mskip1.5mu]} and
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{3},\mathrm{1},\mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{3},\mathrm{2},\mathrm{1}\mskip1.5mu]\mskip1.5mu]}.
We could use the function $concat$
to merge the lists together, like:
\ensuremath{\Varid{concat}\mathbin{\char92 \$}\Varid{map}\;(\Varid{insall}\;\Varid{x})\mathbin{\char92 \$}\Varid{perms}\;\Varid{xs}};
$concatMap$ is much more convenient:
it performs mapping and merging in one step.

We have not yet noted explicitly
that, when talking about permutations,
we treat sequences as Haskell lists.
Important is that the elements in permutation lists
are distinct. In a list like \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{2}\mskip1.5mu]},
we cannot distinguish the last two elements
leading to errors in counting possible permutations.
In fact, when we say \term{sequence},
we mean an ordering of the elements of a \term{set}.
Sets, by definition, do not contain duplicates.
We will look at sets more closely in the next section.

So, how many possible permutations are there
for a list with $n$ elements?
We have seen that for the empty list
and for any list with only one element,
there is just one possible arrangement.
For a list with two elements,
there are two permuntations (\ensuremath{[\mskip1.5mu \Varid{a},\Varid{b}\mskip1.5mu],[\mskip1.5mu \Varid{b},\Varid{a}\mskip1.5mu]}).
For a list with three elments,
there are six permutations.
Indeed, for a list with three elements,
we can select three different elements
as the head of the list and we then have
two possible permutations for the tail of each of these three list.
This suggests that the number of permutations is again
a recursive sequence of numbers:
for a list with 2 elements, 
there are $2 \times 1$ possible permutations;
for a list with 3 elements,
there are $3 \times 2$ possible permutations
or, more generally,
for a list with $n$ elements,
there are $n$ times the number 
of possiblilities for a list with $n-1$ elements.
This function is called \term{factorial}
and is defined as:

\begin{equation}
n! = \prod_{k=1}^{n}{k}.
\end{equation}

We can define factorial in Haskell as follows:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{fac}\mathbin{::}(\Conid{Num}\;\Varid{a},\Conid{Eq}\;\Varid{a})\Rightarrow \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[3]{}\Varid{fac}\;\mathrm{0}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{fac}\;\Varid{n}\mathrel{=}\Varid{n}\mathbin{*}\Varid{fac}\;(\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

There is sometimes confusion about the fact
that $0!$ is $1$ and not,
as one might expect, $0$.
There are however good arguments for this choice.
The first is that the empty list
is something that we can present as an input
to a function creating permutations.
If the output were nothing,
then the empty list would have vanished
by some mysterious trick.
The output should therefore be the empty list again
and, thus, there is exactly one possible permutation
for the empty list.

Another argument is that,
if $0!$ were $0$,
we could not include $0$ into the recursive
definition of factorial.
Otherwise, the result of any factorial would be zero!
The inversion of factorial, \ie\

\begin{equation}
  n! = \frac{(n+1)!}{n+1},
\end{equation}

would not work either.
$4!$ is for instance 
$\frac{5! = 120}{5} = 24$,
$3!$ is $\frac{4!=24}{4} = 6$,
$2! = \frac{3!=6}{3} = 2$,
$1! = \frac{2!=2}{2} = 1$
and, finally, $0! = \frac{1!=1}{1} = 1$.

The first factorials,
which you can create by \ensuremath{\Varid{map}\;\Varid{fac}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mathrm{7}\mskip1.5mu]}, are:
$1, 1, 2, 6, 24, 120,\\ 720, 5040$.
They, then, increase very quickly,
$10!$, for instance, is \num{3628800}.
Knuth mentions that this value
is a rule of thumb for the limit
of what is reasonably computable.
Algorithms that need more than $10!$ steps,
quickly get out of hand,
consuming too much space or time.
Techniques to increase the available computational power
may push this limit a bit ahead,
but factorial grows even faster than Moore's law,
drawing a definite line for computability.

Unfortunately, no closed form of the factorial function
is known. There are approximations,
at which we will look later in this book,
but to obtain the precise value,
a recursive computation is necessary,
making factorial an expensive operation.

But let us have another look at permutations,
which are very interesting beast.
In the literature, different notations are used
to describe permutations. 
A very simple, but quite verbose one is
the \term{two-line} notation used by the 
great French mathematician Augustin-Louis Cauchy (1789 -- 1857).
In this notation, the original sequence is given in one line
and the resulting sequence in a second line, hence, 
for a permutation $\sigma$:

\[
\sigma = \begin{pmatrix}
         1 & 2 & 3 & 4 & 5 \\
         2 & 5 & 4 & 3 & 1 
         \end{pmatrix}.
\]

According to this definition,
the permutation $\sigma$ would
substitute 2 for 1, 5 for 2, 4 for 3 and 3 for 4 
and, finally, 1 for 5.
The alternative \term{tuple notation} 
would just give the second line as (2,5,4,3,1)
and assume a \term{natural} ordering for the original sequence.
This notation is useful, when several permutations
on the same sequence are discussed.
The original sequence would be introduced once,
and afterwards only the variations are given.

More elegant, however, is the \term{cycle notation},
which describes the effect of subsequent
applications of $\sigma$. 
In the example above, you see, for instance,
that one application of $\sigma$ on 1 would yield 2,
\ie\ $2$ takes the place of $1$.
Another application of $\sigma$,
\ie\ the application on $2$ in the second line,
would result in $5$ (since $\sigma(2) = 5$).
The next application, this time on $5$,
would put $1$ back into place (since $\sigma(5) = 1$).
These subsequent applications describe an \term{orbit}
of the permutation $\sigma$.
Each orbit is presented
as a sequence of numbers in parentheses of the form
$(x~\sigma(x)~\sigma(\sigma(x))~\sigma(\sigma(\sigma(x)))~\dots)$,
leaving out the final step 
where the cycle returns to the original configuration.
An element that is fixed under this permutation,
\ie\ that remains at its place,
may be presented as an orbit with a single element
or left out completely.
The permutation $\sigma$ above in cycle notation is
$(1~2~5)(3~4)$.
The first orbit describes the following relations:
$\sigma(1) = 2$, $\sigma(2) = 5$
and $\sigma(5) = 1$, restoring 1 in its original place.
The second orbit describes the simpler relation
$\sigma(3) = 4$ and $\sigma(4) = 3$.
This describes the permutation $\sigma$ 
completely.

Can we devise a Haskell function
that performs a permutation given in cycle notation?
We first need a function that 
creates a result list by replacing
elements in the original list.
Since orbits define substitutions
according to the original list,
we need to refer to this list,
whenever we make a substitution in the result list.
Using the result list as a reference,
we would, as in the case of 2,
substitute a substitution, 
\eg\ 2 for 5 at the first place
instead of the second place.
Here is the $replace$ function:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{44}{@{}>{\hspre}c<{\hspost}@{}}%
\column{44E}{@{}l@{}}%
\column{47}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{replace}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Varid{a}\to \Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{replace}\;\anonymous \;\anonymous \;[\mskip1.5mu \mskip1.5mu]\;\anonymous \mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{replace}\;\Varid{p}\;\Varid{s}\;(\Varid{y}\mathbin{:}\Varid{ys})\;(\Varid{z}\mathbin{:}\Varid{zs}){}\<[30]%
\>[30]{}\mid {}\<[30E]%
\>[33]{}\Varid{y}\equiv \Varid{p}{}\<[44]%
\>[44]{}\mathrel{=}{}\<[44E]%
\>[47]{}\Varid{s}\mathbin{:}\Varid{zs}{}\<[E]%
\\
\>[30]{}\mid {}\<[30E]%
\>[33]{}\Varid{otherwise}{}\<[44]%
\>[44]{}\mathrel{=}{}\<[44E]%
\>[47]{}\Varid{z}\mathbin{:}\Varid{replace}\;\Varid{p}\;\Varid{s}\;\Varid{ys}\;\Varid{zs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

In this function, $p$ is the element from the original list
that will be substituted, 
the substitute is $s$.
We pass through the original list and the result list
in parallel assuming 
that the result list is initially equal to the original list.
When $p$ is found in the original, 
$s$ is placed at its postition 
and the function terminates.
(Since Haskell lists, in this case, represent sequences
that do not contain duplicates, we just terminate
after the first substitution.)
Otherwise, the value already there at this position
in the resulting list is preserved
and the search continues.

We will use $replace$ in the definition
of a function creating permutations
according to a definition in cycle notation.
Cycle notation is translated to Haskell as a list of lists,
each inner list representing one orbit:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\mathbf{type}\;\Conid{Perm}\;\Varid{a}\mathrel{=}[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The $permute$ function takes such a $Perm$
and a list on which to perform the permutation.
An orbit consisting of the empty list 
or of only one element
is the identity and, hence, ignored.
Otherwise, one orbit after the other is processed:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}c<{\hspost}@{}}%
\column{26E}{@{}l@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}c<{\hspost}@{}}%
\column{35E}{@{}l@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{permute}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Perm}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{permute}\;[\mskip1.5mu \mskip1.5mu]\;{}\<[21]%
\>[21]{}\Varid{xs}{}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{permute}\;([\mskip1.5mu \mskip1.5mu]\mathbin{:}\Varid{ps})\;{}\<[21]%
\>[21]{}\Varid{xs}{}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\Varid{permute}\;\Varid{ps}\;\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{permute}\;([\mskip1.5mu \Varid{p}\mskip1.5mu]\mathbin{:}\Varid{ps})\;{}\<[21]%
\>[21]{}\Varid{xs}{}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\Varid{permute}\;\Varid{ps}\;\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{permute}\;(\Varid{p}\mathbin{:}\Varid{ps})\;{}\<[21]%
\>[21]{}\Varid{xs}{}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\Varid{permute}\;\Varid{ps}\mathbin{\$}\Varid{orbit}\;(\Varid{head}\;\Varid{p})\;\Varid{xs}\;\Varid{p}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{orbit}\;\anonymous \;\Varid{rs}\;{}\<[24]%
\>[24]{}[\mskip1.5mu \mskip1.5mu]{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{rs}{}\<[E]%
\\
\>[12]{}\Varid{orbit}\;\Varid{x}\;\Varid{rs}\;[\mskip1.5mu \Varid{u}\mskip1.5mu]{}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{replace}\;\Varid{u}\;\Varid{x}\;\Varid{xs}\;\Varid{rs}{}\<[E]%
\\
\>[12]{}\Varid{orbit}\;\Varid{x}\;\Varid{rs}\;(\Varid{p1}\mathbin{:}\Varid{p2}\mathbin{:}\Varid{pp}){}\<[35]%
\>[35]{}\mathrel{=}{}\<[35E]%
\>[38]{}\Varid{orbit}\;\Varid{x}\;(\Varid{replace}\;\Varid{p1}\;\Varid{p2}\;\Varid{xs}\;\Varid{rs})\;(\Varid{p2}\mathbin{:}\Varid{pp}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

For every orbit (that contains more than one element),
$permute$ is applied to the result of the function $orbit$,
which takes the first element of the current orbit,
the input list and the current orbit as a whole.
The function processes the orbit by replacing
the first element by the second,
the second by the third and so on.
The last element is replaced by the head of the orbit,
which, for this purpose, is explicitly passed to the function. 

Note that each call to $orbit$ 
and, hence, each recursion step of permute
creates a result list, which is then used
for the next recursion step.
Since orbits do not share elements,
no change in the result list made according to one orbit
will be touched when processing another orbit;
only elements not yet handled by the previous orbits
will be changed.
It is therefore safe to substitute the input list
by the list resulting from processing the previous orbits.

The cyclic notation introduces the idea
of composing permutations,
\ie\ applying a permutation on the result
of another.
The permutation above applied to itself,
for instance,
would yield \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{1},\mathrm{3},\mathrm{4},\mathrm{2}\mskip1.5mu]};
applying it once again results in \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{3},\mathrm{5}\mskip1.5mu]}.
Six subsequent applications would 
return to the original list:

\ensuremath{\mathbf{let}\;\Varid{sigma}\mathrel{=}\Varid{permute}\;[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{5}\mskip1.5mu]\;[\mskip1.5mu \mathrm{3},\mathrm{4}\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{5},\mathrm{4},\mathrm{3},\mathrm{1}\mskip1.5mu]}.\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{2},\mathrm{5},\mathrm{4},\mathrm{3},\mathrm{1}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{1},\mathrm{3},\mathrm{4},\mathrm{2}\mskip1.5mu]}.\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{5},\mathrm{1},\mathrm{3},\mathrm{4},\mathrm{2}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{3},\mathrm{5}\mskip1.5mu]}.\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{3},\mathrm{5}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{5},\mathrm{3},\mathrm{4},\mathrm{1}\mskip1.5mu]}.\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{2},\mathrm{5},\mathrm{3},\mathrm{4},\mathrm{1}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{5},\mathrm{1},\mathrm{4},\mathrm{3},\mathrm{2}\mskip1.5mu]}.\\
\ensuremath{\Varid{sigma}\;[\mskip1.5mu \mathrm{5},\mathrm{1},\mathrm{4},\mathrm{3},\mathrm{2}\mskip1.5mu]} is \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]}.

The result in the third line is funny:
It is almost identical to the original list,
but with 3 and 4 swapped.
The two orbits of the permutation $\sigma$
appear to move at different speed:
the first orbit with three elements
needs three applications 
to return to the original configuration;
the second orbit with two elements
needs only two applications.
Apparently, 2 does not divide 3;
the orbits are therefore out of sink
until the permutation was performed $2 \times 3=6$ times.

One could think 
of systems of permutations
(and people have actually done so),
such
that the application of the permutations within this system
to each other,
\ie\ the composition of permutations (denoted: $a \cdot b$),
would always yield the same set of sequences.
Trivially, all possible permutations of a list form such a system.
More interesting are subsets of all possible permutations.
Let us simplifiy the original list above to \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu]},
which has 4 elements and, hence, $4!=24$ possible permutations.
On this list, we define a set of permutations, namely

\begin{eqnarray}
e = (1)(2)(3)(4)\\
a = (1~2)(3)(4)\\
b = (1)(2)(3~4)\\
c = (1~2)(3~4)
\end{eqnarray}

The first permutation $e$ is just the identity that fixes all elements.
The second permutation, $a$, swaps 1 and 2 and fixes 3 and 4.
One application of $a$ would yield \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{3},\mathrm{4}\mskip1.5mu]}
and two applications ($a \cdot a$) would yield the original list again.
The third permutation, $b$, fixes 1 and 2 and swaps 3 and 4.
One application of $b$ would yield \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{4},\mathrm{3}\mskip1.5mu]}
and two applications ($b \cdot b$) would yield the original list again.
The fourth permutation, $c$, swaps 1 and 2 and 3 and 4.
It is the same as $a \cdot b$, thus creating \ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{1},\mathrm{4},\mathrm{3}\mskip1.5mu]}
and $c \cdot c$ would return to the original list.
We can now observe that all possible compositions of these permutations,
create permuations that are already part of the system:

\begin{eqnarray*}
a \cdot a = b \cdot b = e\\
b \cdot a \cdot b \cdot a = e\\
a \cdot b = b \cdot a = c\\
c \cdot c = e\\
c \cdot a \cdot b = e
\end{eqnarray*}

You can try every possible combination,
the result is always a permutation
that is already there.
This property of composition
of the set of permutations above bears some similarity 
with natural numbers together with
the operations addition and multiplication:
The result of an addition or multiplication
with any two natural numbers 
is again a natural number,
and the result of a composition
of any two permutations in the system
is again in the system.

Such systems of permutations, hence, are magmas
(as defined in the previous chapter) 
where the carrier set is the set of permutations
and the binary operation is composition.
Furthermore, the permutation system 
fulfils associativity:
$a \cdot (b \cdot c) = (a \cdot b) \cdot c = a \cdot b \cdot c$.
So, it is also a semigroup.
Since the identity permutation
is part of the system,
the system is also a monoid and, to be more specific,
an abelian monoid, since commutativity, as well,
is a property of the composition permutations.

Since we designed the system in a way
that every permutation, applied to itself,
restores the original sequence,
such that $x \cdot y = y \cdot x = e$,
there is also an inverse element 
to every element in the system:
the inverse of a permutation is the composition with itself!
$a \cdot a = e$.
This means that we have found a group!

The set of all possible permutations of a sequence, trivially,
is always a group and called the \term{symmetric group}:
since all possible permutations are in the group,
every possible composition of two permutations
leads to a permutation that is in the group as well,
so it is closed under composition;
composition, as we have seen, is associative;
since, again, all permutations are in the group,
there is an identiy element (the permutation that fixes all elements) and,
since all possible permutations are in the group,
 there is for each permutation 
 a permutation that returns to the original configuration,
the inverse element.
These properties make the symmetric group a group.

But, of course, not all possible subsets of the 
symmetric group are groups. 
Subsets of the symmetric group
that do not contain the identity
are not groups;
sets containing permutations that,
composed with each other,
yield a permutation that is not part of the set
are not groups either.

\section{Random Permutations} % kshuffle
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Random}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{Types}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{Fact}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{\Conid{System}.Random}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Varid{qualified}\;\Conid{\Conid{Data}.\Conid{Vector}.Mutable}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

We have discussed how we can generate
all permutations of a given sequence.
But we have not discussed the much more
frequent task of creating a \term{random}
permutation of a given sequence.

Algorithms creating random permutations
are relatively simple 
compared to those creating all permutations
-- if there were not
the adjective \speech{random}.
Randomness, in fact, is quite a difficult
issue in particular when we are thinking
of ways to achieve real randomness.
Randomness in mathematics is usually
defined in terms of a sequence.
According to the definition
of the great Russian mathematician Andrey Kolmogorov (1903 -- 1987)
who actually axiomatised and thereby 
modernised probability theory,
a sequence is random,
when it cannot be created by a program
that, interpreted as a string, 
is shorter than that sequence.
For instance, we could look at any
sequence of numbers such as
$1,2,3,\dots$ A program to create
such a sequence is just \ensuremath{\Varid{genSeq}\;\Varid{n}\mathrel{=}\Varid{n}\mathbin{:}\Varid{genSeq}\;(\Varid{n}\mathbin{+}\mathrm{1})}
and, obviously, much shorter than the resulting sequence.

When you think of it,
it is indeed difficult to create a sequence
without any \term{patterns} in it,
such as regular distances between elements, 
periodic repetitions and so on.
You may think of any of the sequences we have
looked at so far: there was always a pattern
that led to a way to define a program to
generate that sequence and the program
was always represented as a finite string
of Haskell code that was much shorter
than the sequence, which, usually,
was infinite. For instance, the definitions
of the Fibonacci sequence or of Factorials
are much shorter than that sequences, which
are infinite.
But, even with finite sequences,
we have the same principle.
Look, for instance, at the sequence
$5,16,8,4,2,1$, which, on the first sight,
appears completely random.
However, there is a program that
generates this as well as many other
similar sequences, namely the \term{hailstone}
algorithm:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}c<{\hspost}@{}}%
\column{29E}{@{}l@{}}%
\column{32}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{hailstone}\mathbin{::}\Conid{Integer}\to [\mskip1.5mu \Conid{Integer}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{hailstone}\;\mathrm{1}\mathrel{=}[\mskip1.5mu \mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{hailstone}\;\Varid{n}{}\<[16]%
\>[16]{}\mid \Varid{even}\;\Varid{n}{}\<[29]%
\>[29]{}\mathrel{=}{}\<[29E]%
\>[32]{}\Varid{n}\mathbin{:}\Varid{hailstone}\;(\Varid{n}\mathbin{\Varid{`div`}}\mathrm{2}){}\<[E]%
\\
\>[16]{}\mid \Varid{otherwise}{}\<[29]%
\>[29]{}\mathrel{=}{}\<[29E]%
\>[32]{}\Varid{n}\mathbin{:}\Varid{hailstone}\;(\mathrm{3}\mathbin{*}\Varid{n}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

One may argue that this code is in fact longer
than the resulting sequence. 
But it would be very easy to encode it in a
more concise way, where, for instance,
numerical codes represent the tokens of
the Haskell language. Furthermore,
the code implements the general case that creates
the hailstone sequence for any number $>1$.
For $n=11$, it is already a bit longer:
$11,34,17,52,26,13,40,20,10,5,16,8,4,2,1$.

The hailstone algorithm, by the way,
always terminates with 1, independent of the number $n$
we start with.
This is the \term{Collatz conjecture},
named after the German mathematician 
Lothar Collatz (1910 -- 1990),
who posed the problem in 1937.
It is unproven and it might be undecidable
according to results from John Conway.
But that is another story.

Kolmogorov randomness 
does not only apply to numerical sequences.
When we have a sequence of symbols like
$a,b,c,d,\dots$, there is either some regularity
or it is not possible to define a program
that does not contain the sequence itself and,
hence, has no potential to be shorter
than the sequence in the first place.
The question arises:
how do we generate a random sequence,
if there is no program that generates it
and is significantly shorter than that sequence?
Would that not mean that,
to generate $n$ bits of randomness,
we would need a program that is at least $n$ bits long?
Yes, that is basically the case.
Any short deterministic program,
however this program is implemented,
will follow some rules and will eventually
create a sequence that still bears traces
of that regularity.

The only way to generate true randomness
is to pick up numbers from outside of
the current problem domain, that is
we have to look around to find numbers
from other contexts. 
But, careful: many numbers you see around you still
contain regularities. For instance,
all numbers generated with the current date
as input bear regularity related to the date.
It would not be a good idea to use such a date-related
number to create, say, a session key for
securely encrypted communication through an open channel.

Random number generators implemented in modern
operating systems collect numbers
that are created by the system while operating.
A typical source of randomness is keystrokes.
Every single keystroke creates some data
that is stored in a pool 
for randomness from which other programs
can later request some bits of randomness.
To get access to true random data, thus, implies
that the program requesting those data
needs to interact with the operating system.
Therefore, whenever we need randomness
in Haskell, we need the \ensuremath{\Conid{IO}\;\Conid{Monad}}. 
This adds some complexity to our code;
but, in fact, this complexity just reflects
reality: randomness \emph{is} complex.

In Haskell, there is a module 
called \ensuremath{\Conid{\Conid{System}.Random}}
that provides functions to create
random numbers, both \term{pseudo-random} numbers,
which create sequences that appear random on the
first sight, but are generated by deterministic
algorithms, and true random numbers.
Interesting for us in this module is the function
\ensuremath{\Varid{randomRIO}},
which creates random objects within a
range defined as a tuple.
The call \ensuremath{\Varid{randomRIO}\;(\mathrm{0},\mathrm{9})},
for instance, would create a random number
between 0 to 9 (both included).
Since \ensuremath{\Varid{randomRIO}} does not know our number type
\ensuremath{\Conid{Natural}}, we would have to define a way
for \ensuremath{\Varid{randomRIO}} to create random \ensuremath{\Conid{Natural}}s.
It is much simpler, however, to use a type
known to \ensuremath{\Varid{randomRIO}} and to convert the result
afterwards.
Here is a simple implementation of a function
\ensuremath{\Varid{randomNatural}} that generates a random
natural number:

\begin{minipage}{\textwidth}
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{randomNatural}\mathbin{::}(\Conid{Natural},\Conid{Natural})\to \Conid{IO}\;\Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{randomNatural}\;(\Varid{l},\Varid{u})\mathrel{=}{}\<[26]%
\>[26]{}\mathbf{let}\;{}\<[31]%
\>[31]{}\Varid{il}\mathrel{=}\Varid{fromIntegral}\;\Varid{l}{}\<[E]%
\\
\>[31]{}\Varid{iu}\mathrel{=}\Varid{fromIntegral}\;\Varid{u}{}\<[E]%
\\
\>[26]{}\mathbf{in}\;{}\<[31]%
\>[31]{}\Varid{fromIntegral}\mathbin{<\$>}{}\<[E]%
\\
\>[31]{}(\Varid{randomRIO}\;(\Varid{il},\Varid{iu})\mathbin{::}(\Conid{IO}\;\Conid{Integer})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}
\ignore{$}

The range we want the result to lie in is defined
by the tuple \ensuremath{(\Varid{l},\Varid{u})}, for \ensuremath{\Varid{lower}} and \ensuremath{\Varid{upper}}.
We convert the elements of the tuples to 
\ensuremath{\Varid{li}} and \ensuremath{\Varid{iu}}, which, as we see in an instance,
are of type \ensuremath{\Conid{Integer}}.
We then call the random number generator
with the type signature \ensuremath{\Conid{IO}\;\Conid{Integer}}
defining the output type.
This output is finally converted back
to natural using \ensuremath{\Varid{fromIntegral}}.

The canonical algorithm for generating random numbers
is called \term{Fisher-Yates shuffle}, 
after its inventors Ronald Fisher (1890 -- 1962)
and Frank Yates (1902 --1994), but is also called
\term{Knuth shuffle}, because it became popular through
Knuth's masterpiece.
The algorithm goes through the sequence
we want to permute and, for each index $i$,
that is the place of the element in the sequence
starting from 0, it generates a random number $j$
between 0 and $n-1$, where $n$ is the number
of elements in the sequence.
If this number is different from the current
index, it swaps the elements at positions $i$ and $j$.

Until now, we have worked only with lists.
Lists are extremely efficient, when passing through
from the head to the last.
Now, however, we need to refer to other places
in the list that may be ahead to the end of the sequence
or behind closer to its head,
depending on the value of $j$.
Also, we have to change the list by going through it.
This is essential, because, we might change 
the same place more than once.
For the \ensuremath{\Varid{fold}}-kind of processing that was
so typical for the functions we have studied so far,
this would be extremely ineffecient.
We therefore use another data type,
a mutable vector, defined in \ensuremath{\Conid{\Conid{Data}.\Conid{Vector}.Mutable}}.
First, we will look at a function that creates
a mutable vector from a list:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{createVector}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{IO}\;(\Conid{\Conid{V}.IOVector}\;\Varid{a}){}\<[E]%
\\
\>[3]{}\Varid{createVector}\;\Varid{xs}\mathrel{=}\mathbf{do}\;{}\<[25]%
\>[25]{}\Varid{v}\leftarrow \Varid{\Conid{V}.new}\;(\Varid{length}\;\Varid{xs}){}\<[E]%
\\
\>[25]{}\Varid{initV}\;\Varid{v}\;\mathrm{0}\;\Varid{xs}{}\<[E]%
\\
\>[25]{}\Varid{return}\;\Varid{v}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{initV}\;\anonymous \;\anonymous \;[\mskip1.5mu \mskip1.5mu]{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{return}\;(){}\<[E]%
\\
\>[12]{}\Varid{initV}\;\Varid{v}\;\Varid{i}\;(\Varid{z}\mathbin{:}\Varid{zs}){}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{\Conid{V}.unsafeWrite}\;\Varid{v}\;\Varid{i}\;\Varid{z}{}\<[E]%
\\
\>[33]{}\sequ \Varid{initV}\;\Varid{v}\;(\Varid{i}\mathbin{+}\mathrm{1})\;\Varid{zs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We first create a new vector of the size of the list.
Then we initialise this vector just passing
through the list in a \ensuremath{\Varid{map}} fashion, but
incrementing the index $i$ at each step.
We use the vector function \ensuremath{\Varid{unsafeWrite}},
which takes a vector, \ensuremath{\Varid{v}}, an index, \ensuremath{\Varid{i}},
and the value to write, \ensuremath{\Varid{z}}.
The function is called \ensuremath{\Varid{unsafe}} because
it does not perform a boundary check
(and is, as such, much faster than its
safe cousin).
Since we are careful to move within the boundaries,
there is no huge risk involved in using the
\ensuremath{\Varid{unsafe}} version of this operation.
Finally, we just return the initialised vector.

The next function does the opposite:
it converts a vector back to a list:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{37}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{vector2list}\mathbin{::}\Conid{\Conid{V}.IOVector}\;\Varid{a}\to \Conid{Int}\to \Conid{IO}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{vector2list}\;\Varid{v}\;\Varid{n}\mathrel{=}\Varid{go}\;\mathrm{0}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{i}{}\<[17]%
\>[17]{}\mid \Varid{i}\equiv \Varid{n}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{return}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[17]{}\mid \Varid{otherwise}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\mathbf{do}\;{}\<[37]%
\>[37]{}\Varid{x}\leftarrow \Varid{\Conid{V}.unsafeRead}\;\Varid{v}\;\Varid{i}{}\<[E]%
\\
\>[37]{}(\Varid{x}\mathbin{:})\mathbin{<\$>}\Varid{go}\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}
\ignore{$}

The function is quite simple.
It goes through the vector reading one position
after the other and, when it reaches $n$,
just returns the empty list.
On each step, the value at position $i$ is read
and inserted as the head of the list that results
from recursing on \ensuremath{\Varid{go}}.
Now we are ready to actually implement
the \ensuremath{\Varid{kshuffle}}:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{27}{@{}>{\hspre}c<{\hspost}@{}}%
\column{27E}{@{}l@{}}%
\column{28}{@{}>{\hspre}c<{\hspost}@{}}%
\column{28E}{@{}l@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{42}{@{}>{\hspre}l<{\hspost}@{}}%
\column{48}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{kshuffle}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{IO}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{kshuffle}\;\Varid{xs}\mathrel{=}\mathbf{do}\;{}\<[21]%
\>[21]{}\mathbf{let}\;\Varid{n}\mathrel{=}\Varid{length}\;\Varid{xs}{}\<[E]%
\\
\>[21]{}\Varid{vs}\leftarrow \Varid{createVector}\;\Varid{xs}{}\<[E]%
\\
\>[21]{}\Varid{is}\leftarrow \Varid{randomidx}\;\Varid{n}\;\mathrm{0}{}\<[E]%
\\
\>[21]{}\Varid{go}\;\mathrm{0}\;\Varid{is}\;\Varid{vs}{}\<[E]%
\\
\>[21]{}\Varid{vector2list}\;\Varid{vs}\;\Varid{n}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{randomidx}\;\Varid{n}\;\Varid{k}{}\<[27]%
\>[27]{}\mid {}\<[27E]%
\>[30]{}\Varid{k}\equiv \Varid{n}{}\<[42]%
\>[42]{}\mathrel{=}\Varid{return}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[27]{}\mid {}\<[27E]%
\>[30]{}\Varid{otherwise}{}\<[42]%
\>[42]{}\mathrel{=}\mathbf{do}\;{}\<[48]%
\>[48]{}\Varid{i}\leftarrow \Varid{randomRIO}\;(\mathrm{0},\Varid{n}\mathbin{-}\mathrm{1}){}\<[E]%
\\
\>[48]{}(\Varid{i}\mathbin{:})\mathbin{<\$>}\Varid{randomidx}\;\Varid{n}\;(\Varid{k}\mathbin{+}\mathrm{1}){}\<[E]%
\\
\>[12]{}\Varid{go}\;\anonymous \;[\mskip1.5mu \mskip1.5mu]\;\anonymous {}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Varid{return}\;(){}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{k}\;(\Varid{i}\mathbin{:}\Varid{is})\;\Varid{vs}{}\<[28]%
\>[28]{}\mathrel{=}{}\<[28E]%
\>[31]{}\Varid{when}\;(\Varid{k}\not\equiv \Varid{i})\;(\Varid{\Conid{V}.unsafeSwap}\;\Varid{vs}\;\Varid{k}\;\Varid{i}){}\<[E]%
\\
\>[31]{}\sequ \Varid{go}\;(\Varid{k}\mathbin{+}\mathrm{1})\;\Varid{is}\;\Varid{vs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}
\ignore{$}

We start by creating the vector
using the function \ensuremath{\Varid{createVector}} defined above.
Note that, since we need it
more than once, we initially store
the size of the list in the variable \ensuremath{\Varid{n}}.
Since we compute it again in \ensuremath{\Varid{createVector}},
there is potential for improvement.

In the next step, we create a list of \ensuremath{\Varid{n}} numbers
using \ensuremath{\Varid{randomidx}}. \ensuremath{\Varid{randomidx}} calls \ensuremath{\Varid{randomRIO}}
\ensuremath{\Varid{n}} times making each result head of the list
that is constructed by recursion.
Note that we do not use \ensuremath{\Varid{randomNatural}}.
We will see in \ensuremath{\Varid{go}} that the results of \ensuremath{\Varid{randomidx}}
are used as vector indices and, since vector indices
are of type \ensuremath{\Conid{Int}}, we spare some forth and back conversions.
\ensuremath{\Varid{go}} expects three arguments: an \ensuremath{\Conid{Int}} called $k$, a list of \ensuremath{\Conid{Int}},
these are the random indices just created, and the vector
on which we are operating.
For each index in the list, we swap 
the value at position \ensuremath{\Varid{k}} in the vector,
which is the index in the natural ordering starting from 0,
with the value at position \ensuremath{\Varid{i}} and continue with the recursion
on \ensuremath{\Varid{go}} with \ensuremath{\Varid{k}\mathbin{+}\mathrm{1}} and the tail
of the list of random indices.
Finally, we call \ensuremath{\Varid{vector2list}} on the manipulated vector
yielding a permutation of the input list.

One may be tempted to say that the permutation
is generated by a permutation of the indices
of the initial list. But do not be fooled!
The random indices we are generating do not consitute,
at least not necessarily, a valid permutation
of the natural ordering of the input list.
Each index is generated randomly -- completely
\term{independent} of the other indices.
In consequence, some of the values we get
back from \ensuremath{\Varid{randomRIO}}, in fact, at least theoretically,
all of them, may be equal --
and this is the whole point of this shuffle.

Consider the input list $a,b,c,d,e$
with the natural ordering of positions
$0,1,2,3,4$, \ie\ at postion 0, we have $a$,
at position 1, we have $b$,
at position 2, we have $c$ and so on.
\ensuremath{\Varid{randomidx}} could result in a list of
random indices like, for example, $2,0,1,3,4$,
which would be a permutation of the natural order.
However, it may also result in a list like
$2,0,1,1,4$, which is not a permutation.
The \ensuremath{\Varid{kshuffle}} algorithm does not require
the constraint that the indices we create
form a permutation of the initial order.
It guarantees that the overall result is
actually a permutation of the input list
without such a constraint.
This saves us from the trouble of checking
the result of the random number generator
and calling it again each time,
there is a collision.

Imagine \ensuremath{\Varid{randomidx}} would create the list
$1,1,1,1,1$, which we could obtain with a probability
of 
$\frac{1}{5} \times \frac{1}{5} \times$
$\frac{1}{5} \times \frac{1}{5} \times \frac{1}{5} =$
$\frac{1}{5^5} = \frac{1}{3125}$.
We now \ensuremath{\Varid{go}} through the natural positions \ensuremath{\Varid{k}},
$0\dots 4$ and the vector
initially representing the list $a,b,c,d,e$.
It is essential to realise that operations
on a mutable vector are \term{destructive},
that is all operations are performed on
the current state of the vector, which
changes from step to step, such that
the output of each step is the input to
the next step. What happens is the following:

\begin{enumerate}
\item We swap position 0 and 1 resulting in
      $b,a,c,d,e$;
\item We do not do anything, because 
      the indices $k$ and $i$ are both 1
      in the second step, maintaining
      $b,a,c,d,e$;
\item We swap positions 2 and 1 resulting in
      $b,c,a,d,e$;
\item We swap positions 3 and 1 resulting in
      $b,d,a,c,e$;
\item We swap positions 4 and 1 resulting in
      $b,e,a,c,d$,
\end{enumerate}

resulting overall in a valid permutation
of the input list.
 

\section{Binomial Coefficients}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Binom}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Fact}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Closely related to permutations
are problems of selecting a number of items
from a given set.
Whereas 
permutation problems have the structure of
shuffling cards,
selection problems have that of dealing cards.
This analogy
leads to an intuitive and simple algorithm
to find all possible selections of 
$k$ out of a set with $n$ elements by taking
the first $k$ objects from all possible
permutations of this set
and, afterwards, removing the duplicates.
Consider the set $\lbrace 1,2,3\rbrace$ and
let us find all possible selections
of two elements of this set.
We start by choosing 
the first two elements of 
the given sequence and get 
$\lbrace\lbrace 1,2\rbrace\rbrace$.
Now we create a permutation: $\lbrace 2,1,3\rbrace$
and, again, take the first two elements.
The result set is now: 
$\lbrace \lbrace 1,2\rbrace,\lbrace 2,1\rbrace\rbrace$.
We continue with the next permuation
$\lbrace 2,3,1\rbrace$, which leads us to the result set
$\lbrace 
 \lbrace 1,2\rbrace, 
 \lbrace 2,1\rbrace, 
 \lbrace 2,3\rbrace\rbrace$.
Going on this way --
and we already have defined an algorithm
to create all possible permutations of a set
in the previous section --
we finally get to the result set 
$\lbrace 
 \lbrace 1,2\rbrace, \lbrace 2,1\rbrace, 
 \lbrace 2,3\rbrace, \lbrace 3,2\rbrace,
 \lbrace 3,1\rbrace, \lbrace 1,3\rbrace\rbrace$.
Since, as we know from the previous section,
there are $3! = 6$ permutations,
there are also six sequences with the first $k$ elements
of these six permutations.
But, since we want unique selections,
not permutations of the same elements,
we now remove the duplicates from this result set
and arrive at
$\lbrace
 \lbrace 1,2\rbrace,
 \lbrace 2,3\rbrace,
 \lbrace 1,3\rbrace\rbrace$,
that is three different selections 
of two elements out of three.

This algorithm suggests
that the number of $k$ selections out of $n$ elements
is somehow related to the factorial function.
But, obviously, the factorial is too big a result,
we have to reduce the factorial 
by the number of the permutations of the results.
Let us think along the lines of permutation:
we have 3 ways to select 1 object out of 3:
$\lbrace
 \lbrace 1\rbrace, 
 \lbrace 2\rbrace,
 \lbrace 3\rbrace\rbrace$.
For the factorial,
we said that we now combine 
all permutations of the remaining 2 objects
with this 3 possible solutions 
and compute the number of these permutations as $3 \times 2$.
However, since order does not matter,
the first selection conditions the following selections.
After the first step, we seemingly have two options 
for each of the first selections in step 2:

\begin{center}
\begin{tabular}{ c | c }
step 1 & step 2 \\\hline 
1      & $\lbrace 2,3\rbrace$ \\ 
2      & $\lbrace 1,3\rbrace$ \\ 
3      & $\lbrace 1,2\rbrace$ 
\end{tabular}
\end{center}

But note that, when we select 2 in the first row,
the option 1 in the second row will vanish,
since we already selected $\lbrace 1,2\rbrace$,
which is the same as $\lbrace 2,1\rbrace$.
Likewise, when we select 3 in the first row,
we cannot select 1 in the third row
because, again, $\lbrace 1,3\rbrace$ is the same as
$\lbrace 3,1\rbrace$.
It, therefore, would be much more appropriate
to represent our options as in the following table:

\begin{center}
\begin{tabular}{ c | c }
step 1 & step 2 \\\hline 
1      & $\lbrace 2,3\rbrace$ \\ 
2      & $\lbrace   3\rbrace$ \\ 
3      & $\lbrace    \rbrace$ 
\end{tabular}
\end{center}

At the beginning,
we are completely free to choose
any element,
but when we come to the second,
the options are suddenly reduced
and at the third step
there are no options left at all.
For the case 2 out of 3, we see
that the first selection halves our options in the second step.
This suggests that we have to divide the number of options per step.
With permutation, we had
$n \times (n-1)$,
but with selection, we apparently have
something like $n \times \frac{n-1}{2}$,
which, for the case 2 out of 3, is 
$3 \times \frac{3-1}{2} = 3 \times \frac{2}{2} = 3 \times 1 = 3$.
When we continue this scheme,
considering that each choice that was already made
conditions the next choice,
we get a product of the form:
$\frac{n}{1} \times \frac{n-1}{2} \times \frac{n-2}{3} \times \dots$
Selecting 3 out of 5, for instance, is:
$\frac{5}{1} \times \frac{4}{2} \times \frac{3}{3} = 10$.
This leads to the generalised product for $k$ out $n$:
$\frac{n}{1} \times \frac{n-1}{2} \times \dots \times \frac{n - (k-1)}{k}$.
This product is known as the binomial coefficient $\binom{n}{k}$
pronounced $n$ \emph{choose} $k$.

We easily see
that the part below the fraction line  
is $k!$ 
The part above the line
is a partial factorial of $n$,
called falling factorial or  \term{to-the-k}$^{th}$\term{-falling}:

\begin{equation}
  n^{\underline{k}} = n \times (n-1) \times \dots \times (n-k+1) = 
  \prod_{j=1}^{k}{n + 1 - j}.
\end{equation}

We, therefore, can represent the
binomial coefficient as either:

\begin{equation}\label{eq:binomProduct}
\binom{n}{k} = 
\prod_{j=1}^{k}{\frac{n + 1 - j}{j}}
\end{equation}

or:

\begin{equation}\label{eq:binomFalling}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}.
\end{equation}

But there is still another formula,
which, even though less efficient
in terms of computational complexity,
is often used to ease proofs
involving binomial coefficients and
which is closer to our first
intuition that the selection is
somehow related to factorials
reduced by some value:

\begin{equation}
\binom{n}{k} = \frac{n!}{k! \times (n-k)!}.
\end{equation}

It can be seen immediately that this formula
is equivalent to equation \ref{eq:binomFalling},
whenever $k \le n$, since the values of $n!$
in the numerator cancel out with the values of
$(n-k)!$ in the denominator.
Indeed, $n!$ could be split into two
halves (which are not necessarily equal of course), 
the upper product $n^{\underline{k}}$
($n \times (n-1) \times \dots \times (n-k+1)$)
and the lower product 
($1 \times 2 \times\dots \times (n-k)$).
By cancelling out the lower half,
we remove the lower product
from numerator and denominator
and are left with the falling factorial
in the numerator.

We could have derived equation \ref{eq:binomFalling}
much more easily with a different kind of reasoning:
Given a set with $n$ elements,
there are $n^{\underline{k}}$ permutations
of $k$ elements of this set.
There are $n$ ways to choose the first element,
$n-1$ ways to choose the second element and so on
and $n-k+1$ ways to choose the $k^{th}$ element.
Obviously, we could reach the same result,
all permutations of $k$ elements out of $n$,
by first selecting these $k$ elements
and then create all possible permutations
of these $k$ elements. 
The number of possibilities of
choosing $k$ out of $n$ is the binomial coefficient, $\binom{n}{k}$,
which we would like to derive. 
The possible permutations of these $k$ elements
is of course $k!$
We now have to combine these two steps:
We have for any selection of $k$ elements out of $n$
$k!$ permutations, that is $\binom{n}{k} \times k!$
Since this processing has the same result
as choosing all permutations of $k$ out of $n$
in the first place, we come up with the equation:

\begin{equation}
n^{\underline{k}} = \binom{n}{k} \times k!
\end{equation}

To know what the expression $\binom{n}{k}$ is
we just divide $k!$ on both sides of the equation
and get equation \ref{eq:binomFalling}:

\begin{equation}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}.
\end{equation}

Let us look at some concrete values
of the binomial coefficients:
$\binom{n}{0} = \binom{n}{n} = 1$ and
% $\binom{n}{1} = \binom{n}{n-1} = n$. 
for $k < 0$ or $k > n$: $\binom{n}{k} = 0$.
For $0 \le k \le n$, for instance:
$\binom{3}{2} = 3$, 
$\binom{4}{2} = 6$, 
$\binom{4}{3} = 4$, 
$\binom{5}{2} = 10$,
$\binom{5}{3} = 10$.
We can arrange the results
in a structure, called Pascal's Triangle, after the great
French mathematician and philosopher Blaise Pascal (1623 -- 1662)
who used binomial coefficients 
to investigate probabilities and,
in the process, created a new branch of mathematics,
namely probability theory:

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
0 &   &   &   &   &    &    &    &    &     &  1 &     &    &    &    &    &   &   &   &   &  \\
1 &   &   &   &   &    &    &    &    &   1 &    &   1 &    &    &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &    &    &  1 &     &  2 &     &  1 &    &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &    &  1 &    &   3 &    &   3 &    &  1 &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &  1 &    &  4 &     &  6 &     &  4 &    &  1 &    &   &   &   &   &  \\
5 &   &   &   &   &  1 &    &  5 &    &  10 &    &  10 &    &  5 &    &  1 &   &   &   &   &  \\   
6 &   &   &   & 1 &    &  6 &    & 15 &     & 20 &     & 15 &    &  6 &    & 1 &   &   &   &  \\
7 &   &   & 1 &   &  7 &    & 21 &    &  35 &    &  35 &    & 21 &    &  7 &   & 1 &   &   &  \\
8 &   & 1 &   & 8 &    & 28 &    & 56 &     & 70 &     & 56 &    & 28 &    & 8 &   & 1 &   & \\
9 & 1 &   & 9 &   & 36 &    & 84 &    & 126 &    & 126 &    & 84 &    & 36 &   & 9 &   & 1
\end{tabular}

In this triangle, each row represents 
the coefficients for one specific value of $n$ in $\binom{n}{k}$.
The left-most value in each line
represents the value $\binom{n}{0} = 1$
and the right-most value is $\binom{n}{n} = 1$. 
The values between the outermost ones
represent the values for $\binom{n}{1} \dots \binom{n}{n-1}$.
The line for $n = 2$, \ie\ the third line, for instance,
shows the values 
$\binom{2}{0} = 1$, $\binom{2}{1} = 2$ and $\binom{2}{2} = 1$.
The line for $n = 3$ shows the values
$\binom{3}{0} = 1$, 
$\binom{3}{1} = 3$,
$\binom{3}{2} = 3$ and
$\binom{3}{3} = 1$,
the line for $n = 4$ shows the values
$\binom{4}{0} = 1$, 
$\binom{4}{1} = 4$,
$\binom{4}{2} = 6$, 
$\binom{4}{3} = 4$ and
$\binom{4}{4} = 1$ and so on.

This extraordinary triangle
reveals many ``hidden'' relations of the binomial coefficients.
We can observe, to start with this one,
that the triangle is horizontally symmetric,
\ie\ $\binom{3}{1} = \binom{3}{2} =  3$,
     $\binom{6}{2} = \binom{6}{4} = 15$,
     $\binom{7}{2} = \binom{7}{5} = 21$
or, in general, 
$\binom{n}{k} = \binom{n}{n-k}$.
This is a strong hint 
how we can optimise 
the computation of the binomal coefficients.
Indeed, whenever $k$ in $\binom{n}{k}$
is more than the half of $n$,
we can use the corresponding value
from the first half of $k$'s,
\ie\ 

\begin{equation}
\binom{n}{k} = \begin{cases}
                 \binom{n}{n-k} & \textrm{if $2k > n$}\\
                 \prod_{j=0}^{k}{\frac{n + 1 - j}{j}} & \textrm{otherwise}
               \end{cases}
\end{equation}

Thank you, Triangle!

Another observation is
that every coefficient is the sum
of two preceding coefficients,
namely the one left-hand up and the one right-hand up,
\eg\
$\binom{3}{1} = \binom{2}{0} + \binom{2}{1} =  3$,
$\binom{4}{2} = \binom{3}{1} + \binom{3}{2} =  6$,
$\binom{5}{2} = \binom{4}{1} + \binom{4}{2} = 10$
or, in general:

\begin{equation}\label{eq:binomPascalRule}
\binom{n+1}{k} = \binom{n}{k-1} + \binom{n}{k}.
\end{equation}

This identity called \term{Pascal's Rule}
does not only help us to guess the next value
in a sequence,
but is also the basis for techniques
to manipulate equations involving binomial coefficients.

A real light bulb moment, however,
comes when realising 
the relation of binomial coefficients 
to multiplication.
We discussed several times already
that there are certain patterns in
multiplication,
which now turn out of have a name:
\term{binomial coefficient}.
Indeed, this relation is one of the most important
theorems in mathematics, the \term{binomial theorem},
which we will formulate in a second.
First, let us look at the multiplication pattern.
The distributive law tells us that

\begin{equation}
(a + b) (c + d) = ac + ad + bc + bd.
\end{equation}

Now, what happens if $a = c$ and $b = d$?
We would then get:

\begin{equation}
(a + b) (a + b) = aa + ab + ba + bb,
\end{equation}

which is the same as

\begin{equation}
\mathbf{(a + b) (a + b) = a^2 + 2ab + b^2}.
\end{equation}

When we now multiply $(a + b)$ with this result, we get:

\begin{equation}
\begin{split}
(a + b) (a^2 + 2ab + b^2) = a^3 + 2a^2b + ab^2 + ba^2 + 2ab^2 + b^3 = \\
a^3 + 2a^2b + ba^2 + ab^2 + 2ab^2 + b^3 = \\
\mathbf{a^3 + 3a^2b + 3ab^2 + b^3} 
\end{split}
\end{equation}

Multiplied with $(a + b)$ once again:

\begin{equation}
\begin{split}
(a + b) (a^3 + 3a^2b + 3ab^2 + b^3) = \\
a^4 + 3a^3b + 3a^2b^2 + ab^3 + ba^3 + 3a^2b^2 + 3ab^3 + b^4 = \\
a^4 + 3a^3b + ba^3 + 3a^2b^2 + 3a^2b^2 + ab^3 + 3ab^3 + b^4 = \\
\mathbf{a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + b^4} 
\end{split}
\end{equation}

The coefficients in these formulas, as you can see, 
equal the binomial coefficients in Pascal's Triangle.
The Triangle can thus be interpreted
as results of power functions:

\[
(a + b)^0 = 1
\]
\[
(a + b)^1 = 1a + 1b
\]
\[
(a + b)^2 = 1a^2 + 2ab + 1b^2
\]
\[
(a + b)^3 = 1a^3 + 3a^2b + 3ab^2 + 1b^3
\]
\[
(a + b)^4 = 1a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + 1b^4
\]
\[
(a + b)^5 = 1a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + 1b^5
\]
\[
\dots
\]

This, in general, is the binomial theorem:

\begin{equation}
\begin{split}
(x + y)^n = \binom{n}{0} x^ny^0 + \binom{n}{1}x^{n-1}y^1 + \dots +
            \binom{n}{n} x^0y^n \\
= \sum_{k=0}^{n}{\binom{n}{k}x^ky^{n-k}}
\end{split}
\end{equation}

But why is this so? 
According to multiplication rules,
the multiplication of two factors $(a+b) (c+d)$
yields a combination of each of the terms
of one of the factors with the terms of the other factor:
$ac + ad + bc + bd$.
If $a = c$ and $b = d$,
we will create combinations of terms with themselves:
$aa + ab + ba + bb$.
How many ways are there
to combine $a$ with $a$ in $(a + b) (a + b)$?
There is exactly one way,
because the $a$ of the first factor
will find exactly one $a$ in the second factor.
But how many ways are there 
to combine $a$ and $b$?
Well, the $a$ in the first factor 
will find one $b$ in the second,
and the $b$ in the first factor 
will find one $a$ in the second.
There are hence two ways to combine $a$ and $b$
and we could interpret these two combinations
as two different \term{strings},
the string $ab$ and the string $ba$.
We know that there are $\binom{2}{1} = 2$ different ways
to select one of these strings:
either $ab$ or $ba$.
Since these strings represent products
of $a$ and $b$ and, according to the commutative law,
the order of the factors does not matter,
we can just add them up, which leaves us with 
a coefficient that states exactly how many
strings of homogeneous $a$s and $b$s 
there are in the sum.

There is a nice illustration of this argument:
Let us look at the set of the two numbers $\lbrace 1,2\rbrace$.
There are two possibilities to select one of these numbers: 1 or 2.
Now, we could interpret these numbers as answer to the question
``What are the positions where one of the characters 'a' and 'b'
can be placed in a two-character string?''
The answer is: either at the beginning 
or at the end, \ie\ either $\mathbf{a}b$ 
or $b\mathbf{a}$.
For $(a + b)^3$, this is even more obvious.
Compare the positions of the $a$'s in terms with two $a$'s
with the possible selections
$\lbrace
 \lbrace 1,2\rbrace,
 \lbrace 1,3\rbrace,
 \lbrace 2,3\rbrace
 \rbrace$
of two out of the set $\lbrace 1,2,3\rbrace$:
$(a + b) (aa + ab + ba + bb) = 
 aaa + \mathbf{aa}b + \mathbf{a}b\mathbf{a} + abb + b\mathbf{aa} + bab + bba + bb$.

This is a subtle argument.
To assure ourselve
that the theorem really holds for all $n$,
we should try a proof by induction. 
We have already demonstrated
that it indeed holds for several cases,
like $(a + b)^0$, $(a + b)^1$, $(a + b)^2$
and so on.
Any of these cases serves as base case.
Assuming the base case holds,
we will show that

\begin{equation}\label{eq:binomProof1}
  (a + b)^{n + 1} = \sum_{k=0}^{n + 1}{\binom{n + 1}{k}a^kb^{n+1-k}}.
\end{equation}

We start with the simple equation

\begin{equation}
  (a + b)^{n + 1} = (a + b)^n (a + b)
\end{equation}

and then reformulate it replacing $(a + b)^n$ by the base case:

\begin{equation}
  (a + b)^{n + 1} = \left(\sum_{k=0}^{n}{\binom{n}{k}a^kb^{n-k}}\right) (a + b).
\end{equation}

We know that, to multiply a sum with another sum, 
we have to distribute
all the terms of one sum over all terms of the second sum.
This is, we multiply $a$ with the summation 
and then we multiply $b$ with the summation.
In the first case, the exponents of $a$ within the summation
are incremented by one, in the second case,
the exponents of $b$ are incremented by one:

\begin{equation}\label{eq:binomProofDist1}
  (a + b)^{n + 1} = \sum_{k=0}^{n}{\binom{n}{k}a^{k+1}b^{n-k}} +
                    \sum_{k=0}^{n}{\binom{n}{k}a^kb^{n+1-k}}.
\end{equation}

The second term looks already quite similar to the case
in equation \ref{eq:binomProof1}, 
both have $a^kb^{n+1-k}$.
Now, to make the first term match as well,
we will use one of those \term{tricks} 
that make many feel that math is just 
about pushing meaningless symbols back and forth.
Indeed, since we are working with sums here,
the proof involves much more technique 
than the proofs we have seen so far.
The purpose, however, is still the same:
we want to show that we can transform 
one formula into another 
by manipulating these formulas according to
simple grammar rules.
That this has a very technical, even \term{tricky}
flavour is much more related to the limitations
of our mind that does not see through things
as simple as numbers,
but has to create formal apparatus
not to get lost in the dark woods of reasoning.

Well, what is that trick then?
The trick consists in raising the $k$
in the summation index and to change the terms
in the summation formula accordingly,
that is, instead of $a^{k+1}$, we want to have $a^k$
and we achieve this, by not letting $k$ 
run from 0 to $n$,
but from 1 to $n+1$:

\begin{equation}\label{eq:binomProofDirty}
  (a + b)^{n + 1} = \sum_{k=1}^{n+1}{\binom{n}{k-1}a^{k}b^{n+1-k}} +
                    \sum_{k=0}^{n}{\binom{n}{k}a^kb^{n+1-k}}.
\end{equation}

Please confirm for yourself with pencil and paper 
that the first summation in equations 
\ref{eq:binomProofDist1} and \ref{eq:binomProofDirty}
is the same:

\[
  \sum_{k=0}^{n}{\binom{n}{k}a^{k+1}b^{n-k}} =
  \sum_{k=1}^{n+1}{\binom{n}{k-1}a^{k}b^{n+1-k}} 
\]

All we have done is pushing the index of the summation one up
and, to maintain the value of the whole, reducing $k$ by one
in the summation formula.

Now we want to combine the two sums,
but, unfortunately, 
after having pushed up the summation index,
the two sums do not match anymore.
Apparently, while trying to solve one problem,
we have created another one.
But hold on!
Let us try a bit
and just take the case $k=n+1$ in the first term
and the case $k=0$ in the second term out.
The case $k=n+1$ corresponds to the expression
$\binom{n}{n+1-1}a^{n+1}b^{n+1-(n+1)}$,
which, of course, is simply $a^{n+1}$,
since $\binom{n}{n+1-1} = \binom{n}{n} = 1$
and $b^{n+1-(n+1)} = b^{n+1-n-1} = b^0 = 1$.
Accordingly, the case $k=0$ in the second term
corresponds to
$\binom{n}{0}a^0b^{n+1-0} = b^{n+1}$.
When we combine all those again, we get to:

\begin{equation}
  (a + b)^{n + 1} = a^{n+1} + 
                    \sum_{k=1}^{n}{\binom{n}{k-1}a^{k}b^{n+1-k}} +
                    \sum_{k=1}^{n}{\binom{n}{k}a^kb^{n+1-k}} +
                    b^{n+1}.
\end{equation}

The next step provides you with a test
of how well you have internalised
the distributive law.
The sum of the two summations has the form:
$(\alpha c + \alpha d) + (\beta c + \beta d)$,
where $\alpha = \binom{n}{k-1}$ 
and   $\beta  = \binom{n}{k}$
and $c$ and $d$ represent 
different steps of the summations,
\ie\ $c$ is $a^kb^{n+1-k}$ for $k = 1$ 
and $d$ the same for $k=2$ and so on.
Please make sure that you see this analogy!

By applying the distributive law once,
we get to
$\alpha (c + d) + \beta (c + d)$.
This is really fundamental -- 
please make sure you get to the same result
by distributing $\alpha$ and $\beta$
over their respective $(c + d)$!

Now we apply the distributive law once again
taking $(c + d)$ out:
$(\alpha + \beta) (c + d)$.
Please make sure again that this holds for you
by distributing $(c + d)$ over $(\alpha + \beta)$!

When we substitute $\alpha$ and $\beta$
by the binomial coefficients,
we get $(\binom{n}{k-1} + \binom{n}{k}) (c + d)$,
right?
In the next equation, we have just applied these little
steps:

\begin{equation}
  (a + b)^{n + 1} = a^{n+1} + 
                    \sum_{k=1}^{n}{\left(\binom{n}{k-1} + \binom{n}{k}\right)a^{k}b^{n+1-k}} +
                    b^{n+1}.
\end{equation}

Now you might recognise Pascal's rule 
given in equation \ref{eq:binomPascalRule} above.
Indeed, the Almighty Triangle tells us
that $\binom{n}{k-1} + \binom{n}{k} = \binom{n+1}{k}$.
In other words, 
we can simplify the equation to

\begin{equation}
  (a + b)^{n + 1} = a^{n+1} + 
                    \sum_{k=1}^{n}{\binom{n+1}{k}a^{k}b^{n+1-k}} +
                    b^{n+1}.
\end{equation}

Finally, we integrate the special cases $k=0$ and $k=n+1$ again,
just by manipulating the summation index:

\begin{equation}
  (a + b)^{n + 1} = \sum_{k=0}^{n+1}{\binom{n+1}{k}a^{k}b^{n+1-k}} 
\end{equation}

and we are done, since 
-- using some mathematical trickery --
we have just derived
equation \ref{eq:binomProof1}.

Coming back to the question of how to implement
binomial coefficients efficiently,
we should compare the two alternatives
we have already identified as possible candidates,
\viz\ equations \ref{eq:binomProduct} and 
\ref{eq:binomFalling},
which are repeated here for convenience:

\begin{equation}
\binom{n}{k} = 
\prod_{j=1}^{k}{\frac{n + 1 - j}{j}},
\end{equation}

\begin{equation}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}.
\end{equation}

The first option
performs $k$ divisions and $k-1$ multiplications:
one division per step and the multiplications
of the partial results,
that is $k + k - 1 = 2k-1$ operations in total,
not counting the sum $n + 1 -j$,
which is a minor cost factor.

The second option
performs $k-1$ multiplications for $n^{\underline{k}}$,
$k-1$ multiplications for $k!$ and one division,
hence, $k - 1 + k - 1 + 1 = 2k-1$ operations.
That looks like a draw.

In general terms,
there is an argument concerning implementation strategy
in favour of the first option.
With the second option, we first create
two potentially huge values that must be kept in memory,
namely $n^{\underline{k}}$ and $k!$.
When we have created these values,
we reduce them again dividing one by the other.
The first option, in contrast,
builds the final result by stepwise incrementation
without the need 
to create values greater than the final result.
So, let us implement the first option:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}c<{\hspost}@{}}%
\column{15E}{@{}l@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{choose}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{choose}\;\Varid{n}\;\mathrm{0}{}\<[15]%
\>[15]{}\mathrel{=}{}\<[15E]%
\>[18]{}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{choose}\;\Varid{n}\;\mathrm{1}{}\<[15]%
\>[15]{}\mathrel{=}{}\<[15E]%
\>[18]{}\Varid{n}{}\<[E]%
\\
\>[3]{}\Varid{choose}\;\Varid{n}\;\Varid{k}{}\<[15]%
\>[15]{}\mid {}\<[15E]%
\>[18]{}\Varid{k}\mathbin{>}\Varid{n}{}\<[29]%
\>[29]{}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[15]{}\mid {}\<[15E]%
\>[18]{}\mathrm{2}\mathbin{*}\Varid{k}\mathbin{>}\Varid{n}{}\<[29]%
\>[29]{}\mathrel{=}\Varid{choose}\;\Varid{n}\;(\Varid{n}\mathbin{-}\Varid{k}){}\<[E]%
\\
\>[15]{}\mid {}\<[15E]%
\>[18]{}\Varid{otherwise}{}\<[29]%
\>[29]{}\mathrel{=}\Varid{go}\;\mathrm{1}\;\mathrm{1}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{m}\;\Varid{i}{}\<[19]%
\>[19]{}\mid \Varid{i}\mathbin{>}\Varid{k}{}\<[32]%
\>[32]{}\mathrel{=}\Varid{m}{}\<[E]%
\\
\>[19]{}\mid \Varid{otherwise}{}\<[32]%
\>[32]{}\mathrel{=}\Varid{go}\;(\Varid{m}\mathbin{*}(\Varid{n}\mathbin{-}\Varid{k}\mathbin{+}\Varid{i})\mathbin{\Varid{`div`}}\Varid{i})\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The implementation is straight forward.
The function $choose$ is defined over natural numbers,
so we do not have to deal with negative numbers.
The first parameter corresponds to $n$, 
the second one to $k$ in $\binom{n}{k}$.
Whenever $k = 0$, the result is $1$
and if $k=1$, the result is $n$.
For all other cases,
we first test if $k > n$;
if so, the result is just 0.
Otherwise, we make the distinction $2k > n$
and if so, we calculate $choose$ for $n$ and $n-k$,
\eg\ $\binom{5}{4} = \binom{5}{1}$.
Otherwise we build the product using the function $go$
that is defined as follows:
If $i > k$, we use $m$,
otherwise we recurse with the result of 
$\frac{m \times (n - k + i)}{i}$
and $i + 1$.

Let us look at the example $\binom{5}{3}$.
We start with \ensuremath{\Varid{go}\;\mathrm{1}\;\mathrm{1}}, which expands to

\ensuremath{\Varid{go}\;(\mathrm{1}\mathbin{*}(\mathrm{5}\mathbin{-}\mathrm{3}\mathbin{+}\mathrm{1})\mathbin{\Varid{`div`}}\mathrm{1})\;(\mathrm{1}\mathbin{+}\mathrm{1})},

which is \ensuremath{\Varid{go}\;\mathrm{3}\;\mathrm{2}}.
This, in its turn, expands to 

\ensuremath{\Varid{go}\;(\mathrm{3}\mathbin{*}(\mathrm{5}\mathbin{-}\mathrm{3}\mathbin{+}\mathrm{2})\mathbin{\Varid{`div`}}\mathrm{2})\;(\mathrm{2}\mathbin{+}\mathrm{1})},

which equals \ensuremath{\Varid{go}\;\mathrm{6}\;\mathrm{3}}, expands to

\ensuremath{\Varid{go}\;(\mathrm{6}\mathbin{*}(\mathrm{5}\mathbin{-}\mathrm{3}\mathbin{+}\mathrm{3})\mathbin{\Varid{`div`}}\mathrm{3})\;(\mathrm{3}\mathbin{+}\mathrm{1})}

and results in \ensuremath{\Varid{go}\;\mathrm{10}\;\mathrm{4}}.
Since $i$ is now greater than $k$, $4 > 3$,
we just get back $m$, which is 10 and, thus, the correct result.

A word of caution might be in place here.
The $choose$ function above
does not implement the product in the formula
one-to-one.
There is a slight deviation,
in that we multiply the result of the previous step
with the sum of the current step,
before we apply the division.
The reason becomes obvious,
when we look at $\binom{6}{3}$, for instance.
According to the formula,
we would compute 
$\frac{6 + 1 - 1 = 6}{1} \times
 \frac{6 + 1 - 2 = 5}{2} \times \dots$.
The second factor, $\frac{5}{2}$,
is not a natural number --
we cannot express this value with 
the only tool we own so far.
The result however is the same,
which you can prove to yourself
simply by completing the product above
and comparing your result with the All-knowing Triangle.
We will investigate binomial coefficients
more deeply, especially the question
why they always result in an integer
in spite of division being involved.

\ignore{
  - worshipping?
}

\section{Combinatorial Problems with Sets}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Set}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Varid{qualified}\;\Conid{\Conid{Data}.Vector}\;\Varid{as}\;\Conid{V}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{\Conid{Data}.Vector}\;((\mathbin{!})){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;{}\<[20]%
\>[20]{}\Conid{\Conid{Data}.List}\;(\Varid{nub},\Varid{delete},(\mathbin{\char92 \char92 })){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

Many real-world problems can be modelled
in terms of \term{sets}.
We have already used sets informally
and, indeed, they are of tremendous importance
in the whole field of mathematics --
set theory is even believed to provide
a sound fundamentation for most areas of mathematics.
This, however, is strongly contested since more than
hundred years now and today 
there are other candidates for this role
besides set theory. But today
many, if not most mathematicians, 
after long battles
over the foundations of math
mainly during the first half of the $20^{th}$, 
are tired of discussing these issues.

Anyway, what is a set in the first place?
Georg Cantor (1845 -- 1918), one of the main inventors of set theory,
provided several definitions, 
for instance: 
``a set is a Many that allows being thought of as a One''
or: ``a collection of distinct objects''.
Both definitions are quite abstract,
but, in this respect, 
they express a major aspect of set theory quite well.

The second definition, ``collection of distinct objects'',
serves our purposes well enough. 
A set can consist of any kind of objects,
as long as these objects can be clearly distinguished.
Examples are: 
The set of all green things,
the set of all people,
The set of Peter, Paul and Mary,
the set of all animals that belong to the emperor,
the set of the natural numbers from 1 to 9
the set of all natural numbers
and so on.

There are different ways to define sets.
We can first give a definition:
the set of the natural numbers from 1 to 9,
the set of all people, 
the members of the Simpsons family, \etc\
But we can also name the members explicitly:
Homer, Marge, Bart, Lisa and Maggie or
$1,2,3,4,5,6,7,8,9$.

The first way to define a set is called 
by \term{intension}. The intension of a set is
what it implies, without referring explicitly
to its members.
The second way is called by \term{extension}.
The extension of a set consists of all its members.

This distinction is used in different ways of
defining lists in Haskell.
One can define a list by extension:
\haskell{[1,2,3,4,5]}
or by intension:
\haskell{[1..5]}, \haskell{[1..]}.
A powerful tool to define lists by intension 
is list comprehension, for instance:

\ensuremath{[\mskip1.5mu \Varid{x}\mid \Varid{x}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mathrm{100}\mskip1.5mu],\Varid{even}\;\Varid{x},\Varid{x}\mathbin{\Varid{`mod`}}\mathrm{3}\not\equiv \mathrm{0}\mskip1.5mu]},

which would contain all even numbers
between 1 and 100 that are not multiples of 3.

Defining sets by intension is 
very powerful. 
The overhead of constructing a set by extension,
\ie\ by naming all its members,
is quite heavy.
If we had to mention all numbers we wanted to use
in a program beforehand, the code would become incredibly large
and we would need to work on it literally an eternity.
Instead, we just define the kind of objects we want to work with.
However, intension bears the risk
of introducing some mind-boggling complications,
one of which is infinite sets.
For the time being, 
we will steer clear of any of these complications.
We have sufficient work with the kind of math
that comes without fierce creatures like infinity.

Sets, as you have already seen, are written in braces
like, for instance:
$\lbrace 1,2,3\rbrace$.
The members of a set, here the numbers 1, 2 and 3,
are called elements of this set,
it holds true, for example, that
$1 \in \lbrace 1,2,3\rbrace$ and 
$0 \not\in \lbrace 1,2,3\rbrace$.

A similar relation is \term{subset}.
A set $A$ is subset of another set $B$, iff
all elements of $A$ are also in $B$:
$\lbrace 1\rbrace \subseteq \lbrace 1,2,3\rbrace$,
$\lbrace 1,3\rbrace \subseteq \lbrace 1,2,3\rbrace$
and also
$\lbrace 1,2,3\rbrace \subseteq \lbrace 1,2,3\rbrace$.
The last case is interesting,
because it asserts that every set is subset of itself.
To exclude this case and only talk about subsets
that are smaller than the set in question,
we refer to the \term{proper} or \term{strict subset},
denoted as $A \subset B$.
An important detail of the subset relation is
that there is one special set that is subset of any set,
\viz\ the \term{empty set} $\lbrace\rbrace$
that does not contain any element and which is 
often denoted as $\varnothing$,

As you can see in the example $\lbrace 1,2,3\rbrace$,
a set may have many subsets.
The set of all possible subsets of a set
is called the \term{powerset} of this set,
often written $P(S)$ for a set $S$.
The powerset of $\lbrace 1,2,3\rbrace$, for example, is:
$\lbrace
\varnothing,
\lbrace 1\rbrace,
\lbrace 2\rbrace,
\lbrace 3\rbrace,
\lbrace 1,2\rbrace,
\lbrace 1,3\rbrace,
\lbrace 2,3\rbrace,
\lbrace 1,2,3\rbrace\rbrace$.

Does this remind you of something?
Perhaps not yet.
What if I was to ask: 
how many elements are there 
in the powerset of a set with $n$ elements?
Well, there is the empty set,
the set itself,
then sets with one element,
sets with two elements
and so on.
How many sets with $k$ elements
are there in the powerset of a set with $n$ elements?
The answer is: 
there are as many sets of $k$ elements
as there are ways to select $k$ items out of $n$.
In other words, 
the size of the powerset equals 
the sum of all binomial coefficients 
$\binom{n}{k}$ for one specific value of $n$,
\ie\ the sum of all values 
in one row of Pascal's Triangle.
For $n=0$, $n$ is the number of elements of the set,
we have: $\binom{0}{0} = 1$,
since the only subset of $\varnothing$ is $\varnothing$.
For $n=1$, we have: $\binom{1}{0} + \binom{1}{1} = 2$.
For $n=2$, we have:
$\binom{2}{0} + \binom{2}{1} + \binom{2}{2}$,
which is $1 + 2 + 1 = 4$.
For $n=3$, we have:
$\binom{3}{0} + \binom{3}{1} + \binom{3}{2} + \binom{3}{3}$,
which is $1 + 3 + 3 + 1 = 8$,
for $n=4$, we have:
$\binom{4}{0} + \binom{4}{1} + \binom{4}{2} + \binom{4}{3} + \binom{4}{4}$,
which is $1 + 4 + 6 + 4 + 1 = 16$.

Probably, you already see  the pattern.
For 5 elements, there are 32 possible subsets;
for 6 elements, there are 64 subsets,
for 7, there are 128 and for 8 there are 256 subsets.
In general,
for a set with $n$ elements,
there are $2^n$ subsets and,
as you may confirm in the Triangle in the previous section,
the sum of all binomial coefficients in one row of the Triangle
is also $2^n$.
This, in its turn, implies that the sum
of the coefficients in an expression of the form 
$a^n + \binom{n}{1}a^{n-1}b + \dots + \binom{n}{n-1}ab^{n-1} + b^n$,
as well, is $2^n$.

Is there a good algorithm 
to construct the powerset of a given set?
There are in fact many ways to build the powerset,
some more efficient or more elegant than others,
but really \emph{good} in the sense
that it efficiently creates powersets
of arbitrarily large sets
is none of them.
The size of the powerset 
increases exponentially in the size
of the input set, which basically means
that it is not feasible at all to 
create the powerset in most cases.
The powerset of a set of 10 elements,
for instance, has \num{1024} elements.
That of a set of 15 elements
has already \num{32768}
and a set of 20 elements has more than a million.

Here is a Haskell implementation
of a quite elegant and simple algorithm:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{ps}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{ps}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{ps}\;(\Varid{x}\mathbin{:}\Varid{xs})\mathrel{=}\Varid{ps}\;\Varid{xs}\plus \Varid{map}\;(\Varid{x}\mathbin{:})\;(\Varid{ps}\;\Varid{xs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Note that we use lists instead of sets.
There is a set module in Haskell,
but since we will not work too much
with sets, we stick to lists
with the convention
that there should be no duplicates
in lists that represent sets.

Let us see
how the $ps$ function works for the 
input $\lbrace 1,2,3\rbrace$.
We start with \ensuremath{\Varid{ps}\;(\mathrm{1}\mathbin{:}[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu])}
and immediately continue with \ensuremath{\Varid{ps}\;(\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{3}\mskip1.5mu])}
and, in the next round, with \ensuremath{\Varid{ps}\;(\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu])},
which then leads to the base case 
\ensuremath{\Varid{ps}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]}.
On the way back,
we then have \ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]\plus \Varid{map}\;(\mathrm{3}\mathbin{:})\;[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]},
which leads to the result \ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu]\mskip1.5mu]}.
This, one step further back,
leads to 
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu]\mskip1.5mu]\plus \Varid{map}\;(\mathrm{2}\mathbin{:})\;[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu]\mskip1.5mu]},
which results in
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]\mskip1.5mu]}.
In the previous step, we then have:
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]\mskip1.5mu]\plus \Varid{map}\;(\mathrm{1}\mathbin{:})\;[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu]\mskip1.5mu]},
resulting in\\
\ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]\mskip1.5mu]}
and we are done.

A completely different approach
is based on the observation
that there are $2^n$ possible subsets
of a set with $n$ elements
and this happens to be the number
of values one can represent
with a binary number of length $n$,
namely the values $0 \dots n - 1$.
Binary numbers, which we will discuss in more detail later,
use only the digits 0 and 1
instead of the digits $0\dots 9$
as we do with decimal numbers.
In binary numbers we would count like

\begin{center}
\begin{tabular}{r|r}
binary & decimal \\\hline\hline
0 & 0 \\
1 & 1 \\\hline
10 & 2 \\
11 & 3 \\\hline
100 & 4 \\
101 & 5 \\
110 & 6 \\
111 & 7 \\\hline
1000 & 8 \\
1001 & 9 \\
1010 & 10 \\
1011 & 11 \\
1100 & 12 \\
1101 & 13 \\
1110 & 14 \\
1111 & 15
\end{tabular} 
\end{center}

Indeed, there are 10 kinds of people in the world:
those who understand binary numbers
and those who do not.

To construct the powerset of a set of $n$ elements,
we can use binary numbers with $n$ digits.
We would loop over this numbers starting from 0
and move up to the greatest number representable
with $n$ digits. For $n = 3$, we would loop through:
$000$, $001$, $010$, $011$, $100$, $101$, $110$, $111$.
Each of these numbers describes one subset of the input set,
such that the $k^{th}$ digit of each number would tell us,
whether the $k^{th}$ element of the input set
is part of the current subset.
The number $000$ would indicate the empty set.
The number $001$ would indicate 
that the first element is in the set: $\lbrace 1\rbrace$.
The number $010$ would indicate
that the second element is in the set: $\lbrace 2\rbrace$.
The number $011$ would indicate
that the first and the second element 
are in the set: $\lbrace 1,2\rbrace$
and so on.

An important issue to gain any speed advantage
by this scheme is how to map the binary numbers
to elements in the input set.
We could na\"ively use an underlying representation
of binary numbers like lists -- 
as we have done for our natural numbers --
iterate through these lists and,
every time we find a 1,
add the corresponding element of the input set
to the current subset.
But this would mean that we had to loop
through $2^n$ lists of length $n$.
That does not sound very efficient.

The key is to realise
that we are talking about numbers.
We do not need to represent binary numbers
as lists at all.
Instead, we can just use decimal numbers
and extract the positions
where, in the binary representation of each number,
there is a 1. 

To illustrate this,
remember that the value of a decimal number
is computed as a sum of powers of 10:
$1024 = 1 \times 10^3 + 0 \times 10^2 + 2 \times 10^1 + 4 \times 10^0$.
The representation of \num{1024} as powers of two
is of course much simpler: 
$1024 = 1 \times 2^{10} + 
        0 \times 2^9 + 0 \times 2^8 + \dots + 0 \times 2^0$
or, for short:
$1024 = 2^{10}$.
Let us look at a number
with a simple decimal representation like \num{1000},
which, in powers of 10, is simply:
$10^3$. Represented as powers of two, however:
$2^9 + 2^8 + 2^7 + 2^6 + 2^5 + 2^3$,
which is 
$512 + 256 + 128 + 64 + 32 + 8 = 1000$.

The point is that the exponents
of \num{1000} represented as powers of two
indicate where the binary representation of \num{1000}
has a 1. \num{1000} in the binary system, indeed, is:
$1111101000$, whereas \num{1024} is
$10000000000$.
Let us index these numbers, first \num{1000}:

\begin{tabular}{ r r r r r r r r r r r}
10 & 9 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1 & 0\\\hline
 0 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0
\end{tabular}

and \num{1024}:

\begin{tabular}{ r r r r r r r r r r r}
10 & 9 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1 & 0\\\hline
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{tabular}

You see that the indexes in the first row
that have a 1 in the second row
correspond to the exponents of the powers
of two that sum up to the respective number,
\ie\ 9, 8, 7, 6, 5 and 3 for \num{1000}
and 10 for \num{1024}.
We can, hence, interpret the exponents of 
the powers of two as indexes into the set
for which we want to construct the powerset.
Think of the input set as an array
in a language like $C$,
where we can refer to an element of the set
directly by addressing the memory cell
where is resides: 
\haskell{x = set[0];} for instance,
would give us the first element of the set.

When we look at how a number, say $d$, is computed
as sum of powers of two,
we can derive the following algorithm:
compute the greatest power of two
that is smaller than or equal to $d$ 
and than do the same with the difference
between $d$ and this power of two
until the difference,
which in analogy to division,
we may call the remainder,
is zero.
The greatest power of two $\le d$ 
is just the log base 2
of this number rounded down
to the next natural number.
A function implementing this in Haskell
would be (cheating on our number type 
by using floating point numbers):

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{42}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{natLog}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to (\Conid{Natural},\Conid{Natural}){}\<[E]%
\\
\>[3]{}\Varid{natLog}\;\Varid{b}\;\Varid{n}\mathrel{=}{}\<[17]%
\>[17]{}\mathbf{let}\;\Varid{e}\mathrel{=}\Varid{floor}\mathbin{\$}\Varid{logBase}\;{}\<[42]%
\>[42]{}(\Varid{fromIntegral}\;\Varid{b})\;{}\<[E]%
\\
\>[42]{}(\Varid{fromIntegral}\;\Varid{n}){}\<[E]%
\\
\>[17]{}\hsindent{1}{}\<[18]%
\>[18]{}\mathbf{in}\;(\Varid{e},\Varid{n}\mathbin{-}\Varid{b}\mathbin{\uparrow}\Varid{e}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This function takes a natural number $b$,
the base, and a natural number $n$.
The result consists of 
two numbers $(e,r)$ that
shall fulfil the condition $n = b^e + r$.

We can use this function 
to obtain all exponents of the powers of two
that sum up to a given number:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{binExp}\mathbin{::}\Conid{Natural}\to [\mskip1.5mu \Conid{Natural}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{binExp}\;\mathrm{0}{}\<[13]%
\>[13]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{binExp}\;\mathrm{1}{}\<[13]%
\>[13]{}\mathrel{=}[\mskip1.5mu \mathrm{0}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{binExp}\;\Varid{n}{}\<[13]%
\>[13]{}\mathrel{=}\mathbf{let}\;(\Varid{e},\Varid{r})\mathrel{=}\Varid{natLog}\;\mathrm{2}\;\Varid{n}\;\mathbf{in}\;\Varid{e}\mathbin{:}\Varid{binExp}\;\Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

That is, for input 0 and 1,
we explicitly define the results \ensuremath{[\mskip1.5mu \mskip1.5mu]}
and \ensuremath{[\mskip1.5mu \mathrm{0}\mskip1.5mu]}.
Here, \ensuremath{[\mskip1.5mu \mskip1.5mu]} means that
there is no 1 in the binary representation
and \ensuremath{[\mskip1.5mu \mathrm{0}\mskip1.5mu]} means that
there is 1 at the first position (indexed by 0).
For any other number $n$,
we calculate the exponent $e$ 
of the greatest power of two $\le n$
and the remainder $r$ and add $e$
to the list that will result from
applying $binExp$ to $r$.
Let us look at the example \num{1000}.
We start with \ensuremath{\Varid{natLog}\;\mathrm{2}\;\mathrm{1000}}:

\ensuremath{\Varid{binExp}\;\mathrm{1000}} = $(9, 1000 - 2^9 = 1000 - 512 = 488)$\\
\ensuremath{\mathrm{9}\mathbin{:}\Varid{binExp}\;\mathrm{488}} = $(8, 488 - 2^8 = 488 - 256 = 232)$\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\Varid{binExp}\;\mathrm{232}} = $(7, 232 - 2^7 = 232 - 128 = 104)$\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\mathrm{7}\mathbin{:}\Varid{binExp}\;\mathrm{104}} $ = (6, 104 - 2^6 = 104 - 64 = 40)$\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\mathrm{7}\mathbin{:}\mathrm{6}\mathbin{:}\Varid{binExp}\;\mathrm{40}} $ = (5, 40 - 2^5 = 40 - 32 = 8)$\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\mathrm{7}\mathbin{:}\mathrm{6}\mathbin{:}\mathrm{5}\mathbin{:}\Varid{binExp}\;\mathrm{8}} $ = (3, 8 - 2^3 = 8 - 8 = 0)$\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\mathrm{7}\mathbin{:}\mathrm{6}\mathbin{:}\mathrm{5}\mathbin{:}\mathrm{3}\mathbin{:}\Varid{binExp}\;\mathrm{0}\mathrel{=}[\mskip1.5mu \mskip1.5mu]}\\
\ensuremath{\mathrm{9}\mathbin{:}\mathrm{8}\mathbin{:}\mathrm{7}\mathbin{:}\mathrm{6}\mathbin{:}\mathrm{5}\mathbin{:}\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu]},

which, indeed, is the list of the exponents
of the powers of two that add up to \num{1000}.

Now we need a function
that loops through all numbers $0 \dots 2^n-1$,
calculates the exponents of the powers of two for each number
and then retrieves the elements in the input set
that corresponds to the exponents:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}c<{\hspost}@{}}%
\column{34E}{@{}l@{}}%
\column{37}{@{}>{\hspre}l<{\hspost}@{}}%
\column{41}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{ps2}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{ps2}\;[\mskip1.5mu \mskip1.5mu]{}\<[11]%
\>[11]{}\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{ps2}\;\Varid{xs}{}\<[11]%
\>[11]{}\mathrel{=}\Varid{go}\;(\mathrm{2}\mathbin{\uparrow}(\Varid{length}\;\Varid{xs})\mathbin{-}\mathrm{1})\;\mathrm{0}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\Varid{n}\;\Varid{i}{}\<[20]%
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{i}\equiv \Varid{n}{}\<[34]%
\>[34]{}\mathrel{=}{}\<[34E]%
\>[37]{}[\mskip1.5mu \Varid{xs}\mskip1.5mu]{}\<[E]%
\\
\>[20]{}\mid {}\<[20E]%
\>[23]{}\Varid{otherwise}{}\<[34]%
\>[34]{}\mathrel{=}{}\<[34E]%
\>[37]{}\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{map}\;\Varid{exp2idx}\mathbin{\$}\Varid{binExp}\;\Varid{i}{}\<[E]%
\\
\>[37]{}\mathbf{in}\;{}\<[41]%
\>[41]{}\Varid{s}\mathbin{:}\Varid{go}\;\Varid{n}\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]%
\\
\>[12]{}\Varid{exp2idx}\;\Varid{x}\mathrel{=}\Varid{xs}\mathbin{!!}(\Varid{fromIntegral}\;\Varid{x}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The function $ps2$ returns just a set that contains the empty set
when called with the empty set.
Otherwise, it enters a loop with two parameters:
$2^{(length~xs)}-1$, which is the greatest number 
that can be represented with a binary number with $n$ digits,
when we start to count at 0,
and the number we start with, namely 0.
For each number $i$:
if we have reached the last number, we just know
the corresponding subset is the input set itself.
Otherwise, we map the function $exponent \rightarrow index$
to the result of $binExp$ applied to the current number $i$.
The mapping function, $exp2idx$, 
uses the list index operator \haskell{!!} 
to get the element of the input list $xs$
at the position $x$, which is just an exponent.
(Note that we have to convert x from $Natural$ to $Int$,
since \haskell{!!} expects an $Int$ value.)

This algorithm exploits a fascinating
\term{isomorphism} -- an analogous structure --
between binary numbers and powersets.
With an appropriate data structure
to represent sets, like \ensuremath{\Conid{Vector}},
and, of course, a more efficient number representation
than our humble natural numbers,
the algorithm definitely beats the one
we implemented as $ps$.
Furthermore, this algorithm can be paralellised
according to number ranges, which is not possible
with the previous algorithm, since, there, results
depend on inermediate results, such that
each step builds on a predecessor.

Unfortunately, lists show very bad performance
with random access such as indexing.
Therefore, $ps2$ is slower than $ps$.
But using Haskell vectors (implemented in module $Data.Vector$)
and Integers instead of our $Natural$,
$ps2$ is indeed faster.
The changes, by the way, are minimal.
Just compare the implementation of $ps2$ and $psv$:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}c<{\hspost}@{}}%
\column{22E}{@{}l@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{36}{@{}>{\hspre}c<{\hspost}@{}}%
\column{36E}{@{}l@{}}%
\column{39}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{psv}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{psv}\;[\mskip1.5mu \mskip1.5mu]{}\<[11]%
\>[11]{}\mathrel{=}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{psv}\;\Varid{xs}{}\<[11]%
\>[11]{}\mathrel{=}{}\<[14]%
\>[14]{}\mathbf{let}\;\Varid{v}\mathrel{=}\Varid{\Conid{V}.fromList}\;\Varid{xs}{}\<[E]%
\\
\>[14]{}\mathbf{in}\;\Varid{go}\;\Varid{v}\;(\mathrm{2}\mathbin{\uparrow}(\Varid{length}\;\Varid{xs})\mathbin{-}\mathrm{1})\;\mathrm{0}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\Varid{v}\;\Varid{n}\;\Varid{i}{}\<[22]%
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{i}\equiv \Varid{n}{}\<[36]%
\>[36]{}\mathrel{=}{}\<[36E]%
\>[39]{}[\mskip1.5mu \Varid{xs}\mskip1.5mu]{}\<[E]%
\\
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{otherwise}{}\<[36]%
\>[36]{}\mathrel{=}{}\<[36E]%
\>[39]{}\mathbf{let}\;\Varid{s}\mathrel{=}\Varid{map}\;(\Varid{exp2idx}\;\Varid{v})\;(\Varid{binExp}\;\Varid{i}){}\<[E]%
\\
\>[39]{}\mathbf{in}\;{}\<[43]%
\>[43]{}\Varid{s}\mathbin{:}\Varid{go}\;\Varid{v}\;\Varid{n}\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]%
\\
\>[12]{}\Varid{exp2idx}\;\Varid{v}\;\Varid{x}\mathrel{=}\Varid{v}\mathbin{!}(\Varid{fromIntegral}\;\Varid{x}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The changes to the code of $ps2$ relate to the introduction
of $v$, a vector created from $xs$ 
by using the $fromList$ function from the vector module,
which is qualified as $V$.

In practical terms, however,
the performance of the powerset function
does not matter too much,
since, as already said, it is not feasible
to compute the powerset of large sets anyway.
Nevertheless, problems related to subsets
are quite common.
An infamous example is the \term{set cover} problem.

The challenge in the set cover problem
is to combine given subsets of a set $A$
so that the combined subsets together 
equal $A$. This involves an operation
on sets we have not yet discussed.
Combining sets is formally called \term{union}:
$A \cup B$.
The union of two sets, $A$ and $B$,
contains all elements that are in $A$ or $B$
(or both),
for example:
$\lbrace 1,2,3\rbrace \cup \lbrace 3,4,5\rbrace = 
 \lbrace 1,2,3,4,5\rbrace$.

Two other important set operations
are intersection and difference.
The intersection of two sets $A$ and $B$,
$A \cap B$, contains all elements $x$,
such that $x \in A$ and $x \in B$.
To continue with the example used above:
$\lbrace 1,2,3\rbrace \cap \lbrace 3,4,5\rbrace
= \lbrace 3\rbrace$.
The intersection of the union of two sets
with one of these sets is just that set,
$(A \cup B) \cap A = A$:
$(\lbrace 1,2,3\rbrace \cup \lbrace 3,4,5\rbrace)
 \cap \lbrace 1,2,3\rbrace = 
 \lbrace 1,2,3,4,5\rbrace \cap \lbrace 1,2,3\rbrace =
 \lbrace 1,2,3\rbrace$.

The difference of two sets $A$ and $B$, $A \setminus B$,
contains all elements in $A$ that are not in $B$,
for example:
$\lbrace 1,2,3\rbrace \setminus \lbrace 3,4,5\rbrace = 
\lbrace 1,2\rbrace$.
If $B$ is a subset of $A$,
then the difference $A \setminus B$ is called
the \term{complement} of $B$ in $A$.

Now let us model the three set operations
union, intersection and difference with Haskell lists.
The simplest case is difference,
since, assuming that we always use lists
without duplicates,
we can just use the predefined list operator 
\textbackslash\textbackslash.
Union is not too difficult either
using the function $nub$,
which removes duplicates from a list:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{union}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{union}\;\Varid{a}\;\Varid{b}\mathrel{=}\Varid{nub}\;(\Varid{a}\plus \Varid{b}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Using $nub$ is necessary,
since merging the two lists
will introduce duplicates for any 
$x \in a$ and $x \in b$.

Intersect is slightly more difficult.
We could implement intersect by means
of $nub$; we used $nub$ in $union$
to remove the duplicates of exactly those elements
that we want to have in intersect.
The intersect, hence, could be implemented as
\ensuremath{\Varid{a}\plus \Varid{b}\mathbin{\char92 \char92 }\Varid{nub}\;(\Varid{a}\plus \Varid{b})}.
This would define the intersect as the difference
of the concatenation of two lists
and the union of these two lists.
Have a look at the example
$A=\lbrace 1,2,3\rbrace$ and
$B=\lbrace 3,4,5\rbrace$:

\ensuremath{\Conid{A}\plus \Conid{B}\mathbin{\char92 \char92 }\Varid{nub}\;(\Conid{A}\plus \Conid{B})\mathrel{=}}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]\plus [\mskip1.5mu \mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]\mathbin{\char92 \char92 }\Varid{nub}\;([\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]\plus [\mskip1.5mu \mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu])\mathrel{=}}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]\mathbin{\char92 \char92 }\Varid{nub}\;([\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu])\mathrel{=}}\\
\ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]\mathbin{\char92 \char92 }[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]\mathrel{=}}\\
\ensuremath{[\mskip1.5mu \mathrm{3}\mskip1.5mu]}.
         
This implementation, however,
is not very efficient.
Preferable is the following one:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}c<{\hspost}@{}}%
\column{19E}{@{}l@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}c<{\hspost}@{}}%
\column{24E}{@{}l@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{47}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{intersect}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{intersect}\;[\mskip1.5mu \mskip1.5mu]\;\anonymous {}\<[19]%
\>[19]{}\mathrel{=}{}\<[19E]%
\>[22]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{intersect}\;\anonymous \;[\mskip1.5mu \mskip1.5mu]{}\<[19]%
\>[19]{}\mathrel{=}{}\<[19E]%
\>[22]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{intersect}\;(\Varid{a}\mathbin{:}\Varid{as})\;\Varid{bs}{}\<[24]%
\>[24]{}\mid {}\<[24E]%
\>[27]{}\Varid{a}\in \Varid{bs}{}\<[40]%
\>[40]{}\mathrel{=}\Varid{a}\mathbin{:}{}\<[47]%
\>[47]{}\Varid{intersect}\;\Varid{as}\;\Varid{bs}{}\<[E]%
\\
\>[24]{}\mid {}\<[24E]%
\>[27]{}\Varid{otherwise}{}\<[40]%
\>[40]{}\mathrel{=}{}\<[47]%
\>[47]{}\Varid{intersect}\;\Varid{as}\;\Varid{bs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

We first define the intersection of the empty set
with any other set as the empty set.
(Note the similarity of the role of the $\varnothing$ 
 in union and intersect with that of $0$ in 
 addition and multiplication!)
For other cases, we start with the first element of the first list, $a$,
and check if it is also in $bs$;
if so, we add $a$ to the result set
and continue with $intersect$ on the tail of the first list;
otherwise, we continue
without adding anything
in this round. 

We now can state the set cover problem more formally:
We have a set $U$, called the \term{universe},
and a set $S = \lbrace s_1,s_2, \dots, s_n\rbrace$ 
of subsets of $U$, 
$s_1 \subseteq U, s_2 \subseteq U, \dots, s_n \subseteq U$,
such that the union of all the sets in $S$ equals $U$,
$s_1 \cup s_2 \cup \dots \cup s_n = U$.
What is the least expensive union of a subset of $S$
that yields $U$?

Least expensive may be interpreted in different ways.
In the pure mathematical sense,
it usually means the smallest number of sets,
but in real world problems,
least expensive may refer to lowest cost,
shortest time, fewest people involved, \etc\
The problem is in fact very common.
It comes up in scheduling problems
where the members of $S$
represent sets of threads assigned
to groups of processors;
very typical are problems of
team building where the sets in $S$
represent teams of people with complementing skills;
but there are also problems 
similar to the \term{travelling salesman}
problem where the sets in $S$ 
represent locations that must be visited 
during a round trip.

So, how many steps do we need to solve this problem?
To find the optimal solution,
we basically have to try out
all combinations of subsets in $S$.
For $S = \lbrace a,b,c\rbrace$,
$\lbrace a\rbrace$ may be the best solution,
$\lbrace b\rbrace$ may be,
$\lbrace c\rbrace$,
$\lbrace a,b\rbrace$,
$\lbrace a,c\rbrace$, 
$\lbrace b,c\rbrace$ and, of course,
$\lbrace a,b,c\rbrace$.
As you should see,
that are $2^n$ possibilities,
\ie\ the sum of all binomial coefficients $\binom{n}{k}$
where $n$ is the size of $S$.
That, as we know, is not feasible to compute
with large $S$'s.
There are, however, solutions for specific problems
using heuristics.

Heuristics are helpers in otherwise exponential
search problems.
In practice, heuristics may be derived from the concrete
problem domain.
With respect to the examples mentioned above,
it is often obvious that we do 
not want to combine threads on one processor
that better work in parallel;
concerning problems with teams,
we could exclude combinations of people
who do not like each other
or we may want to construct gender balanced teams. 
Such restrictions and insights
can be used to drastically reduce 
the number of possible solutions
and, thus, making computation feasible.
But think, for instance,
of a general purpose operating system
that does not have any previous knowledge
about the user tasks it should run.
No real-world heuristics are available
for the kernel to find an optimal balance.

There are purely mathematical
heuristics that may come to aid in cases
where the problem domain itself does not
offer reasonable simplifications.
For the set cover problem,
a known heuristic that reduces 
computational complexity significantly,
is to search for local optimums
instead of the global optimum.
That is, we do not try to find the 
solution that is the best compared
with all other solutions,
but, instead, we make optimal
decisions in each round.
For example, if we had the universe
$U = \lbrace 1,2,3,4,5,6\rbrace$
and $S = \lbrace 
\lbrace 1,2,3\rbrace,
\lbrace 1,2,4\rbrace,
\lbrace 1,4\rbrace,
\lbrace 3,5\rbrace,
\lbrace 1,6\rbrace\rbrace$,
the optimal solution would be
$\lbrace
\lbrace 1, 2, 4\rbrace, 
\lbrace 3, 5\rbrace,\\ 
\lbrace 1, 6\rbrace\rbrace$.
The key to find this solution
is to realise that 
the second set in $S$,
$\lbrace 1,2,4\rbrace$,
is the better choice compared to the first set
$\lbrace 1,2,3\rbrace$.
But to actually realise that,
we have to try all possible combinations of sets,
which are $2^n$ and, hence, too many.
An algorithm
that does not go for the global optimum,
but for local optimums,
would just take the first set,
because, in the moment of the decision,
it is one of two equally good options
and there is nothing that would hint to the fact
that, with the second set, 
the overall outcome would be better.
This \term{greedy} algorithm will
consequently find only a suboptimal solution,
\ie\:
$\lbrace 1,2,3\rbrace$,
$\lbrace 1,4\rbrace$ or even $\lbrace 1,2,4\rbrace$,
$\lbrace 3,5\rbrace$ and
$\lbrace 1,6\rbrace$.
It, hence, needs one set more
than the global optimum.

In many cases,
local optimums are sufficient
and feasible to compute.
This should be motivation enough
to try to implement a greedy solution
for the set cover problem.
The algorithm will in each step
take the set that brings the greatest
reduction in the distance between the current
state and the universe.
We, first, need some way to express this distance
and an obvious notion for distance
is just the size of the difference
between the universe and another set:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{dist}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{Int}{}\<[E]%
\\
\>[3]{}\Varid{dist}\;\Varid{a}\;\Varid{b}\mathrel{=}\Varid{length}\;(\Varid{a}\mathbin{\char92 \char92 }\Varid{b}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Now, we need a function, say, $best$
that uses $dist$ to find the set in $S$
with the least distance to the universe
and another function that 
repeatedly finds the local minimum
using $best$, until either
all sets in $S$ have been used
or no set in $S$ is able to reduce
the distance to the universe anymore.
Here are these functions:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}c<{\hspost}@{}}%
\column{31E}{@{}l@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{48}{@{}>{\hspre}l<{\hspost}@{}}%
\column{54}{@{}>{\hspre}l<{\hspost}@{}}%
\column{63}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{greedySetCover}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{greedySetCover}\;\Varid{u}\;\Varid{s}\mathrel{=}\Varid{loop}\;(\Varid{length}\;\Varid{u})\;[\mskip1.5mu \mskip1.5mu]\;\Varid{s}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{loop}\;\anonymous \;\anonymous \;{}\<[22]%
\>[22]{}[\mskip1.5mu \mskip1.5mu]{}\<[26]%
\>[26]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{loop}\;\Varid{m}\;\Varid{rs}\;\Varid{xs}{}\<[26]%
\>[26]{}\mathrel{=}{}\<[29]%
\>[29]{}\mathbf{let}\;(\Varid{m'},\Varid{p})\mathrel{=}\Varid{best}\;\Varid{m}\;\Varid{rs}\;[\mskip1.5mu \mskip1.5mu]\;\Varid{xs}{}\<[E]%
\\
\>[29]{}\mathbf{in}\;\mathbf{if}\;\Varid{m'}\mathbin{<}\Varid{m}{}\<[E]%
\\
\>[29]{}\hsindent{5}{}\<[34]%
\>[34]{}\mathbf{then}\;\Varid{p}\mathbin{:}\Varid{loop}\;\Varid{m'}\;(\Varid{p}\mathbin{`\Varid{union}`}\Varid{rs})\;(\Varid{delete}\;\Varid{p}\;\Varid{xs}){}\<[E]%
\\
\>[29]{}\hsindent{5}{}\<[34]%
\>[34]{}\mathbf{else}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{best}\;\Varid{m}\;\anonymous \;\Varid{p}\;[\mskip1.5mu \mskip1.5mu]{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}(\Varid{m},\Varid{p}){}\<[E]%
\\
\>[12]{}\Varid{best}\;\Varid{m}\;\Varid{r}\;\Varid{p}\;(\Varid{x}\mathbin{:}\Varid{xs}){}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}\mathbf{let}\;\Varid{m'}\mathrel{=}\Varid{dist}\;\Varid{u}\;(\Varid{x}\mathbin{`\Varid{union}`}\Varid{r}){}\<[E]%
\\
\>[34]{}\mathbf{in}\;\mathbf{if}\;\Varid{m'}\mathbin{<}\Varid{m}\;{}\<[48]%
\>[48]{}\mathbf{then}\;{}\<[54]%
\>[54]{}\Varid{best}\;\Varid{m'}\;{}\<[63]%
\>[63]{}\Varid{r}\;\Varid{x}\;\Varid{xs}{}\<[E]%
\\
\>[48]{}\mathbf{else}\;{}\<[54]%
\>[54]{}\Varid{best}\;\Varid{m}\;{}\<[63]%
\>[63]{}\Varid{r}\;\Varid{p}\;\Varid{xs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The measure for the current optimum is 
the variable $m$ used in $loop$ and $best$.
The whole algorithm starts with $m = length(u)$,
which is the worst possible distance,
\viz\ the distance between $\varnothing$ and the universe.

The second parameter passed to $loop$, $rs$,
is the union of partial results.
It is initially empty.
The third parameter is the set of subsets
we are working on
starting with $S$.
With an empty $S$, $loop$
is just $\varnothing$.
Otherwise, it uses $best$
to get the local optimum,
which is the tuple $(m',p)$,
where $p$ is the best choice for the local optimum
and $m'$ the distance of this set to the universe.
If $m' < m$,
we actually have found a solution 
that improves on the current state
and we continue adding $p$ to the result set,
which results from the recursion of $loop$
with $m'$ as current optimum,
the union of $p$ and the partial result $rs$
and the current instance of $S$ without $p$.
Otherwise, the result is just the empty set.

The function $best$ simply goes through
all elements of the current instance of $S$.
If $best$ arrives at the end of the list,
it just returns the previously identified optimum $(m,p)$.
Otherwise, for each element of the current set of subsets,
it computes the distance and,
should the current distance improve on the result,
continues with this current optimum,
if it does not, it continues with the old parameters.

The fact that we do not go back
in the $loop$ function to test other options,
but always stick with a solution once it was found
makes this algorithm \term{greedy}:
It takes the money and runs.
What is the speed-up
we obtain with this apparently ugly strategy?
One call of $best$ passes through the whole list,
which, initially, is $S$.
$loop$, if $best$ has found an optimum
that improves on the old result,
removes the corresponding element from the list
and repeates the process.
This time, $best$ will go through a list of $n-1$ elements,
where $n$ is the size of $S$.
If it finds a new mimimum again,
the corresponding element is removed,
and we get a list of $n-2$ elements.
The process repeats, until $best$ does not find
a new optimum anymore.
In the worst case, this is only after all elements
in the list have been consumed.
The maximum number of steps
that must be processed, hence,
is $n + n - 1 + n - 2 + \dots + 1$
or simply the series $\sum_{k=1}^{n}{k}$,
which, as we already know as the Little Gauss,
is $\frac{n^2 + n}{2}$.
For a set $S$ with 100 elements,
we would need to consider $2^{100}$ possible cases
to compute the global optimum, 
which is \num{1267650600228229401496703205376}.
With the local optimum,
we can reduce this number 
to $\frac{100 \times 101}{2} = 5050$ steps.
For some cases,
the local minimum is therefore the preferred solution.

\section{Stirling Numbers}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Stirling}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{\Conid{Data}.List}\;(\Varid{nub}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Fact}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Types}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

We saw that the number of all permutations of a set 
equals the factorial of the number of elements in that set.
We also looked at the cycle notation where permutations
are encoded as results of subsequent
applications of this permutation; $(1~2~5)(3~4)$, for instance,
applied once to the sequence $(1~2~3~4~5)$, would yield $(2~5~4~3~1)$.
It is now quite natural to ask -- at least for a mathematician --
how many permutations there are for a given number of orbits
in cycle notation.

To answer this question,
we first have to know how many different combinations 
of a given number of orbits we actually may have.
The orbits in cycle notation, in fact, are \term{partitions}
of a set. Partitions are non-empty, distinct subsets.
The union of the partitions of a 
complete partitioning of a set is just the original set.
The orbits of the permutation given above, for instance,
$(1~2~5)(3~4)$ can be seen as subsets that,
obviously, are not empty and, 
since they have no element in common,
are distinct.
Their union $\lbrace 1,2,5\rbrace \cup \lbrace 3,4\rbrace$,
as you can easily verify, equals
the original set $\lbrace 1,2,3,4,5\rbrace$.

We could, hence, think in the lines of the powerset
to generate partitions. We just leave out the empty set
and, eventually, pick only groups of sets that, together,
add up to the whole set.
It is in fact somewhat more complicated.
To illustrate that let us look at an
algorithm that generates all possibilities to
partition a set into two partitions:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}c<{\hspost}@{}}%
\column{18E}{@{}l@{}}%
\column{19}{@{}>{\hspre}c<{\hspost}@{}}%
\column{19E}{@{}l@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{twoPartitions}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu ([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{a}\mskip1.5mu])\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{twoPartitions}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\Varid{fltr}\mathbin{\circ}\Varid{p2}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{p2}\;[\mskip1.5mu \mskip1.5mu]{}\<[23]%
\>[23]{}\mathrel{=}{}\<[29]%
\>[29]{}[\mskip1.5mu ([\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu])\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{p2}\;(\Varid{x}\mathbin{:}\Varid{xs}){}\<[23]%
\>[23]{}\mathrel{=}{}\<[29]%
\>[29]{}[\mskip1.5mu (\Varid{x}\mathbin{:}\Varid{a},\Varid{b})\mid (\Varid{a},\Varid{b})\leftarrow \Varid{p2}\;\Varid{xs}\mskip1.5mu]\plus {}\<[E]%
\\
\>[23]{}\Varid{fltr}\;{}\<[29]%
\>[29]{}[\mskip1.5mu (\Varid{a},\Varid{x}\mathbin{:}\Varid{b})\mid (\Varid{a},\Varid{b})\leftarrow \Varid{p2}\;\Varid{xs}\mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{fltr}{}\<[19]%
\>[19]{}\mathrel{=}{}\<[19E]%
\>[22]{}\Varid{filter}\;(\lambda \Varid{p}\to \neg \;({}\<[43]%
\>[43]{}\Varid{null}\;(\Varid{fst}\;\Varid{p})\mathrel{\vee}{}\<[E]%
\\
\>[43]{}\Varid{null}\;(\Varid{snd}\;\Varid{p}))){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This innocent looking lines of code are quite tricky.
The basic idea is implemented in \ensuremath{\Varid{p2}}:
For an empty set, \ensuremath{\Varid{p2}} returns a pair of empty sets.
For any other set, it applies \ensuremath{\Varid{p2}} twice on the \ensuremath{\Varid{tail}} of the list
and adds the \ensuremath{\Varid{head}} once to the first of the pair
and once to the second of the pair.
This sounds easy, but there is an issue:
The intermediate result sets will contain empty sets.
In the first result, the second set is empty and,
in the second result, the first one is empty.
To solve this problem, we explicitly filter empty sets out
(using \ensuremath{\Varid{fltr}}).
If we applied the filter once on the overall result,
we would get the following pairs
for the set $\lbrace 1,2,3\rbrace$: 

\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu])}.

All results are correct,
but there are too many of them.
More specifically, some of the results are repeated.
\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu])} and \ensuremath{([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu])} are different tuples of course,
but they describe the same partitioning
consisting of the subsets 
$\lbrace 1,2\rbrace$ and $\lbrace 3\rbrace$.

In the code above, this issue is solved
by applying the filter once again, \viz\
on the second intermediate result.
Let us look at how the second list comprehension develops.
After the first application of \ensuremath{\Varid{p2}},
we have a tuple of two empty sets:

\ensuremath{\Varid{p2}\;(\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu])\mathrel{=}[\mskip1.5mu (\Varid{a},\mathrm{3}\mathbin{:}\Varid{b})\mid (\Varid{a},\Varid{b})\leftarrow [\mskip1.5mu ([\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu])\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{p2}\;(\mathrm{3}\mathbin{:}[\mskip1.5mu \mskip1.5mu])\mathrel{=}[\mskip1.5mu ([\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu])\mskip1.5mu]}.

This result is filtered out, because the first set is empty.
In the next round we consequently have, as input, 
only the result of the first, unfiltered, list comprehension:

\ensuremath{\Varid{p2}\;(\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{3}\mskip1.5mu])\mathrel{=}[\mskip1.5mu (\Varid{a},\mathrm{2}\mathbin{:}\Varid{b})\mid (\Varid{a},\Varid{b})\leftarrow [\mskip1.5mu ([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mskip1.5mu])\mskip1.5mu]\mskip1.5mu]}\\
\ensuremath{\Varid{p2}\;(\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{3}\mskip1.5mu])\mathrel{=}[\mskip1.5mu ([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu])\mskip1.5mu]},\\

which is preserved.
We then get to the final round
where the input is now the result of the first comprehension
plus the one created above:

\ensuremath{\Varid{p2}\;(\mathrm{1}\mathbin{:}[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu])\mathrel{=}[\mskip1.5mu (\Varid{a},\mathrm{1}\mathbin{:}\Varid{b})\mid (\Varid{a},\Varid{b})\leftarrow [\mskip1.5mu ([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu]),([\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mskip1.5mu])\mskip1.5mu]}\\
\ensuremath{\Varid{p2}\;(\mathrm{1}\mathbin{:}[\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu])\mathrel{=}[\mskip1.5mu ([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu]),([\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu])\mskip1.5mu]}.

The result of the first comprehension in the last round consists of the pairs:
\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu])}, which results from the input \ensuremath{([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu])}, and
\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mskip1.5mu])}, which is removed by the filter on the final result of \ensuremath{\Varid{p2}}.
This gives the correct result: 

\ensuremath{([\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{2},\mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu])}\\
\ensuremath{([\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1},\mathrm{2}\mskip1.5mu])}.

As long as we create only two partitions,
we can use pairs and the 
nice list comprehension to make the code clear.
For the generation of $k$ partitions,
the code becomes somewhat more obscure.
It is in particulare not sufficient anymore
to call the filter twice.
Instead, we need an additional function
that removes the permutations of sets of partitions.
To partition the set $\lbrace 1,2,3,4\rbrace$
into three partitions, for example,
we need to remove all but one of

$\lbrace 1,2\rbrace, \lbrace 3\rbrace, \lbrace 4\rbrace$,\\ 
$\lbrace 1,2\rbrace, \lbrace 4\rbrace, \lbrace 3\rbrace$,\\
$\lbrace 4\rbrace, \lbrace 1,2\rbrace, \lbrace 3\rbrace$,\\
$\lbrace 3\rbrace, \lbrace 1,2\rbrace, \lbrace 4\rbrace$,\\
$\lbrace 4\rbrace, \lbrace 3\rbrace, \lbrace 1,2\rbrace$ and\\
$\lbrace 3\rbrace, \lbrace 4\rbrace, \lbrace 1,2\rbrace$.

We will not develop this algorithm here,
because it is highly inefficient.
We would create much more sets than necessary
and, then, we still have to generate permutations
to remove those sets that are superfluous.
Fortunately, there is an alternative
very similar to that we have used to create powersets.
For powersets, we used all binary numbers from 0 to $2^n-1$,
where $n$ is the number of elements in the set.
To generate partitions, we have to modify this idea
in two respects.

First, we do not have the simple decision
whether an element is in the current subset or not,
but in which of $k$ partitions it is.
To get this information, we need a number system
with the base $k$.
For two partitions, this is just the binary number system.
For three partitions, it would be a number system with base 3.
For four partitions, we would use a number system with base 4 
and so on.

Second, we have to restrict the numbers in a way
that the result set points only to distinct partitionings.
Imagine we want to know how to partition 
the set $\lbrace 1,2,3,4,5\rbrace$ into three subsets.
We would then need numbers of the form:
$01200$ or $01002$.
The first number, $01200$, would point to the partitions
$\lbrace 1,4,5\rbrace, \lbrace 2\rbrace, \lbrace 3\rbrace$ and
the second number, $01002$, would point to the partitions
$\lbrace 1,3,4\rbrace, \lbrace 2\rbrace, \lbrace 5\rbrace$.
In other words, the digits of the number indicate
the partition in which the element at this position
(in the original sequence) would be starting to count at
index 0 for the first partition.
Obviously, the number $01000$ would be no good,
because it does not describe a set of three partitions,
but only one of two partitions.
Also, with number $00012$ already in the result,
we do not want to generate the number $22210$,
because the corresponding sequences of partitions
$\lbrace 1,2,3\rbrace, \lbrace 4\rbrace, \lbrace 5\rbrace$ and
$\lbrace 5\rbrace, \lbrace 4\rbrace, \lbrace 1,2,3\rbrace$ 
are permutations of each other and, thus, describe 
the same set of partitions.

There is a simple trick to avoid such duplications
and this trick has a name: \term{Restricted Growth String}s,
\acronym{rgs} for short.
\acronym{rgs} are similar to numbers, but have leading zeros --
they are therefore strings rather than proper numbers.
The length of \acronym{rgs} depends on the purpose
for which they are used. In our case,
we want their length to equal the number of elements
in the original set.

When counted up, \acronym{rgs} grow in an ordered fashion,
such that lesser digits appear before greater ones.
For instance, we allow numbers like $0012$ and $0102$,
but do not allow such like $0201$ or $1002$.
This implies that each new digit is at most one greater
than the greatest digit already in the number, \ie\
$a_i \le 1 + \max{a_1,a_2,...a_{i-1}}$.
This restriction rules out numbers with a combination of digits
that has already appeared with smaller \acronym{rgs} before.
Of the strings
$0123$, $0132$, $0213$, $0231$, $0312$ and $0321$
only the first is a valid \acronym{rgs}.
With the others, either 3 or 2 appear
before 1, violating the restriction that no digit
must be greater than the greatest number appeared so far
plus 1. You can easily verify that
all those strings point to the same partitioning
of set $\lbrace 1,2,3,4\rbrace$, namely permutations
of the set
$\lbrace\lbrace 1\rbrace,
        \lbrace 2\rbrace,
        \lbrace 3\rbrace,
        \lbrace 4\rbrace\rbrace$.

The ordered growth also implies 
that the first digit in an \acronym{rgs} is always 0.
Otherwise, if 0 did not appear in the string at all,
the first partition would be empty and
the partitioning would, hence, be invalid;
if 0 did appear later in the string,
the string would not be ordered, \ie\
a greater number would appear 
before the smallest possible number \ensuremath{\Varid{zero}}.

To be sure that the \acronym{rgs}-technique effectively
avoids duplication of subset by suppressing permuations,
we should at least sketch a proof of the concept.
We should prove that restricted growth makes 
complementing groups of digits impossible, such
that all digits $k_1$ and $k_2$ swap their positions
from one string to the other.
The following diagram shows four positions
in a string where, at positions $i$ and $i+1$,
there is the digit $k_1$ and, at positions $j$ and $j+1$,
there is the digit $k_2$:

\begin{tabular}{c|c|c|c|c|c|c}
$\dots$ & $i$   & $i+1$ & $\dots$ & $j$   & $j+1$ & $\dots$\\\hline
$\dots$ & $k_1$ & $k_1$ & $\dots$ & $k_2$ & $k_2$ & $\dots$
\end{tabular}

We assume that $j > i+1$ and we assume that all occurences of $k_1$ and $k_2$
in the string are shown.
In other words, this partial string shows the partitions
$k_1 = \lbrace i,i+1\rbrace$ and
$k_2 = \lbrace j,j+1\rbrace$.

We prove by contradiction on restricted growth and assume
that this string is possible with both cases,
$k_1 < k_2$ or, alternatively, $k_2 < k_1$.
Consider the case $k_2 < k_1$.
In this case $k_2$ must appear in a position $p < i$,
otherwise, ordering would be violated. 
But this contradicts the assumption that $k_2 = \lbrace j,j+1\rbrace$.
So, either we violate ordering or $k_2$ is not shown completely
in the diagram above and, then, the subsets are not complementing. 
Ordering is violated because it implies that any digit $a_i$ in the string
is at most $1 + \max{a_1, a_2, \dots, a_{i-1}}$.
$i$ is either 0, then $k_1$, per definition, is 0 as well
and no (natural) number is less than 0, 
hence $k_2$ cannot be less than $k_1$;
or $i$ is not 0, then there must be a digit $k_0 = k_1 - 1$.
If we assume that $k_2 < k_1$, we must assume that 
$k_1 - 1 < k_2 < k_1$ and, hence, that $0 < k2 < 1$.
But that cannot be, 
since $k_2$ is still a natural number.$\qed$

To implement the \acronym{rgs} analogy,
we first need a function that converts decimal numbers
into numbers with base $b$.
We have already looked at such functions,
for $b = 2$ in the previous section,
which we called \ensuremath{\Varid{binExp}}, and, for $b=10$,
in the previous chapter in the context of
the conversion function \ensuremath{\Varid{integer2Num}}.
\ensuremath{\Varid{binExp}} was tailored for binary numbers,
since it yielded only the positions 
where the binary result would have a 1.
That information is obviously not sufficient
for number systems with $b > 2$, where the decision
which number to put at a given position is not binary.

Let us recall how we converted integer to 
our natural number type.
We divided the number by 10,
collecting the remainders and continuing on the quotient,
like in the following example:

\ensuremath{\mathrm{1000}\mathbin{`\Varid{quotRem}`}\mathrm{10}\mathrel{=}(\mathrm{100},\mathrm{0})}\\
\ensuremath{\mathrm{100}\mathbin{`\Varid{quotRem}`}\mathrm{10}\mathrel{=}(\mathrm{10},\mathrm{0})}\\
\ensuremath{\mathrm{10}\mathbin{`\Varid{quotRem}`}\mathrm{10}\mathrel{=}(\mathrm{1},\mathrm{0})}\\
\ensuremath{\mathrm{1}\mathbin{`\Varid{quotRem}`}\mathrm{10}\mathrel{=}(\mathrm{0},\mathrm{1})}.

Now, the remainders of the subsequent divisions
bottom-up would read $1,0,0,0$, which are just 
the components of the decimal representation 
of the number \num{1000}.
If we do this with $b=2$, we would see:

\ensuremath{\mathrm{1000}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{500},\mathrm{0})}\\
\ensuremath{\mathrm{500}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{250},\mathrm{0})}\\
\ensuremath{\mathrm{250}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{125},\mathrm{0})}\\
\ensuremath{\mathrm{125}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{62},\mathrm{1})}\\
\ensuremath{\mathrm{62}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{31},\mathrm{0})}\\
\ensuremath{\mathrm{31}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{15},\mathrm{1})}\\
\ensuremath{\mathrm{15}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{7},\mathrm{1})}\\
\ensuremath{\mathrm{7}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{3},\mathrm{1})}\\
\ensuremath{\mathrm{3}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{1},\mathrm{1})}\\
\ensuremath{\mathrm{1}\mathbin{`\Varid{quotRem}`}\mathrm{2}\mathrel{=}(\mathrm{0},\mathrm{1})}.

\num{1000}, in the binary system, hence, 
is $1111101000$, the same result we have already obtained
in the previous section.
We can implement this procedure in Haskell as:\footnote{
We use \ensuremath{\Conid{Int}} instead of \ensuremath{\Conid{Natural}} here, because,
in the following, we will need list functions
like \ensuremath{\Varid{length}} or \ensuremath{\Varid{take}} quite often;
with \ensuremath{\Conid{Natural}}, we would have to add a lot of conversions,
which is much harder to read.}

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{toBaseN}\mathbin{::}\Conid{Int}\to \Conid{Int}\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{toBaseN}\;\Varid{b}\mathrel{=}\Varid{reverse}\mathbin{\circ}\Varid{go}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{x}\mathrel{=}{}\<[19]%
\>[19]{}\mathbf{case}\;\Varid{x}\mathbin{`\Varid{quotRem}`}\Varid{b}\;\mathbf{of}{}\<[E]%
\\
\>[19]{}(\mathrm{0},\Varid{r})\to [\mskip1.5mu \Varid{r}\mskip1.5mu]{}\<[E]%
\\
\>[19]{}(\Varid{q},\Varid{r})\to \Varid{r}\mathbin{:}\Varid{go}\;\Varid{q}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The \ensuremath{\Varid{go}}-part of this function applied to base $b = 3$
and, say, \num{1024} would develop
as follows:

\ensuremath{\Varid{go}\;\mathrm{1024}\mathrel{=}\mathrm{1024}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\Varid{go}\;\mathrm{341}\mathrel{=}\mathrm{341}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\Varid{go}\;\mathrm{113}\mathrel{=}\mathrm{113}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{2}\mathbin{:}\Varid{go}\;\mathrm{37}\mathrel{=}\mathrm{37}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{1}\mathbin{:}\Varid{go}\;\mathrm{12}\mathrel{=}\mathrm{12}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{1}\mathbin{:}\mathrm{0}\mathbin{:}\Varid{go}\;\mathrm{4}\mathrel{=}\mathrm{4}\mathbin{`\Varid{quotRem}`}\mathrm{3}}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{1}\mathbin{:}\mathrm{0}\mathbin{:}\mathrm{1}\mathbin{:}\Varid{go}\;\mathrm{1}\mathrel{=}[\mskip1.5mu \mathrm{1}\mskip1.5mu]}\\
\ensuremath{\mathrm{1}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{2}\mathbin{:}\mathrm{1}\mathbin{:}\mathrm{0}\mathbin{:}\mathrm{1}\mathbin{:}\mathrm{1}},

which, reversed, is \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{0},\mathrm{1},\mathrm{2},\mathrm{2},\mathrm{1}\mskip1.5mu]}
and the correct representation of \num{1000}
in the ternary system.

Now we need some functions to convert 
the number given in the $b$-ary system
into an \acronym{rgs}. 
First we fill the number with leading zeros
until it has the desired size $n$:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}c<{\hspost}@{}}%
\column{24E}{@{}l@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{rgs}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{rgs}\;\Varid{b}\;\Varid{n}\;\Varid{i}\mathrel{=}{}\<[16]%
\>[16]{}\mathbf{let}\;{}\<[21]%
\>[21]{}\Varid{r}{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\Varid{toBaseN}\;\Varid{b}\;\Varid{i}{}\<[E]%
\\
\>[21]{}\Varid{d}{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\Varid{n}\mathbin{-}\Varid{length}\;\Varid{r}{}\<[E]%
\\
\>[21]{}\Varid{p}{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\mathbf{if}\;\Varid{d}\mathbin{>}\mathrm{0}\;\mathbf{then}\;\Varid{take}\;\Varid{d}\;(\Varid{repeat}\;\mathrm{0})\;\mathbf{else}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[16]{}\mathbf{in}\;\mathbf{if}\;\Varid{d}\mathbin{<}\mathrm{0}\;\mathbf{then}\;[\mskip1.5mu \mskip1.5mu]\;\mathbf{else}\;\Varid{p}\plus \Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Note that, if the length of the result of \ensuremath{\Varid{toBaseN}} exceeds $n$,
the function yields the empty list.
This, in fact, is an error case that could be handled explicitly.
On the other hand, returning the empty list
is a good enough indication for an error
and we could check for this error in code
using \ensuremath{\Varid{rgs}} later.

We now define a wrapper around this conversion function
to apply the restrictions:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{33}{@{}>{\hspre}c<{\hspost}@{}}%
\column{33E}{@{}l@{}}%
\column{36}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{toRgs}\mathbin{::}([\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Bool})\to {}\<[E]%
\\
\>[3]{}\hsindent{9}{}\<[12]%
\>[12]{}\Conid{Int}\to \Conid{Int}\to \Conid{Int}\to (\Conid{Int},[\mskip1.5mu \Conid{Int}\mskip1.5mu]){}\<[E]%
\\
\>[3]{}\Varid{toRgs}\;\Varid{rst}\;\Varid{b}\;\Varid{n}\;\Varid{i}\mathrel{=}\Varid{go}\;(\Varid{rgs}\;\Varid{b}\;\Varid{n}\;\Varid{i}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{r}{}\<[17]%
\>[17]{}\mid {}\<[17E]%
\>[20]{}\neg \;(\Varid{rst}\;\Varid{r}){}\<[33]%
\>[33]{}\mathrel{=}{}\<[33E]%
\>[36]{}\Varid{toRgs}\;\Varid{rst}\;\Varid{b}\;\Varid{n}\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{otherwise}{}\<[33]%
\>[33]{}\mathrel{=}{}\<[33E]%
\>[36]{}(\Varid{i},\Varid{r}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This function converts a decimal number
to an \acronym{rgs} and checks if the result
obeys the restrictions, which are passed in as a boolean function.
If it does not, then the input is incremented by one
and the function is called again.
Otherwise, the function yields a tuple consisting 
of the decimal number that we have eventually reached
and the \acronym{rgs}.

We define the growth restriction as follows:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}c<{\hspost}@{}}%
\column{26E}{@{}l@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{rGrowth}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Bool}{}\<[E]%
\\
\>[3]{}\Varid{rGrowth}\;{}\<[12]%
\>[12]{}[\mskip1.5mu \mskip1.5mu]{}\<[20]%
\>[20]{}\mathrel{=}{}\<[20E]%
\>[23]{}\Conid{True}{}\<[E]%
\\
\>[3]{}\Varid{rGrowth}\;{}\<[12]%
\>[12]{}(\Varid{x}\mathbin{:}\Varid{xs}){}\<[20]%
\>[20]{}\mathrel{=}{}\<[20E]%
\>[23]{}\Varid{go}\;\Varid{x}\;\Varid{xs}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;\anonymous \;{}\<[18]%
\>[18]{}[\mskip1.5mu \mskip1.5mu]{}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\Conid{True}{}\<[E]%
\\
\>[12]{}\Varid{go}\;\Varid{d}\;{}\<[18]%
\>[18]{}(\Varid{z}\mathbin{:}\Varid{zs}){}\<[26]%
\>[26]{}\mathrel{=}{}\<[26E]%
\>[29]{}\mathbf{if}\;\Varid{z}\mathbin{-}\Varid{d}\mathbin{>}\mathrm{1}\;\mathbf{then}\;\Conid{False}{}\<[E]%
\\
\>[29]{}\mathbf{else}\;{}\<[35]%
\>[35]{}\mathbf{let}\;\Varid{d'}\mathrel{=}\Varid{max}\;\Varid{d}\;\Varid{z}\;\mathbf{in}\;\Varid{go}\;\Varid{d'}\;\Varid{zs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Since we want to see
only partitionings with $b$ subsets,
we, still, need another restriction.
We do not want to see  \acronym{rgs} of the form
$01000$, when we ask for three partitions, or
$01230$, when we ask for five.
For this end, we need the restriction
that there must be $b$ different digits
in the resulting \acronym{rgs}.
The restriction is easily implemented as:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{hasN}\mathbin{::}\Conid{Int}\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Bool}{}\<[E]%
\\
\>[3]{}\Varid{hasN}\;\Varid{b}\;\Varid{r}\mathrel{=}\Varid{length}\;(\Varid{nub}\;\Varid{r})\equiv \Varid{b}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

We apply this restrictions in yet another wrapper 
to call \ensuremath{\Varid{toRgs}}:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{6}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{toRgsN}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}\to (\Conid{Int},[\mskip1.5mu \Conid{Int}\mskip1.5mu]){}\<[E]%
\\
\>[3]{}\Varid{toRgsN}\;\Varid{b}\mathrel{=}\Varid{toRgs}\;\Varid{rst}\;\Varid{b}{}\<[E]%
\\
\>[3]{}\hsindent{3}{}\<[6]%
\>[6]{}\mathbf{where}\;\Varid{rst}\;\Varid{r}\mathrel{=}\Varid{rGrowth}\;\Varid{r}{}\<[31]%
\>[31]{}\mathrel{\wedge}\Varid{hasN}\;{}\<[40]%
\>[40]{}\Varid{b}\;\Varid{r}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Finally, we can implement a loop
that counts \acronym{rgs} up from 1 to the last number
with leading 0:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}c<{\hspost}@{}}%
\column{17E}{@{}l@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}c<{\hspost}@{}}%
\column{31E}{@{}l@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{countRgs}\mathbin{::}\Conid{Int}\to \Conid{Int}\to [\mskip1.5mu [\mskip1.5mu \Conid{Int}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{countRgs}\;\mathrm{1}\;\Varid{n}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}[\mskip1.5mu \Varid{rgs}\;\mathrm{1}\;\Varid{n}\;\mathrm{1}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{countRgs}\;\Varid{b}\;\Varid{n}{}\<[17]%
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{b}\equiv \Varid{n}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}[\mskip1.5mu [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{n}\mathbin{-}\mathrm{1}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{b}\mathbin{>}{}\<[25]%
\>[25]{}\Varid{n}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[17]{}\mid {}\<[17E]%
\>[20]{}\Varid{otherwise}{}\<[31]%
\>[31]{}\mathrel{=}{}\<[31E]%
\>[34]{}\Varid{go}\;\mathrm{1}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{i}{}\<[17]%
\>[17]{}\mathrel{=}{}\<[17E]%
\>[20]{}\mathbf{let}\;(\Varid{j},\Varid{r})\mathrel{=}\Varid{toRgsN}\;\Varid{b}\;\Varid{n}\;\Varid{i}{}\<[E]%
\\
\>[20]{}\mathbf{in}\;\mathbf{if}\;\Varid{head}\;\Varid{r}\not\equiv \mathrm{0}\;\mathbf{then}\;[\mskip1.5mu \mskip1.5mu]\;\mathbf{else}\;\Varid{r}\mathbin{:}\Varid{go}\;(\Varid{j}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Note that we jump over numbers that do not obey
the restrictions: we continue always with \ensuremath{\Varid{go}} applied to $j$, 
the first return value of \ensuremath{\Varid{toRgsN}}.
$j$ does not necessarily equal $i$;
it depends on how many numbers have been ignored
by \ensuremath{\Varid{toRgs}} because they did not obey the restrictions.
When we call countRgs on 3, the numbers of partitions we want to have,
and 4, the number of elements in the original set,
we get the following \acronym{rgs}:

\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{0},\mathrm{1},\mathrm{2}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{0},\mathrm{2}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{1},\mathrm{2}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{2},\mathrm{0}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{2},\mathrm{1}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{0},\mathrm{1},\mathrm{2},\mathrm{2}\mskip1.5mu]},

which correspond to the partitionings of $\lbrace 1,2,3,4\rbrace$:

$\lbrace 1,2\rbrace, \lbrace 3  \rbrace, \lbrace 4  \rbrace$\\
$\lbrace 1,3\rbrace, \lbrace 2  \rbrace, \lbrace 4  \rbrace$\\
$\lbrace 1  \rbrace, \lbrace 2,3\rbrace, \lbrace 4  \rbrace$\\
$\lbrace 1,4\rbrace, \lbrace 2  \rbrace, \lbrace 4  \rbrace$\\
$\lbrace 1  \rbrace, \lbrace 2,4\rbrace, \lbrace 3  \rbrace$\\
$\lbrace 1  \rbrace, \lbrace 2  \rbrace, \lbrace 3,4\rbrace$.

This analogy between \acronym{rgs} and partitions is implemented as:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{30}{@{}>{\hspre}c<{\hspost}@{}}%
\column{30E}{@{}l@{}}%
\column{33}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{rgs2set}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{rgs2set}\;[\mskip1.5mu \mskip1.5mu]\;\anonymous \;{}\<[17]%
\>[17]{}\Varid{ps}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{ps}{}\<[E]%
\\
\>[3]{}\Varid{rgs2set}\;\anonymous \;{}\<[14]%
\>[14]{}[\mskip1.5mu \mskip1.5mu]\;\Varid{ps}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{ps}{}\<[E]%
\\
\>[3]{}\Varid{rgs2set}\;(\Varid{r}\mathbin{:}\Varid{rs})\;(\Varid{x}\mathbin{:}\Varid{xs})\;\Varid{ps}{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{rgs2set}\;\Varid{rs}\;\Varid{xs}\;(\Varid{ins}\;\Varid{r}\;\Varid{x}\;\Varid{ps}){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{ins}\;\anonymous \;\anonymous \;[\mskip1.5mu \mskip1.5mu]{}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\bot {}\<[E]%
\\
\>[12]{}\Varid{ins}\;\mathrm{0}\;\Varid{p}\;(\Varid{z}\mathbin{:}\Varid{zs}){}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}(\Varid{p}\mathbin{:}\Varid{z})\mathbin{:}\Varid{zs}{}\<[E]%
\\
\>[12]{}\Varid{ins}\;\Varid{i}\;\Varid{p}\;(\Varid{z}\mathbin{:}\Varid{zs}){}\<[30]%
\>[30]{}\mathrel{=}{}\<[30E]%
\>[33]{}\Varid{z}\mathbin{:}\Varid{ins}\;(\Varid{i}\mathbin{-}\mathrm{1})\;\Varid{p}\;\Varid{zs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The function receives three arguments:
The \acronym{rgs}, the original set we want to partition
and the result set, which initially 
should contain $k$ empty lists
with $k$ the number of partitions we want to obtain. 
(This pre-condition, actually, is not enforced in this code.)
Note that the logic of using $k$ empty lists is very similar 
to the trick we used in \ensuremath{\Varid{twoParatitions}} above.

When we have exhausted either the \acronym{rgs} or
the original set, the result is just $ps$,
the result set we passed in.
Otherwise, we recurse with the tails 
of the \acronym{rgs} and the set
(this way establishing the analogy)
inserting the element of the set that corresponds
to the current position of the \acronym{rgs}
to the partition that, in its turn, corresponds
to the digit of the \acronym{rgs} at this position.
If the digit is 0, we just insert the element into the first list,
otherwise we recurse (on \ensuremath{\Varid{ins}}) 
decrementing the digit by 1.
Note that \ensuremath{\Varid{ins}} is \ensuremath{\bot } for the case
that the result set is empty before we reach 0.
This, obviously, would hint to an erroneous \acronym{rgs}
and, hence, to a coding error.

We now can put everything together:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}c<{\hspost}@{}}%
\column{21E}{@{}l@{}}%
\column{22}{@{}>{\hspre}c<{\hspost}@{}}%
\column{22E}{@{}l@{}}%
\column{23}{@{}>{\hspre}c<{\hspost}@{}}%
\column{23E}{@{}l@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{41}{@{}>{\hspre}c<{\hspost}@{}}%
\column{41E}{@{}l@{}}%
\column{44}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{nPartitions}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{nPartitions}\;\anonymous \;[\mskip1.5mu \mskip1.5mu]{}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{nPartitions}\;\mathrm{1}\;\Varid{xs}{}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}[\mskip1.5mu [\mskip1.5mu \Varid{xs}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{nPartitions}\;\Varid{k}\;\Varid{xs}{}\<[22]%
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{k}\geq \Varid{length}\;\Varid{xs}{}\<[41]%
\>[41]{}\mathrel{=}{}\<[41E]%
\>[44]{}[\mskip1.5mu [\mskip1.5mu [\mskip1.5mu \Varid{x}\mskip1.5mu]\mid \Varid{x}\leftarrow \Varid{xs}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[22]{}\mid {}\<[22E]%
\>[25]{}\Varid{otherwise}{}\<[41]%
\>[41]{}\mathrel{=}{}\<[41E]%
\>[44]{}\Varid{go}\;(\Varid{countRgs}\;\Varid{k}\;(\Varid{length}\;\Varid{xs})){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{go}\;[\mskip1.5mu \mskip1.5mu]{}\<[23]%
\>[23]{}\mathrel{=}{}\<[23E]%
\>[26]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{go}\;(\Varid{r}\mathbin{:}\Varid{rs}){}\<[23]%
\>[23]{}\mathrel{=}{}\<[23E]%
\>[26]{}\Varid{rgs2set}\;\Varid{r}\;\Varid{xs}\;(\Varid{take}\;\Varid{k}\;(\Varid{repeat}\;[\mskip1.5mu \mskip1.5mu]))\mathbin{:}\Varid{go}\;\Varid{rs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The function \ensuremath{\Varid{nPartitions}} receives 
the number of partitions we would like to have, $k$,
and the original set, \ensuremath{\Varid{xs}}.
If the set is empty, the result is empty, too.
If we just want one partition, we return the set as its only partition.
If $k$ equals or exceeds the size of the set,
we just return each element in its own set.
(We could return an error for the case that $k$ exceeds the size
of the set, but, for sake of simplicity, we allow this case, 
returning an incomplete result.)
Otherwise, we call \ensuremath{\Varid{countRgs}}
and apply \ensuremath{\Varid{rgs2set}} to all elements of the result.
The set of empty lists we need to start with \ensuremath{\Varid{rgs2set}}
is created by \ensuremath{\Varid{repeat}\;[\mskip1.5mu \mskip1.5mu]} which creates an infinite list
containing the empty list -- \ensuremath{[\mskip1.5mu [\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu],\mathbin{...}\mskip1.5mu]} --
from which we just take $k$, \ie\ the number of partitions.
The call \ensuremath{\Varid{nPartitions}\;\mathrm{2}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} would
yield the expected result of all possibilities
to partition \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu]} in 2 subsets:

\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{2}\mskip1.5mu]}\\
\ensuremath{[\mskip1.5mu \mathrm{1}\mskip1.5mu],[\mskip1.5mu \mathrm{3},\mathrm{2}\mskip1.5mu]}.

This result corresponds to the \acronym{rgs}:

$001$,\\
$010$ and \\
$011$.

The order of elements within partitions is explained by the fact
that \ensuremath{\Varid{ins}} (in \ensuremath{\Varid{rgs2set}}) adds new elements using ``\ensuremath{\mathbin{:}}'' --
the element that was first inserted into the list
is therefore the last one in the resulting partition.

It should be noted that, as for the powerset,
we can optimise this code by using other data structures than lists.
Since, as for the powerset as well, 
the number of possible partitionings of huge sets
is incredibly large, it is not feasible
to computate all partitionings of great sets anyway.
We, therefore, leave it with a non-optimal implementation.

Now, that we have arrived here,
the mathematically natural question occurs
of how many partitions there are for a set of size $n$.
Well, we have a tool to try that out.
The following -- \speech{er} -- triangle 
shows results of calls of \ensuremath{\Varid{nPartitions}}.
Each line corresponds to the call 
\ensuremath{[\mskip1.5mu \Varid{length}\;(\Varid{nPartitions}\;\Varid{k}\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu])\mid \Varid{k}\leftarrow [\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\Varid{n}\mskip1.5mu]\mskip1.5mu]},
where $n$ starts with 1 at the top of the triangle
and is counted up until 7 at its bottom.

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
1 &   &   &   &   &    &    &    &     &     &   1 &     &     &    &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &    &    &     &   1 &     &   1 &     &    &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &    &    &   1 &     &   3 &     &   1 &    &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &    &  1 &     &   7 &     &   6 &     &  1 &    &    &   &   &   &   &  \\
5 &   &   &   &   &    &  1 &    &  15 &     &  25 &     &  10 &    &  1 &    &   &   &   &   &  \\
6 &   &   &   &   &  1 &    & 31 &     &  90 &     &  65 &     & 15 &    &  1 &   &   &   &   &  \\   
7 &   &   &   & 1 &    & 63 &    & 301 &     & 350 &     & 140 &    & 21 &    & 1 &   &   &   &  
\end{tabular}

On the first sight, the values in this triangle
besides the ones in the outer diagonals appear less regular
than those in Pascal's nice and tidy triangle.
Nevertheless, already the second sight reveals some curious relations.
The second diagonal from top-right to left-bottom,
which reads 1,3,7,15,31,63, corresponds to the values $2^n-1$.
The second diagonal from top-left to right-bottom 
shows other numbers we already know, namely 1,3,6,10,15,21.
If you take the differences, you will observe that each
number is the sum of $n$ and its predecessor.
In other words, this diagonal contains the sum of all numbers
from 1 to $n$, where $n$ is the line number.

The triangle overall shows the \term{Stirling set numbers},
also known as the \term{Stirling numbers of the second kind},
which are denoted by

\begin{equation}
  S(n,k) = \stirlingTwo{n}{k}.
\end{equation}

The formula to compute Stirling numbers remarkably
resembles Pascal's rule:

\begin{equation}
\stirlingTwo{n}{k} = \begin{cases}
                       0 & \textrm{if $n = 0$}\\
                       1 & \textrm{if $n = 1$}\\
                       k \times \stirlingTwo{n-1}{k} + 
                                \stirlingTwo{n-1}{k-1} &
                         \textrm{otherwise}.
                   \end{cases}
\end{equation}

This translates into Haskell as:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}c<{\hspost}@{}}%
\column{18E}{@{}l@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}c<{\hspost}@{}}%
\column{32E}{@{}l@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{stirling2}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{stirling2}\;\mathrm{0}\;\anonymous {}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{stirling2}\;\anonymous \;\mathrm{0}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{stirling2}\;\mathrm{1}\;\mathrm{1}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{stirling2}\;\Varid{n}\;\Varid{k}{}\<[18]%
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{k}\mathbin{>}\Varid{n}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\mathrm{0}{}\<[E]%
\\
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{otherwise}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\Varid{k}\mathbin{*}{}\<[40]%
\>[40]{}(\Varid{stirling2}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{k})\mathbin{+}{}\<[E]%
\\
\>[40]{}(\Varid{stirling2}\;(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{k}\mathbin{-}\mathrm{1})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

The code is almost a one-to-one translation
of the mathematical formulation.
However, there is an extra line, namely
the base case \ensuremath{\Varid{stirling2}\;\anonymous \;\mathrm{0}\mathrel{=}\mathrm{0}}.
This base case must be introduced to avoid
that we go below zero for $k$ with the rule part
\ensuremath{\Varid{stirling2}\;(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{k}\mathbin{-}\mathrm{1})}.
The natural numbers are not defined for the range
less than zero and so we have to stop here explicitly.

The sum of all Stirling numbers of the same row,
\ie\ $\sum_{k=1}^{n}{\stirlingTwo{n}{k}}$,
is called the \term{Bell number} of $n$.
The first 7 Bell numbers are: 1, 2, 5, 15, 52, 203, 877.

Since Stirling numbers of the second kind count the ways
to partition a set into a given number of subsets,
Bell numbers indicate all ways to partition a set,
not restricting the number of subsets we want to obtain.
The following table shows this relation 
for the set $\lbrace 1,2,3,4\rbrace$:

\bgroup
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|c|c|c|c|}\hline
$\stirlingTwo{4}{1} = 1$ & $\stirlingTwo{4}{2} = 7$ & 
$\stirlingTwo{4}{3} = 6$ &  $\stirlingTwo{4}{4} = 1$\\\hline\hline
% -- 1st row -------------------------------------------------------------------------------------
$\lbrace 1,2,3,4\rbrace$ & 
$\lbrace 1,2,3\rbrace, \lbrace 4\rbrace$ & 
$\lbrace 1,2\rbrace, \lbrace 3\rbrace, \lbrace 4\rbrace$ & 
$\lbrace 1\rbrace, \lbrace 2\rbrace, \lbrace 3\rbrace, \lbrace 4\rbrace$ \\\cline{2-3}
% -- 2nd row -------------------------------------------------------------------------------------
 & 
$\lbrace 1,2,4\rbrace, \lbrace 3\rbrace$ & 
$\lbrace 1,3\rbrace, \lbrace 2\rbrace, \lbrace 4\rbrace$ & \\\cline{2-3}
% -- 3rd row -------------------------------------------------------------------------------------
 & 
$\lbrace 1,3,4\rbrace, \lbrace 2\rbrace$ & 
$\lbrace 1,4\rbrace, \lbrace 2\rbrace, \lbrace 3\rbrace$ & \\\cline{2-3}
% -- 4th row -------------------------------------------------------------------------------------
 & 
$\lbrace 2,3,4\rbrace, \lbrace 1\rbrace$ & 
$\lbrace 1\rbrace, \lbrace 2,3\rbrace, \lbrace 4\rbrace$ & \\\cline{2-3}
% -- 5th row -------------------------------------------------------------------------------------
 & 
$\lbrace 1,2\rbrace, \lbrace 3,4\rbrace$ & 
$\lbrace 1\rbrace, \lbrace 2,4\rbrace, \lbrace 3\rbrace$ & \\\cline{2-3}
% -- 6th row -------------------------------------------------------------------------------------
 & 
$\lbrace 1,3\rbrace, \lbrace 2,4\rbrace$ & 
$\lbrace 1\rbrace, \lbrace 2\rbrace, \lbrace 3,4\rbrace$ &  \\\cline{2-3}
% -- 7th row -------------------------------------------------------------------------------------
 & 
$\lbrace 1,4\rbrace, \lbrace 2,3\rbrace$ &  & \\\hline
\end{tabular}
\end{center}
\egroup

If you count the partitionings in one column,
you get the Stirling number for that column;
the number of all partitions in all columns
equals the Bell number of 4, which is 15.

The inventors -- or discoverers, depending on 
your philosophical view -- of Bell numbers and Stirling numbers
are two very interesting characters.
Eric Temple Bell (1883 -- 1960) was professor of mathematics
in the United States for most of his life.
But he was also an early science fiction writer
and a math historian.
His science fiction reached a higher level of science
than most other publications in this genre at his time,
but was often critised as poorly written and, in particular,
for its weak characterisation of protagonists.
His contributions to math history were even more fiercely critised as
fictitious and romantic (as in the case of his biographical sketch
of Évarist Galois) or as stereotypical (as in the case
of his description of the life of Georg Cantor).

James Stirling (1692 -- 1770) was from Scotland.
He studied and taught in Oxford for some years, but had to
flee from England, when he was accused of conspiracy
based on his correspondence with Jacobites, 
that is supporters of the catholic kings, 
in particular James II who was deposed in 1688.
After ten years of exile in Venice,
he started to fear for his life again, because
he discovered a trade secret of the glassmakers of Venice
and returned to England with the help of his friend
Isaac Newton. Much of Stirling's work
is in fact tightly coupled with that of Newton.
Stirling very much promoted Newton's discoveries and methods,
for instance
in his book \term{Methodus differentialis}.
During the last years of his life,
he was manager of the Scots Mining Company.
During this period, he published mainly on topics
of applied mathematics.

But let us return to the intial question.
We were investigating the possible permutations
with a given number of orbits in the cycle notation
and have just learnt how to generate all possible $k$ orbits
of a set with $n$ elements.
We have found out how to partition 
a set of $n$ elements into $k$ subsets.
However, the distinct subsets are not yet sufficient
to generate all possible permutations,
since the permutation of $(1~2~3~4~5)$
$\sigma1 = (1~2~5) (3~4)$
is not the same as $\sigma2 = (1~5~2) (3~4)$:

\[
\sigma1 = 2~5~4~3~1
\]
\[
\sigma2 = 5~1~4~3~2
\]

So, do we need all permutations of the subsets?
(Was our survey of \acronym{rgs} in vain?)
Apparently not, since $\sigma1$ is just the same
permutation as $\sigma3 = (1~2~5) (4~3)$.
It is also the same as $(5~1~2) (3~4)$.
In fact, the cycle notation is indifferent
concerning the starting point -- for this reason, it is called cyclic.
Therefore, not all permutations are relevant,
but only those that change the order after the first element.
An orbit permutating function aware of this peculiarity is:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}c<{\hspost}@{}}%
\column{22E}{@{}l@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{27}{@{}>{\hspre}c<{\hspost}@{}}%
\column{27E}{@{}l@{}}%
\column{30}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{permOrbits}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Perm}\;\Varid{a}\to [\mskip1.5mu \Conid{Perm}\;\Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{permOrbits}\;[\mskip1.5mu \mskip1.5mu]{}\<[22]%
\>[22]{}\mathrel{=}{}\<[22E]%
\>[25]{}[\mskip1.5mu [\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{permOrbits}\;(\Varid{o}\mathbin{:}\Varid{oo}){}\<[22]%
\>[22]{}\mathrel{=}{}\<[22E]%
\>[25]{}\Varid{concat}\;[\mskip1.5mu \Varid{map}\;(\mathbin{:}\Varid{x})\;(\Varid{oPerms}\;\Varid{o})\mid \Varid{x}\leftarrow \Varid{permOrbits}\;\Varid{oo}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{oPerms}\;[\mskip1.5mu \mskip1.5mu]{}\<[27]%
\>[27]{}\mathrel{=}{}\<[27E]%
\>[30]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[12]{}\Varid{oPerms}\;(\Varid{x}\mathbin{:}\Varid{xs}){}\<[27]%
\>[27]{}\mathrel{=}{}\<[27E]%
\>[30]{}[\mskip1.5mu \Varid{x}\mathbin{:}\Varid{ps}\mid \Varid{ps}\leftarrow \Varid{perms}\;\Varid{xs}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This function just passes through all orbits
of the input permutation
creating permutations of each one using \ensuremath{\Varid{oPerms}}.
It looks a bit weird that we do not just use \ensuremath{\Varid{map}}
to create that result, but a list comprehension
to which we even further apply \ensuremath{\Varid{concat}}.
However, \ensuremath{\Varid{map}} does not yield the desired result.
\ensuremath{\Varid{map}} would just create a list of permutated orbits --
but we want to obtain complete cycles each of which
may consist of more than one orbit.
For this reason, we create permutations of the head
and insert all permutations of the head to all results
of the recursion of \ensuremath{\Varid{permOrbits}}.

The function we use for creating permutations of orbits
is \ensuremath{\Varid{oPerms}}, which applies
\ensuremath{\Varid{perms}}, all permutations of a list,
to the tail of the input list.
The head of the list, hence, remains always the same.
For instance, \ensuremath{\Varid{oPerms}\;[\mskip1.5mu \mathrm{3},\mathrm{4}\mskip1.5mu]} is just \ensuremath{[\mskip1.5mu \mathrm{3},\mathrm{4}\mskip1.5mu]}.
\ensuremath{\Varid{oPerms}\;[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{5}\mskip1.5mu]}, however, yields \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{5}\mskip1.5mu]} and \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{5},\mathrm{2}\mskip1.5mu]}.

Now, for one possible cycle, we can just apply
all permutations resulting from \ensuremath{\Varid{permOrbits}}:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{permsOfCycle}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Perm}\;\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{permsOfCycle}\;\Varid{os}\;\Varid{xs}\mathrel{=}[\mskip1.5mu \Varid{permute}\;\Varid{o}\;\Varid{xs}\mid \Varid{o}\leftarrow \Varid{permOrbits}\;\Varid{os}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

This function creates all permutations that are possible
given one partitioning of the input set.
We now map this function on all possible partitionings
with $k$ subsets:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{permsWithCycles}\mathbin{::}(\Conid{Eq}\;\Varid{a})\Rightarrow \Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{permsWithCycles}\;\Varid{k}\;\Varid{xs}\mathrel{=}\Varid{concat}\;[\mskip1.5mu {}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{permsOfCycle}\;\Varid{x}\;\Varid{xs}\mid \Varid{x}\leftarrow \Varid{nPartitions}\;\Varid{k}\;\Varid{xs}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

Applied on sets with $n$ elements, for $n = 1 \dots  7$,
and $k = 1 \dots n$, \ensuremath{\Varid{permsWithCycles}} yields results of length:

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
1 &   &   &   &   &    &    &    &     &     &   1 &     &     &    &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &    &    &     &   1 &     &   1 &     &    &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &    &    &   2 &     &   3 &     &   1 &    &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &    &  6 &     &  11 &     &   6 &     &  1 &    &    &   &   &   &   &  \\
5 &   &   &   &   &    & 24 &    &  50 &     &  35 &     &  10 &    &  1 &    &   &   &   &   &  \\
6 &   &   &   &   & 120&    &274 &     & 225 &     &  85 &     & 15 &    &  1 &   &   &   &   &  \\   
7 &   &   &   &720&    &1764&    &1624 &     & 735 &     & 175 &    & 21 &    & 1 &   &   &   &  
\end{tabular}

These, as you may have guessed already,
are the Stirling numbers \term{of the first kind},
also known as \term{Stirling cycle numbers},
since they count the number of possible permutations
with a given number of orbits in the cycle notation.
They are denoted by

\begin{equation}
  s(n,k) = \stirlingOne{n}{k}
\end{equation}

and can be calculated as

\begin{equation}
\stirlingOne{n}{k} = \begin{cases}
                       0 & \textrm{if $n = 0$}\\
                       1 & \textrm{if $n = 1$}\\
                       (n-1) \times \stirlingOne{n-1}{k} + 
                                    \stirlingOne{n-1}{k-1} &
                         \textrm{otherwise}.
                   \end{cases}
\end{equation}

In Haskell, this would be:

\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}c<{\hspost}@{}}%
\column{18E}{@{}l@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{stirling1}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{stirling1}\;\mathrm{0}\;\anonymous {}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{stirling1}\;\anonymous \;\mathrm{0}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{stirling1}\;\mathrm{1}\;\mathrm{1}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[18E]%
\>[21]{}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{stirling1}\;\Varid{n}\;\Varid{k}{}\<[18]%
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{k}\mathbin{>}\Varid{n}{}\<[32]%
\>[32]{}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[18]{}\mid {}\<[18E]%
\>[21]{}\Varid{otherwise}{}\<[32]%
\>[32]{}\mathrel{=}(\Varid{n}\mathbin{-}\mathrm{1})\mathbin{*}{}\<[43]%
\>[43]{}(\Varid{stirling1}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{k})\mathbin{+}{}\<[E]%
\\
\>[43]{}(\Varid{stirling1}\;(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{k}\mathbin{-}\mathrm{1})){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks

We have seen that the sum of all Stirling numbers of the second kind 
in one row, \ie\ $\sum_{k=1}^{n}{\stirlingTwo{n}{k}}$,
is the Bell number of $n$.
Can you guess what this sum is for Stirling numbers of the first kind?

Remember that each Stirling number of the form 
$\stirlingOne{n}{k}$ shows the number of permutations
with a given number of orbits in cycle notation.
If you add up all possible permutations of all numbers 
of orbits $1 \dots n$, what do you get?
Let us see:

For $n=1$, we trivially get 1.\\
For $n=2$, we get $1 + 1 = 2$.\\
For $n=3$, we get $2 + 3 + 1 = 6$.\\
For $n=4$, we get $6 + 11 + 6 + 1 = 24$.\\
For $n=5$, we get $24 + 50 + 35 + 10 + 1 = 120$.

We know these numbers: these are the factorials of $n = n!$.
Since the factorial counts the number of all possible
permutations of a set, it is just natural that the 
Stirling numbers of the first kind, 
which count the possible permutations of a set
with a given number of orbits, add up to 
the number of all possible permutations, \ie\ the factorial
of the size of the set.
The triangle itself hints to that.
The outer left diagonal, actually, shows the factorials!
It shows the factorials of $n - 1$ though (with $n$ indicating the row).
If you think of how we create permutations of orbits
-- \viz\ as permutations of the tail of the orbit,
without touching the head --
it becomes immediately clear why the Stirling number $\stirlingOne{n}{1}$,
the one with only one partition, equals $(n-1)!$.

There is still a lot to say about Stirling numbers.
But that may involve concepts we have not yet discussed.
So, we will have to come back to this topic later.


\section{Eulerian Numbers}
\ignore{
\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{module}\;\Conid{Eulerian}{}\<[E]%
\\
\>[B]{}\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Natural}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{import}\;\Conid{Fact}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
}

\ignore{$\eulerianOne{n}{k}$}
\ignore{$\eulerianTwo{n}{k}$}
\ignore{$}

When we discussed Haskell, we studied a beautiful
sorting algorithm, Hoare's \term{quicksort}.
We already mentioned that \ensuremath{\Varid{quicksort}}
is not the best algorithm in practice 
and it is this question
to which we are coming back now.

The reason why \ensuremath{\Varid{quicksort}} is not optimal is
that it looks at the world in black and white:
a sequence is either sorted or it is not.
But reality is not like this.
Complete order, that is to say,
a sequence where every element is at its
place according to the order of the data type,
starting with the least element in the sequence
and each element being greater than its predecessor,
is very rare, of course, and usually
an effort is necessary to create complete
order in this sense.
Complete disorder, however, that is
a sequence where no element is at its
place, is quite rare too.
In fact, it is as rare as complete order:
for any sequence of unique elements,
there is exactly one permutation showing
complete order and one permutation
showing complete disorder.
Order and disorder balance each other.

For instance, the sequence $1,2,3,4,5$,
is completely ordered;
the permutation showing complete disorder
would be $5,4,3,2,1$ and that is the original
sequence reversed obeying as such another
kind of order, \viz\ $\le$ instead of $\ge$.
All other permutations are in between.
That is they show some order with a leaning
either to the original sequence or to 
its reverse. For example: $5,2,4,3,1$
is close to the opposite order;
reversed, it would be $1,3,4,2,5$
and close to order.

A sorting algorithm that exploits this
pre-existing order in any input is
\term{mergesort}. The underlying idea
of \ensuremath{\Varid{mergesort}} is to merge two ordered lists.
This can be implemented in Haskell simply as

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}c<{\hspost}@{}}%
\column{16E}{@{}l@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{37}{@{}>{\hspre}c<{\hspost}@{}}%
\column{37E}{@{}l@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}c<{\hspost}@{}}%
\column{43E}{@{}l@{}}%
\column{46}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{merge}\mathbin{::}(\Conid{Ord}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{merge}\;[\mskip1.5mu \mskip1.5mu]\;\Varid{xs}{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{merge}\;\Varid{xs}\;[\mskip1.5mu \mskip1.5mu]{}\<[16]%
\>[16]{}\mathrel{=}{}\<[16E]%
\>[19]{}\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{merge}\;(\Varid{x}\mathbin{:}\Varid{xs})\;(\Varid{y}\mathbin{:}\Varid{ys}){}\<[24]%
\>[24]{}\mid \Varid{x}\leq \Varid{y}{}\<[37]%
\>[37]{}\mathrel{=}{}\<[37E]%
\>[40]{}\Varid{x}{}\<[43]%
\>[43]{}\mathbin{:}{}\<[43E]%
\>[46]{}\Varid{merge}\;\Varid{xs}\;(\Varid{y}\mathbin{:}\Varid{ys}){}\<[E]%
\\
\>[24]{}\mid \Varid{otherwise}{}\<[37]%
\>[37]{}\mathrel{=}{}\<[37E]%
\>[40]{}\Varid{y}{}\<[43]%
\>[43]{}\mathbin{:}{}\<[43E]%
\>[46]{}\Varid{merge}\;(\Varid{x}\mathbin{:}\Varid{xs})\;\Varid{ys}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

We first treat the base cases
where one of the lists is empty,
just yielding the respective other list.
Then we compare the heads of the lists.
The smaller one goes to the head of the merged list
and we recurse with what remains.

We now want to use \ensuremath{\Varid{merge}} on an unordered,
that is to say, incompletely ordered list.
Therefore, we split the input into a list
of ordered sublists. 
Ordered sublists are often referred to as \ensuremath{\Varid{run}}s.
The positions where one ordered list ends
and another one begins are called \ensuremath{\Varid{stepdown}}s,
reflecting the idea that the sequence of 
increasing elements is interrupted by stepping
down to a smaller element.
Here is a Haskell function that splits a list
into runs:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{12}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}c<{\hspost}@{}}%
\column{21E}{@{}l@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}c<{\hspost}@{}}%
\column{26E}{@{}l@{}}%
\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
\column{40}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{runs}\mathbin{::}(\Conid{Ord}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{runs}\;[\mskip1.5mu \mskip1.5mu]{}\<[12]%
\>[12]{}\mathrel{=}{}\<[15]%
\>[15]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{runs}\;\Varid{xs}{}\<[12]%
\>[12]{}\mathrel{=}{}\<[15]%
\>[15]{}\mathbf{let}\;(\Varid{r},\Varid{zs})\mathrel{=}\Varid{run}\;\Varid{xs}\;\mathbf{in}\;\Varid{r}\mathbin{:}\Varid{runs}\;\Varid{zs}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[12]%
\>[12]{}\Varid{run}\;[\mskip1.5mu \mskip1.5mu]{}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}([\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu]){}\<[E]%
\\
\>[12]{}\Varid{run}\;[\mskip1.5mu \Varid{x}\mskip1.5mu]{}\<[21]%
\>[21]{}\mathrel{=}{}\<[21E]%
\>[24]{}([\mskip1.5mu \Varid{x}\mskip1.5mu],[\mskip1.5mu \mskip1.5mu]){}\<[E]%
\\
\>[12]{}\Varid{run}\;(\Varid{x}\mathbin{:}\Varid{y}\mathbin{:}\Varid{zs}){}\<[26]%
\>[26]{}\mid {}\<[26E]%
\>[29]{}\Varid{x}\mathbin{>}\Varid{y}{}\<[40]%
\>[40]{}\mathrel{=}([\mskip1.5mu \Varid{x}\mskip1.5mu],\Varid{y}\mathbin{:}\Varid{zs}){}\<[E]%
\\
\>[26]{}\mid {}\<[26E]%
\>[29]{}\Varid{otherwise}{}\<[40]%
\>[40]{}\mathrel{=}\mathbf{let}\;(\Varid{r},\Varid{ys})\mathrel{=}\Varid{run}\;(\Varid{y}\mathbin{:}\Varid{zs})\;\mathbf{in}\;(\Varid{x}\mathbin{:}\Varid{r},\Varid{ys}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function applied to the empty list
just yields the empty list. 
Applied to a non-empty list is calls the helper function \ensuremath{\Varid{run}}
that returns a tuple consisting of two lists of \ensuremath{\Varid{a}}s. 
The first element is then the head of the list
resulting from \ensuremath{\Varid{runs}} applied to the second list.

The helper function \ensuremath{\Varid{run}} applied on the empty list
yields a tuple of twice the empty list.
Applied to a list containing only one element,
it yield a tuple containing this list and the empty list.
Otherwise, the first two elements of the list, $x$ and $y$, 
are compared.
If $x > y$, $x$ goes to the first list of the resulting tuple,
$y$ goes to the rest of the list.
This is a stepdown: the first element $x$ is greater than 
its successor $y$
and, hence, the natural order of the sequence is violated.
Otherwise, we continue with \ensuremath{\Varid{y}\mathbin{:}\Varid{zs}} and insert $x$
as the head of the resulting $r$, the first of the tuple.
This is the case, where the run continues.

Let us look at an example: the permutation
we already used above $1,3,4,2,5$.
When we call \ensuremath{\Varid{run}} for the first time, we have:

\ensuremath{\Varid{run}\;(\mathrm{1}\mathbin{:}\mathrm{3}\mathbin{:}[\mskip1.5mu \mathrm{4},\mathrm{2},\mathrm{5}\mskip1.5mu])}

and we enter the \ensuremath{\Varid{otherwise}} alternative with $x=1$:

\ensuremath{\Varid{run}\;(\mathrm{3}\mathbin{:}\mathrm{4}\mathbin{:}[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu])}.

We again enter \ensuremath{\Varid{otherwise}}, this time $x=3$:

\ensuremath{\Varid{run}\;(\mathrm{4}\mathbin{:}\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{5}\mskip1.5mu])}.

But this time we have $4 > 2$ and enter
the first branch, that is we yield

\ensuremath{([\mskip1.5mu \mathrm{4}\mskip1.5mu],\mathrm{2}\mathbin{:}[\mskip1.5mu \mathrm{5}\mskip1.5mu])}.

Going backwards, we see:

\ensuremath{(\mathrm{3}\mathbin{:}[\mskip1.5mu \mathrm{4}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu])}\\
\ensuremath{(\mathrm{1}\mathbin{:}[\mskip1.5mu \mathrm{3},\mathrm{4}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu])},

which finally appears as result
of the first call to \ensuremath{\Varid{run}} in \ensuremath{\Varid{runs}},
which leads to \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{4}\mskip1.5mu]\mathbin{:}\Varid{runs}\;[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu]}.
\ensuremath{[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu]} is now handled in the same way
and we obtain the overall result \ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{1},\mathrm{3},\mathrm{4}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{5}\mskip1.5mu]\mskip1.5mu]}.
This way, thee entire list is split into two runs.

When we look at the other example,
the permutation that was closer to disorder,
there are more runs: $5|24|3|1$,
which is the proper mathematical notation
for the list \ensuremath{[\mskip1.5mu [\mskip1.5mu \mathrm{5}\mskip1.5mu],[\mskip1.5mu \mathrm{2},\mathrm{4}\mskip1.5mu],[\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]}
consisting of four runs.
If we reverse the list, however,
the number of runs reduces and 
we get the list with only two runs above.

Can we exploit the fact that we can reduce
the number of runs by reverting the list?
Yes, of course, otherwise the question
would not have been asked.
It turns out that, 
for lists with unique elements,
if the number of runs $r$ is greater
than $\frac{n}{2} + 1$, then
$r'$, the number of runs of the reversed list
is less than $r$.
In other words, we could check the number of runs,
before we start to sort, and revert the list
if the number of runs is greater than 
the half of the length of the list plus 1.

Note that in the real world
we often see lists with repeated elements.
The repetitions, however, would not spoil the result,
since repetitions would just reduce
the possible number of runs. In consequence,
it may happen that reverting the list,
even though the number of runs is less
than the half of the list size plus 1, would 
improve performance. But without reverting,
the algorithm is still good, \viz\ comparable
to the performance of a cousin of the same size
without repetitions.

Let us look at an implementation of \ensuremath{\Varid{mergesort}}
that exploits order in the input in this sense.
First, we need a function that merges the runs,
that is a merge for a list of lists.
Let us call this function \ensuremath{\Varid{multimerge}}:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{24}{@{}>{\hspre}c<{\hspost}@{}}%
\column{24E}{@{}l@{}}%
\column{27}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{multimerge}\mathbin{::}(\Conid{Ord}\;\Varid{a})\Rightarrow [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{multimerge}\;[\mskip1.5mu \mskip1.5mu]{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[3]{}\Varid{multimerge}\;[\mskip1.5mu \Varid{xs}\mskip1.5mu]{}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\Varid{xs}{}\<[E]%
\\
\>[3]{}\Varid{multimerge}\;(\Varid{x}\mathbin{:}\Varid{y}\mathbin{:}\Varid{zs}){}\<[24]%
\>[24]{}\mathrel{=}{}\<[24E]%
\>[27]{}\Varid{merge}\;(\Varid{merge}\;\Varid{x}\;\Varid{y})\;(\Varid{multimerge}\;\Varid{zs}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The function reduces a list of lists of \ensuremath{\Varid{a}}
to a plain list of \ensuremath{\Varid{a}}.
For the empty list, it yields the empty list.
For a list that contains only one single list,
it yields just that list.
For a list with more elements,
it merges the first two lists 
and merges the resulting list with the list
that results from \ensuremath{\Varid{multimerging}} the rest.
With an example that will become clearer.
But let us not use the list with two runs,
because that would not let us see the recursion
on \ensuremath{\Varid{multimerge}}. Instead, we use the non-optimal
list with four runs.
We would start with:

\ensuremath{\Varid{multimerge}\;[\mskip1.5mu [\mskip1.5mu \mathrm{5}\mskip1.5mu]\mathbin{:}[\mskip1.5mu \mathrm{2},\mathrm{4}\mskip1.5mu]\mathbin{:}[\mskip1.5mu [\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]}

and perform

\ensuremath{\Varid{merge}\;(\Varid{merge}\;[\mskip1.5mu \mathrm{5}\mskip1.5mu]\;[\mskip1.5mu \mathrm{2},\mathrm{4}\mskip1.5mu])\;(\Varid{multimerge}\;[\mskip1.5mu [\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu])},

which is 

\ensuremath{\Varid{merge}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{5}\mskip1.5mu]\;(\Varid{multimerge}\;[\mskip1.5mu [\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu])}

and results in the call to \ensuremath{\Varid{multimerge}}:

\ensuremath{\Varid{multimerge}\;[\mskip1.5mu [\mskip1.5mu \mathrm{3}\mskip1.5mu],[\mskip1.5mu \mathrm{1}\mskip1.5mu]\mskip1.5mu]}.

This reduces to

\ensuremath{\Varid{merge}\;(\Varid{merge}\;[\mskip1.5mu \mathrm{3}\mskip1.5mu]\;[\mskip1.5mu \mathrm{1}\mskip1.5mu])\;(\Varid{multimerge}\;[\mskip1.5mu \mskip1.5mu])},

which is

\ensuremath{\Varid{merge}\;[\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu]\;[\mskip1.5mu \mskip1.5mu]},

which, in its turn, is \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu]}. 
Going backwards, we obtain

\ensuremath{\Varid{merge}\;[\mskip1.5mu \mathrm{2},\mathrm{4},\mathrm{5}\mskip1.5mu]\;[\mskip1.5mu \mathrm{1},\mathrm{3}\mskip1.5mu]},

which is \ensuremath{[\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu]}.

We can now put everything together:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}c<{\hspost}@{}}%
\column{25E}{@{}l@{}}%
\column{28}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{mergesort}\mathbin{::}(\Conid{Ord}\;\Varid{a})\Rightarrow [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{mergesort}\;\Varid{l}\mathrel{=}{}\<[16]%
\>[16]{}\mathbf{let}\;{}\<[21]%
\>[21]{}\Varid{rs}{}\<[25]%
\>[25]{}\mathrel{=}{}\<[25E]%
\>[28]{}\Varid{runs}\;\Varid{l}{}\<[E]%
\\
\>[21]{}\Varid{n'}{}\<[25]%
\>[25]{}\mathrel{=}{}\<[25E]%
\>[28]{}\Varid{length}\;\Varid{l}{}\<[E]%
\\
\>[21]{}\Varid{n}{}\<[25]%
\>[25]{}\mathrel{=}{}\<[25E]%
\>[28]{}\mathbf{if}\;\Varid{even}\;\Varid{n'}\;\mathbf{then}\;\Varid{n'}\;\mathbf{else}\;\Varid{n'}\mathbin{+}\mathrm{1}{}\<[E]%
\\
\>[16]{}\hsindent{1}{}\<[17]%
\>[17]{}\mathbf{in}\;{}\<[21]%
\>[21]{}\mathbf{if}\;\Varid{length}\;\Varid{rs}\mathbin{>}(\Varid{n}\mathbin{\Varid{`div`}}\mathrm{2})\mathbin{+}\mathrm{1}{}\<[E]%
\\
\>[21]{}\mathbf{then}\;\Varid{multimerge}\mathbin{\$}\Varid{runs}\;(\Varid{reverse}\;\Varid{l}){}\<[E]%
\\
\>[21]{}\mathbf{else}\;\Varid{multimerge}\;\Varid{rs}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}
\ignore{$}

We first create the runs for the input list \ensuremath{\Varid{l}}.
We then compute the length of \ensuremath{\Varid{l}} \ensuremath{\Varid{n'}}.
If \ensuremath{\Varid{n'}} is not even, we add one,
just to be sure, we later refer to at least the half of \ensuremath{\Varid{n'}}.
Then, if the length of the runs is more than half of \ensuremath{\Varid{n}} plus 1,
then we run \ensuremath{\Varid{multimerge}} on the runs of the reversed input list,
otherwise, we run \ensuremath{\Varid{multimerge}} on the runs of \ensuremath{\Varid{l}}.

There is a well-known mathematical concept 
that is closely related to the conept of runs:
the \term{Eulerian numbers}, which, as the Stirling numbers,
come in two flavours ingeniously called 
Eulerian numbers of the first kind and
Eulerian numbers of the second kind.

The Eulerian numbers of the first kind,
denoted by $\eulerianOne{n}{m}$, count
the number of permutations of a set of $n$ 
distinct elements where $m$ numbers are greater
than the previous element.
They, hence, do not count the number of runs
directly. 
They, first, use the number of elements 
that are actually member of a run without the first 
element of that run (which is a stepdown).
This, actually, is the value of $m$,
which counts, in other words, how many of the elements
are not stepdowns.
They, second, count the number of permutations
that have $m$ elements in such a configuration.
For instance, the set $\lbrace 1,2,3,4,5\rbrace$
has $5! = 120$ permutations.
There is exactly one permutation where 4 elements
are greater than their predecessor, namely 
the permutation $1,2,3,4,5$.
Hence: $\eulerianOne{5}{4} = 1$.
There is also only one permutation with no element
greater than its predecessor, namely $5,4,3,2,1$.
Hence: $\eulerianOne{5}{0} = 1$.
There are 26 permutations with only 3 elements
greater than their predecessor
and 66 with only 2 elements greater than their
predecessor.

You perhaps see immediately that Eulerian numbers
can be used to compute the average running time
of \ensuremath{\Varid{mergesort}} for input of a given size $n$.
The Eulerian numbers allow us to compute
the probability of that input having
1 run, 2 runs, $\dots$, $n$ runs.
For the example above, we have the probabiliy
$\frac{1}{5! = 120}$ that there is no stepdown at all
and that we, hence, do not have to do anything;
we have the probability $\frac{26}{120} = \frac{13}{60}$
that we have to do only one merge step and
the probability of $\frac{66}{120} = \frac{11}{20}$
that we have to do two merge steps and so on.
(Knuth provides an extensive analysis 
in the third volume of his masterpiece.)

Let us look at a smaller set, where we can actually
look at all permutations, say $\lbrace 1,2,3\rbrace$.
There is of course 1 permutation for 2 non-stepdowns
and also 1 for no non-stepdown at all: 
$\eulerianOne{3}{0} = \eulerianOne{3}{2} = 1$.
For 1 element greater than its predecessor,
there are $\eulerianOne{3}{1} = 4$ possibilities,
namley: $213, 231, 132, 312$.
The sum of all values is of course the number
of all permutations, \ie\ $3! = 6$:
$\eulerianOne{3}{0} + \eulerianOne{3}{1} + \eulerianOne{3}{2} 
= 1 + 4 + 1 = 6$.

Here are some values arranged in -- 
oh, you guessed it already:

\begin{tabular}{l c c c c c c c c c c c c c c c}
1 &   &   &    &    &    &    &     &  1 &     &    &    &    &    &   &   \\
2 &   &   &    &    &    &    &   1 &    &   1 &    &    &    &    &   &   \\
3 &   &   &    &    &    &  1 &     &  4 &     &  1 &    &    &    &   &   \\
4 &   &   &    &    &  1 &    &  11 &    &  11 &    &  1 &    &    &   &   \\
5 &   &   &    &  1 &    & 26 &     & 66 &     & 26 &    &  1 &    &   &   \\
6 &   &   &  1 &    & 57 &    & 302 &    & 302 &    & 57 &    &  1 &   &   \\   
7 &   & 1 &    &120 &    &1191&     &2416&     &1191&    &120 &    & 1 &   
\end{tabular}

Make sure yourself that, for each line $n$, the following identity holds:

\begin{equation}
n! = \sum_{m=0}^{n-1}{\eulerianOne{n}{m}}
\end{equation}

There is a recursive formula 
to compute the Eulerian numbers, which is

\begin{equation}
\eulerianOne{n}{m} = (n-m) \eulerianOne{n-1}{m-1}
                   + (m+1) \eulerianOne{n-1}{m},
\end{equation}

with $\eulerianOne{0}{m} = \eulerianOne{n}{0} = \eulerianOne{n}{n-1} = 1$.
In Haskell, this can be implemented as:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}c<{\hspost}@{}}%
\column{32E}{@{}l@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{36}{@{}>{\hspre}l<{\hspost}@{}}%
\column{61}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{eulerian1}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{eulerian1}\;\mathrm{0}\;\anonymous {}\<[18]%
\>[18]{}\mathrel{=}{}\<[21]%
\>[21]{}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{eulerian1}\;\anonymous \;\mathrm{0}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[21]%
\>[21]{}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{eulerian1}\;\Varid{n}\;\Varid{m}{}\<[18]%
\>[18]{}\mid \Varid{m}\equiv \Varid{n}\mathbin{-}\mathrm{1}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[35]{}\mathrm{1}{}\<[E]%
\\
\>[18]{}\mid \Varid{otherwise}{}\<[32]%
\>[32]{}\mathrel{=}{}\<[32E]%
\>[36]{}(\Varid{n}\mathbin{-}\Varid{m})\mathbin{*}\Varid{eulerian1}\;(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{m}\mathbin{-}\mathrm{1}){}\<[E]%
\\
\>[32]{}\mathbin{+}{}\<[32E]%
\>[36]{}(\Varid{m}\mathbin{+}\mathrm{1})\mathbin{*}\Varid{eulerian1}\;(\Varid{n}\mathbin{-}\mathrm{1})\;{}\<[61]%
\>[61]{}\Varid{m}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

There is also a closed form to compute
the $n$th Eulerian number and this closed form
reveals a relation of the Eulerian numbers
with the binomial coefficients:

\begin{equation}
\eulerianOne{n}{m} = \sum_{k=0}^m{(-1)^k\binom{n+1}{k}(m+1-k)^n}.
\end{equation}

This expands into a series where the results of the products
$\binom{n+1}{k}(m+1-k)^n$ are alternately added or subtracted.
For instance for $\eulerianOne{5}{2}$:

\begin{equation}
\eulerianOne{5}{2} = (-1)^0 \times \binom{6}{0} \times (2+1-0)^5 +  
                     (-1)^1 \times \binom{6}{1} \times (2+1-1)^5 +
                     (-1)^2 \times \binom{6}{2} \times (2+1-2)^5,
\end{equation}

which is 

\begin{align*}
  &&  1  && \times && 3^5 &&   &&             &&=&&  243\\ 
- &&  6  && \times && 2^5 && = && 6 \times 32 &&=&&  192\\
+ && 15  && \times && 1^5 &&   &&             &&=&&   15,
\end{align*}

hence: $243 - 192 + 15 = 66$.

It perhaps helps to get the closed form right
to look at it in Haskell:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{49}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{eu1closed}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{eu1closed}\;\Varid{n}\;\Varid{m}{}\<[18]%
\>[18]{}\mathrel{=}\Varid{go}\;\mathrm{0}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{go}\;\Varid{k}{}\<[18]%
\>[18]{}\mathrel{=}{}\<[21]%
\>[21]{}\mathbf{let}\;{}\<[26]%
\>[26]{}\Varid{a}\mathrel{=}(\mathbin{-}\mathrm{1})\mathbin{\uparrow}\Varid{k}{}\<[E]%
\\
\>[26]{}\Varid{b}\mathrel{=}\Varid{choose}\;(\Varid{n}\mathbin{+}\mathrm{1})\;\Varid{k}{}\<[E]%
\\
\>[26]{}\Varid{c}\mathrel{=}(\Varid{m}\mathbin{+}\mathrm{1}\mathbin{-}\Varid{k})\mathbin{\uparrow}\Varid{n}{}\<[E]%
\\
\>[21]{}\mathbf{in}\;(\Varid{a}\mathbin{*}\Varid{b}\mathbin{*}\Varid{c})\mathbin{+}\mathbf{if}\;\Varid{k}\equiv \Varid{m}\;{}\<[49]%
\>[49]{}\mathbf{then}\;\mathrm{0}{}\<[E]%
\\
\>[49]{}\mathbf{else}\;\Varid{go}\;(\Varid{k}\mathbin{+}\mathrm{1}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The Eulerian numbers of the second kind 
are as well quite interesting. They deal
with \term{multisets}, that is sets with
repeated elements and, as such, they
may pave the way to a theory for real world input.
In concrete, they count the number of permutations
of a multiset with $n$ different elements
with $m$ ascents, where an ascent occurs
whenever a number is greater than its predecessor.
The concept of an ascent, hence, is stronger
than that of a non-stepdown. A non-stepdown
would include a number that equals its predecessor,
but that is not an ascent.

To understand Eulerian numbers of the second kind,
we first have to understand how many permutations
there are for multisets. It is not the factorial,
but the \term{doublefactorial}, also called the
\term{semifactorial}, of $2n-1$, where $n$ is the number of unique
elements in the multiset.
For instance, the multiset $\lbrace 1,1,2,2,3,3\rbrace$
has $n=3$ unique elements, namely 1, 2 and 3.
The doublefactorial of $2n-1 = 5$, denoted by $n!!$, is 15.

The doublefactorial of $n$ is the product of all numbers
$1\dots n$ that have the same parity as $n$.
If $n$ is even, we multiply all even numbers $2\dots n$,
otherwise, if $n$ is odd, we multiply the odd numbers
$1\dots n$. In Haskell, this may look like this:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{doublefac}\mathbin{::}\Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{doublefac}\;\mathrm{0}{}\<[16]%
\>[16]{}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{doublefac}\;\mathrm{1}{}\<[16]%
\>[16]{}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{doublefac}\;\Varid{n}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{n}\mathbin{*}\Varid{doublefac}\;(\Varid{n}\mathbin{-}\mathrm{2}){}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

For instance, $5!! = 1 \times 3 \times 5 = 15$
and $6!! = 2 \times 4 \times 6 = 48$.

The permutations of the multiset $\lbrace 1,1,2,2,3,3\rbrace$ are

332211,\\
221133, 221331, 223311, 233211, 113322, 133221, 331122, 331221,\\
112233, 122133, 112332, 123321, 133122, 122331.

In the first line, there is one permutation
with no ascent,
in the second line, there are 8 permutations
with one ascent and
in the third line, there are 6 permutations
with two ascents.
The Eulerian numbers of the second kind,
denoted by $\eulerianTwo{n}{m}$, for a multiset
with $n=3$ different elements, thus, are:
$\eulerianTwo{3}{0} = 1$,
$\eulerianTwo{3}{1} = 8$ and
$\eulerianTwo{3}{2} = 6$.
Here are some values arranged in 
the last triangle you will see for some time:

\begin{tabular}{l c c c c c c c c c c c c c c}
1 &   &    &    &    &    &     &  1  &     &     &    &     &    &    &   \\
2 &   &    &    &    &    &   1 &     &   2 &     &    &     &    &    &   \\
3 &   &    &    &    &  1 &     &  8  &     &  6  &    &     &    &    &   \\
4 &   &    &    &  1 &    &  22 &     &  58 &     & 24 &     &    &    &   \\
5 &   &    &  1 &    & 52 &     &328  &     &444  &    & 120 &    &    &   \\
6 &   &  1 &    &114 &    &1452 &     &4400 &     &3708&     &720 &    &   \\   
7 & 1 &    &240 &    &5610&     &32120&     &58140&    &33984&    &5040&   
\end{tabular}

Again, this triangle shows some interesting
properties. The sum of each row $n$ is of course
the doublefactorial of $2n-1$, \eg\
$\eulerianTwo{3}{0} + \eulerianTwo{3}{1} + \eulerianTwo{3}{2} = 5!! = 15$ and
$\eulerianTwo{4}{0} + \eulerianTwo{4}{1} + \eulerianTwo{4}{2} + \eulerianTwo{4}{3} =
7!! = 105$.
Neatly, the oughter right-hand diagonal
shows the factorials. The last value of each row,
$\eulerianTwo{n}{n-1}$, hence, is $n!$.

Here comes the formula to compute the Eulerian numbers
of the second kind recursively:

\begin{equation}
\eulerianTwo{n}{m} = (2n-m-1) \eulerianTwo{n-1}{m-1} 
                   + (m+1)    \eulerianTwo{n-1}{m}, 
\end{equation}

with $\eulerianTwo{0}{0} = 1$ and $\eulerianTwo{0}{m} = 0$.
In Haskell this is:

\begin{minipage}{\textwidth}\begingroup\par\noindent\advance\leftskip\mathindent\(
\begin{pboxed}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}c<{\hspost}@{}}%
\column{20E}{@{}l@{}}%
\column{24}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}c<{\hspost}@{}}%
\column{31E}{@{}l@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{45}{@{}>{\hspre}l<{\hspost}@{}}%
\column{52}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{eulerian2}\mathbin{::}\Conid{Natural}\to \Conid{Natural}\to \Conid{Natural}{}\<[E]%
\\
\>[3]{}\Varid{eulerian2}\;\mathrm{0}\;\mathrm{0}{}\<[18]%
\>[18]{}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[3]{}\Varid{eulerian2}\;\mathrm{0}\;\anonymous {}\<[18]%
\>[18]{}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[3]{}\Varid{eulerian2}\;\Varid{n}\;\Varid{m}{}\<[18]%
\>[18]{}\mathrel{=}(\mathrm{2}\mathbin{*}\Varid{n}\mathbin{-}\Varid{m}\mathbin{-}\mathrm{1}){}\<[31]%
\>[31]{}\mathbin{*}{}\<[31E]%
\>[34]{}\Varid{eulerian2}\;{}\<[45]%
\>[45]{}(\Varid{n}\mathbin{-}\mathrm{1})\;(\Varid{m}\mathbin{-}\mathrm{1}){}\<[E]%
\\
\>[18]{}\hsindent{2}{}\<[20]%
\>[20]{}\mathbin{+}{}\<[20E]%
\>[24]{}(\Varid{m}\mathbin{+}\mathrm{1}){}\<[31]%
\>[31]{}\mathbin{*}{}\<[31E]%
\>[34]{}\Varid{eulerian2}\;{}\<[45]%
\>[45]{}(\Varid{n}\mathbin{-}\mathrm{1})\;{}\<[52]%
\>[52]{}\Varid{m}{}\<[E]%
\ColumnHook
\end{pboxed}
\)\par\noindent\endgroup\resethooks
\end{minipage}

The Eulerian numbers, as many other things in mathematics,
are named after Leonhard Euler (1707 -- 1783),
a Swiss mathematician and one of the most important
mathematicians of all time.
He is certainly the most important mathematician of his own time,
and, for sure, the most productive one ever.
He turned his family into a kind of math factory
that produced thousands of papers in 
number theory, analysis, mechanics, optics,
astronomy, ballistics and even music theory.
He is also regarded as the founder of graph theory
and topology.
Modern terminology and notation is strongly based
on Euler. He proved many theorems and 
made even more conjectures.
But he was also a great math teacher,
as his \term{Letters to a German Princess}
show in which he lectured on mathematical subjects
to non-mathematicians.




\end{document}
