\ignore{
We are used to think of numbers
in terms of the \term{number system}
we have learnt, that is in terms of sequences
composed of basic symbols of the form
$\lbrace 0,1,2,\dots,9\rbrace$.
But such symbols are of course
mere signs that refer to other entities.
Otherwise, it would be hard to understand
why operations on these symbols work like
``$2 + 3 = 5$'' and not like
``$2 + 3 = 23$'' or ``3 + 2 = 32''.
}

In his famous article ``What are numbers and what should they be?''
German mathematician Richard Dedekind (1831 -- 1916)
described numbers as ``free creations of the human mind''.
This idea is anything but self-evident.
Dedekind's contemporary and fellow combatant
for a new approach in math, namely set theory,
Georg Cantor 
fiercely defended \term{mathematical platonism},
the idea that mathematical objects 
are real existing and that mathematical methods
are tools that allow us to see these objects like,
perhaps, telescopes, enable us to examine 
stars far away in the universe.

This view clashed with the view of the \term{constructivist}
Leopold Kronecker, a resolute opponent
to the novelties introduced by Cantor, Dedekind and others.
In the view of the constructivists like Kronecker
only those objects that can be constructed
in a finite number of steps from a finite number
of integers are valid objects of mathematics.
Anything else is a chimaera.
``The natural numbers are made by god'', as he put it, 
``all else is the work of man.''

Both views, constructivism and platonism,
even though not mainlines of discussion anymore,
are still influential today.
The influence of platonism is visible, 
when it is said that
some mathematician has ``discovered'' a concept,
\eg\ ``Gauss discovered a construction of the heptadecagon''.
Mathematicians in the tradition of constructivism or its
modern variant \term{intuitivism}, insist in a terminology
that stresses that mathematical concepts are man-made.
They would say Gauss has ``invented'',
``developed'' or, simply, ``constructed''.

It is consensus, however, that, independent
of the question whether numbers are real existing things
or invented concepts,
there must be some kind of representation of numbers
in our mind.
It is also likely that these representations
have some grounding in the nervous system
and are thus part of our genetic legacy.
A strong evidence therefor is the fact
that some animals can distinguish sets
of different numbers of objects.
Birds, for instance, realise when an egg
is missing in their nest,
but do not necessarily realise
that an egg was replaced by another one.
That would suggest
that the ``creation'' of numbers 
is not as ``free'' as it appears to be
according to Dedekind.
The ability to work with numbers 
would then be part of our biology,
just like having language capability
is part of our biology.

When we try to reduce numbers to the very heart
of what they are, we quickly come to the notion
of counting. 
The assumption that numbers basically represent
steps in the process of counting
is supported by the fact
that many number systems in use during history
are modelled on something countable
like fingers (leading to the decimal number system), 
fingers and toes (leading to the vigesimal system), 
finger bones (leading to systems based on multiples of 12)
and so on.
According to this view,
relations between quantities (length, volume, weight, \etc)
would represent more abstract concepts engineered in some way
on top of the fundamental notion of counting.
It is not so clear, however, 
if this hierarchy corresponds to the real historic development.
Among the oldest known math problems
are such as determining the quanitity of corn for baking bread
in acient Egypt
or investigating rhythmic patterns formed by single and multiple beats.
Problems of this kind cannot be solved
by mere counting, but demand comparison of \term{continuous} quantities
like volume, weight or time intervals
leading immediately to fractions.
Numbers may hence be related to a human sense of quantity
that is more general than counting.

Numbers appear to be very simple and clear concepts.
But, in fact, together with the operations defined on them,
addition, multiplication, subtraction and division,
they quickly give rise to intricate problems.
Those problems are not like add-ons to the numbers
that were invented later to complicate things,
they belong to the very core of the number concept.
It appears, in fact, that those problems
have already slumbered in the 
apparently simple concept,
but needed some time
to unfold themselves --
centuries in some cases, millennia in others.
Even if the number concept may have been
created by the human mind,
once fixed, all its consequences and complications are there.
In this sense,
every mathematician would agree to speak of
``discovering'' a problem in a certain field of math.
Mathematicians may still disagree, however, on whether 
the solution to such a problem
was ``discovered'' or ``developed''.

The most notorious group of long-standing problems 
is number theory, which deals mainly with
questions of prime numbers, \ie\ natural numbers
that cannot be constructed from other natural numbers
using multiplication.
Prime numbers are investigated for some three thousand years now,
but, still, many fundamental questions remain unanswered.
Infamous are the number-theoretic theorems drafted,
but not proven, by Pierre de Fermat in the $17^{th}$ century.
Most of these theorems look quite innocent, 
but by proving them in the course of the following three centuries,
math made huge progress developing methods 
that had been unknown to Fermat and his contemporaries.

The most insistent problem, known 
as \term{Fermat's Last Theorem}, was solved only
in the late $20^{th}$ century
by British mathematician Andrew Wiles.
What makes this specific problem so mind-boggling
is its extremely simple formulation -- compared to the
proof of some 200 pages. 
Indeed, Fermat doodled the theorem
on the margin of a page in his copy of an acient math book,
Diophantos' ``Arithmetica'', and, 
to the irritation of generations of mathematicians,
he added: ``I have found a really wonderful proof,
but, unfortunately,
this margin is too narrow to contain it.''
The theorem in question states
that there is no solution for 
equations of the form: $a^z + b^z = c^z$
for $a,b,c$ and $z$ all natural numbers and $z > 2$.
For $z = 2$, many solutions exist,
the so called \term{Pythagorean triples},
\eg: $3^2 + 4^2= 5^2$.

Another set of difficult problems is algebra
and the search for solutions of
higher-degree equations, which
caused a lot of headache throughout the centuries.
The general solution for quadratic equations
of the form $ax^2 + bx + c = 0$
was known since antiquity.
Solutions for cubic and quartic equations
were found by joint efforts and fierce
competition in the $16^{th}$ century by Italian mathematicians,
namely Niccolò Fontana Tartaglia, 
Gerolamo Cardano and Lodovico Ferrari. 
But it took until the early $19^{th}$ century,
before mathematicians were able to make statements
about quintic and even higher-degree equations.
In fact, the investigations took an unpredicted direction
and led to the development of a new branch of mathematics,
\term{group theory}, 
which studies sets of objects -- not only numbers --
and operations defined on these objects,
and \term{abstract algebra},
which generalises group theory 
making objects of completely different areas of mathematics comparable.

Group theory, which was developed during the $18^{th}$ and $19^{th}$
century by Joseph-Louis Lagrange, Paolo Ruffini,
young Niels Henrik Abel and the incredible 
Évariste Galois, is a great helper
in organising the number zoo
that developed out of simpler notions of numbers
in the course of the time.
At the beginning -- if we believe in numbers
being fundamentally related to counting -- 
there may have been only the 
concept of natural numbers,
which was then extended by adding fractions
to cope with relations between quantities
and negative numbers to deal with negative quantities
like debts.
The investigations into geometry by Greek mathematicians
led to the rise of irrational numbers -- 
such as the number $\pi$ --
which, as a surprise to acient mathematicians, 
cannot be expressed in terms of ratios of integers.
The studies in algebra led to complex numbers,
which, in their turn, inspired 
the construction of hypercomplex numbers.

In this chapter, we will start 
with the simplest possible number concept,
natural numbers, and we will stick to it
as long as possible -- perhaps longer.
We then will make a big leap introducing
negative numbers and fractions.
Afterwards we enter the confusing world
of irrational numbers and will probably understand
the annoyance of mathematicians like Kronecker
when faced with mathematical concepts dealing
with these strange creatures.
Later, complex numbers and
even more exotic beasts will come
into focus.
On the way, we will introduce some basic group theory 
helping us to find our way in the number jungle.
