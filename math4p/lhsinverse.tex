%% =======================================================
%% (c) Tobias Schoofs
%% =======================================================
%% Math 4 Programmers - The Inverse Element
%% =======================================================

% Plain Style
\documentclass[tikz]{scrreprt}

% Springer Style
%\documentclass[envcountsame]{llncs}

%
%
\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
  {\@namedef{lhs2tex.lhs2tex.sty.read}{}%
   \newcommand\SkipToFmtEnd{}%
   \newcommand\EndFmtInput{}%
   \long\def\SkipToFmtEnd#1\EndFmtInput{}%
  }\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%

\include{cmds}

\begin{document}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\chapter{The Inverse Element} % c05
\section{Inverse Elements}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Invel}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}
\end{tabbing}
}

In a group, any element $a$ has an inverse $a'$,
such that $a \cdot a' = e$, where $e$ is
the neutral element of that group.
We have already met some groups, permutation groups
and finite groups of modular arithmetic.
We will now look at infinite groups, namely 
the additive group over $\mathbb{Z}$, the integers, and
the multiplicative group over $\mathbb{Q}$, the rationals.

As you may have guessed already,
the inverse element of a natural
number $n$ in the additive group over integers 
is the \term{negation} of $n$, $-n$.
It is easy to see that $n$ plus its inverse,
$-n$, is just $0$, the neutral element of addition:
$n + (-n) = n - n = 0$.
Since it is, of course, also true
that $-n + n = n - n = 0$,
the inverse element of a negative number $-n$
is its positive counterpart $n$.
With the law of double negation, $-(-n) = n$,
that is the negation of the negation of $n$ is $n$,
the formula $n + (-n) = 0$ 
is universally true for both
positive and negative numbers.
When we are talking about $n$, the positive,
we just have : $n + (-n) = n - n = 0$.
When we are talking about $-n$, the negative,
we have: $-n + (-(-n)) = -n + n = n - n = 0$.

Note that with this identity, we can solve
any equation of the form
$a + x = b$ in the additive group of $\mathbb{Z}$.
We just add the inverse of $a$ to both sides
of the equation: $a + (-a) + x = b + (-a)$,
which, of course, is $x = b - 1$.
Without negative numbers, there were equations
we could not solve in this way, for instance
$4 + x = 3$. We had gaps, so to speak,
in our additive equations. But now,
in the group over integers, there are no such
gaps anymore: we just add the inverse of 4
on both sides and get $x = 3 - 4 = -1$.

What is the inverse element of integers
in the multiplicative group?
Well, in this group, it must still hold
that $a \cdot a' = e$, where $\cdot$ is multiplication
and $e$, the neutral element, is unity.
We, hence, have $a \times a' = 1$.
We easily find a solution with division.
We divide $a$ on both sides and get $a' = \frac{1}{a}$
and that is the answer: 
the inverse element of a natural number $n$
in the multiplicative group
is $\frac{1}{n}$.

We see that we now can solve
any multiplicative equation of the form
$ax = b$, just by multiplying the inverse
of $a$ on both sides: $ax\frac{1}{a} = b \frac{1}{a}$,
which is $x\frac{a}{a} = \frac{b}{a}$.
The left-hand side reduces to $1x = x$
and we have $x = \frac{b}{a}$.
As we have already seen in finite groups,
prime numbers simply ``disappear'' with fractions,
since we can now reach them with multiplication,
for instance: $3x = 5$ is equivalent to
$x = \frac{5}{3}$.

Let us go on and build something bigger: a field
consisting of addition and multiplication
where the distributive law holds.
We already know that the distributive law
holds in the world of natural numbers:
$a(b + c) = ab + ac$.
But what, when we have creatures like $-n$ and 
$\frac{1}{n}$?
We obviously need rules to add fractions
and to negate products.

Let us start with fractions.
We may add fractions with the same denominator
by simply adding the numerators, \ie\
$\frac{a}{n} + \frac{b}{n} = \frac{a+b}{n}$.
With different denominators,
we first have to manipulate the fractions in a way
that they have the same denominator, but without 
changing their value.
The simplest way to do that
is to multiply one denominator by the other
and to multiply the numerators correspondingly:
$\frac{a}{n} + \frac{b}{m} = \frac{am}{nm} + \frac{bn}{mn} =$
$\frac{am+bn}{mn}$.

We can reduce the computational complexity
of this operation in most cases by using
the $lcm$ instead of the product $nm$.
As you may remember,
the $lcm$ of two numbers, $a$ and $b$ is 
$\frac{ab}{\gcd(a,b)}$.
We would still multiply the numerator 
by the value by which the denominator changes,
\ie\ the $lcm$ divided by the denominator and would get

\begin{equation}
\frac{a}{n} + \frac{b}{m} = 
\frac{a \times \frac{lcm(n,m)}{n} + b \times \frac{lcm(n,m)}{m}}{lcm(n,m)}.
\end{equation}

That looks complicated, but is simple
when we take two concrete numbers:
$\frac{1}{6} + \frac{2}{9}$.
We first compute the $lcm(6,9)$
as 

\[
\frac{6 \times 9}{\gcd(6,9)}.
\]

The $\gcd$ of 6 and 9 is 3,
the product $6 \times 9$ = 54
and $54 / 3 = 18$.
So, we have:

\[
\frac{1 \times \frac{18}{6} + 2 \times \frac{18}{9}}{lcm(6,9) = 18} =
\frac{3 + 4}{18} = \frac{7}{18}.
\]

It looks simpler now, but still it seems
that we do need more steps than by just
multiplying the respective other denominator
to numerator and denominator of both fractions.
However, when we do this, we have to operate
with greater numbers and, at the end, reduce
the fractions to their \term{canonical form}, which is

\begin{equation}
\frac{a}{b} = \frac{a/\gcd(a,b)}{b/\gcd(a,b)}.
\end{equation}

For the example, this would mean

\[
\frac{1 \times 9}{6 \times 9} + \frac{2 \times 6}{9 \times 6} =
\frac{9}{54} + \frac{12}{54} = \frac{21}{54}.
\]

Now, we reduce to canonical form:

\[
\frac{21/\gcd(21,54)}{54/\gcd(21,54)}, 
\]

which is 

\[
\frac{\frac{21}{3} = 7}{\frac{54}{3} = 18}. 
\]

How to multiply negative numbers of the form $-a(b + c)$?
Let us look at an example:
the additive inverse of 6 is $-6$.
We would therefore expect $-6 + 6 = 0$.
Furthermore, we have $2 \times 3 = 6$.
Now, if we add the inverse of 3 to 3 once,
we get 0:  
$-3 + 3 = 0$.
What should we get, if we add the
inverse of 3 twice to twice 3,
\ie\ $2 \times 3 + 2 \times -3$?
We expect it to be 0, correct?
Therefore and since $2 \times 3$ is 6,
$2 \times -3$ must be the inverse
of 6, hence, $-6$.

The same is true, the other way round,
thus $2 \times 3 + (-2) \times 3 = 0$.
That means that we can move the minus sign
in a product, such that $a \times -b = -a \times b$.
To any such product we can simply add the factor 1
without changing the result:
$a \times b = 1 \times a \times b$.
We, therefore, have
$1 \times a \times -b = 1 \times -a \times b = -1 \times a \times b$.
This facilitates life a bit: we can handle one minus sign
as the additional factor $-1$.
In other words, multiplying by $-1$ has the same effect
has negating: $-1n = -n$.

Going back to the question of how to handle products of the form
$-a(b + c)$, we now can say that $-a(b + c) = -1a(b+c)$.
Multiplying $a$ out in terms of the distributive law,
we get: $-1(ab + ac)$. Now, we multiply $-1$, just as we did above:
$(-1)ab + (-1)ac$ and, since we know that $-1n = -n$,
we derive $-ab - ac$.

What if we have more than one minus sign in a product
like, for instance:  $-2 \times -3$?
We saw above that
$2 \times -3 + 2 \times 3 = 0$
and we can reformulate this as
$-1 \times 2 \times 3 + 2 \times 3 = 0$.
Now, if we have two negative factors,
we add one more minus sign, \ie\ one more factor $-1$:
$-1 \times -1 \times 2 \times 3 + 2 \times 3 = ?$
We just substitute one $-1$ after the other
by negation. We first get
$-1 \times -(2 \times 3)\dots$ and then 
$-(-(2\times 3))$.
We now see clearly that this should be the negation,
\ie\ the inverse of $-(2 \times 3)$.
The inverse of $-(2 \times 3)$, however, is just
$2 \times 3$ and that is 6.
Therefore: $-2 \times -3 + 2 \times 3 = 12$.

So, what happens if we multiply negative numbers
and fractions? 
When we just follow multiplication rules
we get $-1 \times \frac{1}{n} = \frac{-1}{n}$.
We, hence, would say, according to the rules
derived above, that $\frac{-1}{n}$ is the 
additive inverse of $\frac{1}{n}$. 
What about $\frac{1}{-n}$?
This should be the multiplicative inverse of $-n$,
such that $-n \times \frac{1}{-n} = 1$.
We again can move the minus sign around
to get $-1 \times n \times \frac{1}{-n}$ 
leading to $-1 \times \frac{n}{-n} = 1$.
Dividing $n$ on both sides gives 
$-1 \times \frac{1}{-n} = \frac{1}{n}$ and
multiplying $-1$ gives $\frac{1}{-n} = \frac{-1}{n}$.
In other words,
a fraction with a minus sign in it,
independently of where it appears,
in the numerator or the denominator,
is the additive inverse of the same fraction
without the minus sign:
$\frac{a}{b} + \frac{-a}{b} = \frac{a}{b} + \frac{a}{-b} =$
$\frac{a}{b} - \frac{a}{b} = 0$.

In summary, we can define a set of rules
on multiplication that are independent of whatever
$a$ and $b$ are:

\begin{align*}
 a && \times &&  b && = && ab\\
-a && \times && -b && = && ab\\
-a && \times &&  b && = && -ab\\
 a && \times && -b && = && -ab
\end{align*}

With this, we have established an important theoretical 
result, namely that $\mathbb{Q}$, including negative numbers,
is an infinite field with addition and multiplication.
But let us go on. 
There are still operations we have seen
for natural numbers combining multiplication and addition
that may and should have an interpretation with integers
and fractions, namely exponentiation.

From the table above, we see immediately
that products with an even number of negative
numbers are positive -- a fact that we already
used when discussing prime numbers.
In general, any number raised to an
even exponent is positive independent of that number
being positive or negative.

This leads to a difficulty with the 
root operation, since even roots may have
two different results: a positive number
or its additive inverse. For instance, $\sqrt{4}$
could be 2 and $-2$. 
Even further, the operation cannot be applied
to a negative number: $\sqrt{-1}$ has no
meaning -- at least not with the creatures
we have met so far. There may be an object $i$
that fulfil equations of the form $i = \sqrt{-1}$,
but such objects are beyond our imagination
at this point in time.

Now, what is the effect of having negative numbers or fractions
in the exponent?

We will first look at fractions as exponents and
investigate powers of the form
$a^{\frac{1}{n}}$. To clarify the meaning of such expression,
we will use the rules we know so far to observe what happens,
when we multiply two powers with the same base and with fractional exponents:

\begin{equation}\label{eq_invelfracExp1}
x = a^{\frac{1}{n}} \times a^{\frac{1}{m}} = a^{\frac{1}{n} + \frac{1}{m}}.
\end{equation}

Let us look at the special case of the exponent $\frac{1}{2}$:

\begin{equation}
x = a^{\frac{1}{2}} \times a^{\frac{1}{2}} = a^{\frac{1}{2} + \frac{1}{2}}.
\end{equation}

Obviously, $\frac{1}{2} + \frac{1}{2} = 2 \times \frac{1}{2} = 1$.
In other words, $x$, in this case, 
is just $a$. Furthermore, we see 
that there is a number $n = a^{\frac{1}{2}}$, 
such that $n \times n = n^2 = x$.
For which number does this hold?
Well, it is just the definition of the square root:
$\sqrt{x} \times \sqrt{x} = (\sqrt{x})^2 = x$. 
We would conclude that $a^{\frac{1}{n}}$ is equivalent to
$\sqrt[n]{a}$:

\begin{equation}
a^{\frac{1}{n}} = \sqrt[n]{a}.
\end{equation}

This, in fact, makes a lot of sense.
We would expect, for instance, that 
$(a^{\frac{1}{n}})^n$ is just $a$,
since $(\sqrt[n]{a})^n = a$.
When we multiply it out, we get indeed
$a^{\frac{1}{n} \times n} = a^1 = a$.
We would also expect that $(a^{\frac{1}{n}})^{\frac{1}{m}}$
is $\sqrt[m]{\sqrt[n]{a}} = \sqrt[mn]{a}$,
\eg\ $(a^{\frac{1}{2}})^{\frac{1}{2}} = \sqrt{\sqrt{a}} = \sqrt[4]{a}$.
When we multiply it out again, we indeed obtain
$a^{\frac{1}{2} \times \frac{1}{2}} = a^{\frac{1}{4}}$.

Now, what about negative exponents?
We adopt the same technique, \ie\ we multiply 
two powers with the same base:

\[
a \times a^{-1}.
\]

We can write this as $a^1 \times a^{-1}$
and this is

\[
a^1 \times a^{-1} = a^{1-1} = a^0 = 1.
\]

We see $a^{-1}$ is the multiplicative inverse of $a$.
But we already know that the inverse of $a$
is $\frac{1}{a}$. We conclude that

\begin{equation}
a^{-n} = \frac{1}{a^n}.
\end{equation}

This conjecture would imply
that $a^{-n} \times a^n = 1$, since
$\frac{1}{a^n} \times a^n = 1$
and, indeed: $a^{-n} \times a^n = a^{-n + n} = a^0 = 1$.
It would also imply that 
$(a^{-n})^{-1} = a^n$,
since $a^{-n} = \frac{1}{a^n}$,
whose inverse is $a^n$.
Indeed, we have 
$(a^{-n})^{-1} = a^{-n \times -1} = a^n$.

A side effect of this rule is
that we now have a very nice notation 
for the multiplicative inverse. Until now, we have used
the symbol $a'$ to denote the inverse of $a$.
Since $'$ is also used in other contexts,
the notation $a^{-1}$ is much clearer
and we will stick to it from now on.
\section{$\mathbb{Z}$}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Zahl}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~qualified~{\itshape {\itshape Data}.Ratio}~as~{\itshape R}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}\\
\texfamily ~~{\bfseries import}~{\itshape {\itshape Debug}.Trace}~(trace)
\end{tabbing}
}

We will now implement a number type
that takes signedness into account.
We will do so in a way that allows us
to negate objects of different kind,
basically any type of number.
We therefore start by defining a
parametrised data type:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries data}~{\itshape Signed}~a~=~{\itshape Pos}~a~|~{\itshape Neg}~a~\\
\texfamily ~~~~{\bfseries deriving}~({\itshape Eq},{\itshape Show})
\end{tabbing}
\end{minipage}

The data type has two constructors,
\text{\texfamily {\itshape Pos}} and \text{\texfamily {\itshape Neg}} for a positive and a negative
\text{\texfamily a} respectivley.
The expression \text{\texfamily {\bfseries let}\Sp x\Sp =\Sp {\itshape Neg}\Sp 1}
would assign the value $-1$ to \text{\texfamily x}.
We would instantiate a concrete data type
by giving a concrete type for the type parameter,
\eg\:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries type}~{\itshape Zahl}~=~{\itshape Signed}~{\itshape Natural}
\end{tabbing}
\end{minipage}

This type is called \term{Zahl},
the German word for \term{number},
which was used for the designation of the
set $\mathbb{Z}$ of the integers.
When Abstract Algebra started to be
a major field of mathematics,
the University of Göttingen was 
the gravitational centre of the math world
and, since it was not yet common to use English
in scientific contributions, many
German words slipped into math terminology.

For convenience, we add a direct
conversion from \text{\texfamily {\itshape Zahl}} to \text{\texfamily {\itshape Natural}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~z2n~::~{\itshape Zahl}~\char'31~{\itshape Natural}\\
\texfamily ~~z2n~({\itshape Pos}~n)~=~n\\
\texfamily ~~z2n~({\itshape Neg}~\char95 )~=~undefined
\end{tabbing}
\end{minipage}

Another convenience function should be defined
for \text{\texfamily {\itshape Neg}}, namely to guarantee that 0 is always
positive. Otherwise, we could run into situations
where we compare \text{\texfamily {\itshape Pos}\Sp 0} and \text{\texfamily {\itshape Neg}\Sp 0} and obtain
a difference that does not exist.
We therefore define

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~neg0~::~({\itshape Eq}~a,~{\itshape Num}~a)~=>~a~\char'31~{\itshape Signed}~a\\
\texfamily ~~neg0~0~=~{\itshape Pos}~0\\
\texfamily ~~neg0~x~=~{\itshape Neg}~x
\end{tabbing}
\end{minipage}

We now make \text{\texfamily {\itshape Signed}} an instance
of \text{\texfamily {\itshape Ord}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Ord}~a)~=>~{\itshape Ord}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~compare~({\itshape Pos}~a)~~({\itshape Pos}~b)~~=~compare~a~b\\
\texfamily ~~~~compare~({\itshape Neg}~a)~~({\itshape Neg}~b)~~=~compare~b~a\\
\texfamily ~~~~compare~({\itshape Pos}~\char95 )~~({\itshape Neg}~\char95 )~~=~{\itshape GT}\\
\texfamily ~~~~compare~({\itshape Neg}~\char95 )~~({\itshape Pos}~\char95 )~~=~{\itshape LT}
\end{tabbing}
\end{minipage}

The difficult cases are implemented in the
first two lines. When \text{\texfamily a} and \text{\texfamily b} have
the same sign, we need to compare \text{\texfamily a} and
\text{\texfamily b} themselves to decide which number 
is greater than the other.
If the sign differs, we can immediately decide
that the one with the negative sign 
is smaller. 

\text{\texfamily {\itshape Signed}} is also an instance of \text{\texfamily {\itshape Enum}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Enum}~a)~=>~{\itshape Enum}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~toEnum~i~~|~i~\char'35~0~~=~{\itshape Pos}~~\$~toEnum~i\\
\texfamily ~~~~~~~~~~~~~~|~i~<~0~~~=~{\itshape Neg}~~\$~toEnum~i\\
\texfamily ~~~~fromEnum~({\itshape Pos}~a)~~~~=~fromEnum~a\\
\texfamily ~~~~fromEnum~({\itshape Neg}~a)~~~~=~negate~(fromEnum~a)
\end{tabbing}
\end{minipage}

With this definition some more semantics
comes in. We explicitly define that,
converting an integer greater or equal to zero,
we use the \text{\texfamily {\itshape Pos}} constructor; converting
an integer less than zero, we use
the \text{\texfamily {\itshape Neg}} constructor.
Furthermore, when we convert in the opposite direction,
\text{\texfamily {\itshape Pos}\Sp a} is just an \text{\texfamily a}, whereas \text{\texfamily {\itshape Neg}\Sp a}
is the negation of \text{\texfamily a}.

Now we come ot arithmetic, first addition:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Ord}~a,~{\itshape Num}~a)~=>~{\itshape Num}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~({\itshape Pos}~a)~~+~~({\itshape Pos}~b)~~=~{\itshape Pos}~~(a~+~b)\\
\texfamily ~~~~({\itshape Neg}~a)~~+~~({\itshape Neg}~b)~~=~neg0~(a~+~b)\\
\texfamily ~~~~({\itshape Pos}~a)~~+~~({\itshape Neg}~b)~~|~a~>~~~b~~~=~{\itshape Pos}~~(a~-~b)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~|~a~<~~~b~~~=~{\itshape Neg}~~(b~-~a)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~|~a~\char'36~~b~~~=~{\itshape Pos}~~0\\
\texfamily ~~~~({\itshape Neg}~a)~~+~~({\itshape Pos}~b)~~|~a~>~~~b~~~=~{\itshape Neg}~~(a~-~b)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~|~a~<~~~b~~~=~{\itshape Pos}~~(b~-~a)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~|~a~\char'36~~b~~~=~{\itshape Pos}~~0
\end{tabbing}
\end{minipage}

The addition of two positive numbers
is a positive sum.
The addition of two negative numbers
($-a + (-b) = -a - b$) is a negative sum.
The addition of a negative and a positive number
results in a difference, which may be positive
or negative depending on which number is greater:
the negative or the positive one.

We define subtraction in terms
of addition:
subtracting a positive number \text{\texfamily b} from any number \text{\texfamily a}
is the same as adding the negation of \text{\texfamily b} to \text{\texfamily a}.
Vice versa, subtracting a negative number \text{\texfamily b}
is the same as adding a postive number.

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~~~a~-~({\itshape Pos}~b)~=~a~+~(neg0~b)\\
\texfamily ~~~~a~-~({\itshape Neg}~b)~=~a~+~({\itshape Pos}~~b)
\end{tabbing}
\end{minipage}

Multiplication:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~~~({\itshape Pos}~a)~~*~~({\itshape Pos}~b)~~=~{\itshape Pos}~(a~*~b)\\
\texfamily ~~~~({\itshape Neg}~a)~~*~~({\itshape Neg}~b)~~=~{\itshape Pos}~(a~*~b)\\
\texfamily ~~~~({\itshape Pos}~0)~~*~~({\itshape Neg}~\char95 )~~=~{\itshape Pos}~0\\
\texfamily ~~~~({\itshape Pos}~a)~~*~~({\itshape Neg}~b)~~=~{\itshape Neg}~(a~*~b)\\
\texfamily ~~~~({\itshape Neg}~\char95 )~~*~~({\itshape Pos}~0)~~=~{\itshape Pos}~0\\
\texfamily ~~~~({\itshape Neg}~a)~~*~~({\itshape Pos}~b)~~=~{\itshape Neg}~(a~*~b)
\end{tabbing}
\end{minipage}

This is a straight forward implementation
of the rules we have already seen above:
the product of two positive numbers is positive;
the product of two negative numbers is positive;
the product of a positive and a negative number is negative.

The next method is \text{\texfamily negate}.
There is one minor issue we have
to handle: what do we do if the number is 0?
In this case, we assume the number is
positive. But that is a mere convention.
Without this convention, we would have
to introduce a constructor for 0 that is
neither postive nor negative.

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~~~negate~({\itshape Pos}~a)~~|~signum~a~\char'36~0~~=~{\itshape Pos}~a\\
\texfamily ~~~~~~~~~~~~~~~~~~~~|~otherwise~~~~~~=~{\itshape Neg}~a\\
\texfamily ~~~~negate~({\itshape Neg}~a)~~~~=~{\itshape Pos}~a
\end{tabbing}
\end{minipage}

Finally, we have \text{\texfamily abs}, \text{\texfamily signum} and
\text{\texfamily fromInteger}. There is nothing new
in the implementation of these methods:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~~~abs~~~~({\itshape Pos}~a)~~~~=~{\itshape Pos}~a\\
\texfamily ~~~~abs~~~~({\itshape Neg}~a)~~~~=~{\itshape Pos}~a\\
\texfamily ~~~~signum~({\itshape Pos}~a)~~~~=~{\itshape Pos}~(signum~a)\\
\texfamily ~~~~signum~({\itshape Neg}~a)~~~~=~{\itshape Neg}~(signum~\$~negate~a)\\
\texfamily ~~~~fromInteger~i~~~~~|~i~\char'35~~0~~=~{\itshape Pos}~(fromInteger~i)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~|~i~<~~~0~~=~{\itshape Neg}~(fromInteger~\$~abs~i)
\end{tabbing}
\end{minipage}

We make \text{\texfamily {\itshape Signed}} an instance of \text{\texfamily {\itshape Real}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Real}~a)~=>~{\itshape Real}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~toRational~({\itshape Pos}~i)~=~toRational~i\\
\texfamily ~~~~toRational~({\itshape Neg}~i)~=~negate~(toRational~i)
\end{tabbing}
\end{minipage}

We also make \text{\texfamily {\itshape Signed}} an instance of \text{\texfamily {\itshape Integral}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Enum}~a,~{\itshape Integral}~a)~=>~{\itshape Integral}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~quotRem~({\itshape Pos}~a)~~({\itshape Pos}~b)~~=~{\bfseries let}~(q,r)~~=~quotRem~a~b~{\bfseries in}~({\itshape Pos}~~q,~{\itshape Pos}~~r)\\
\texfamily ~~~~quotRem~({\itshape Neg}~a)~~({\itshape Neg}~b)~~=~{\bfseries let}~(q,r)~~=~quotRem~a~b~{\bfseries in}~({\itshape Pos}~~q,~neg0~r)\\
\texfamily ~~~~quotRem~({\itshape Pos}~a)~~({\itshape Neg}~b)~~=~{\bfseries let}~(q,r)~~=~quotRem~a~b~{\bfseries in}~(neg0~q,~{\itshape Pos}~~r)~\\
\texfamily ~~~~quotRem~({\itshape Neg}~a)~~({\itshape Pos}~b)~~=~{\bfseries let}~(q,r)~~=~quotRem~a~b~{\bfseries in}~(neg0~q,~neg0~r)~\\
\texfamily ~~~~toInteger~({\itshape Pos}~a)~~=~toInteger~a\\
\texfamily ~~~~toInteger~({\itshape Neg}~a)~~=~negate~(toInteger~a)
\end{tabbing}
\end{minipage}

The implementation of \text{\texfamily toInteger} contains nothing new.
But have a look at the definition of \text{\texfamily quotRem}.
The first case, where both numbers are positive,
is easy: we return a positive quotient and a positive remainder.
When both numbers are negative,
the quotient is postive and the remainder is negative.
Indeed, when we have the equation $a = qb + r$
and both, $a$ and $b$, are negative,
then a postive $q$ will bring $b$ close to $a$.
With $a=-5$ and $b=-2$, for instance, the quotient would be 2:
$2 \times -2 = -4$. Now, what do we have to add to $-4$ 
to reach -5? Obviously, $-1$. Therefore the remainder
must be negative.

Now, when we have a postive $a$ and a negative $b$,
for instance: $a=5$ and $b=-2$; then a negative quotient
will bring us close to $a$: $-2 \times -2 = 4$.
Missing now is the positive remainder 1.
Finally, when $a$ is negative and $b$ is positive,
we need a negative quotient, \eg: $a=-5$ and $b=2$;
then $-2 \times 2 = -4$. Missing, in this case,
is again a negative remainder, namely $-1$.

In the future, there may arise the need to
make number types signed that do no fit
into the classes from which we derived
\text{\texfamily {\itshape Signed}} so far. In particular,
a signed fraction should inherit from Fractional.
We therefore make \text{\texfamily {\itshape Signed}} an instance of Fractional:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~({\itshape Eq}~a,~{\itshape Ord}~a,~{\itshape Fractional}~a)~=>~{\itshape Fractional}~({\itshape Signed}~a)~{\bfseries where}\\
\texfamily ~~~~({\itshape Pos}~a)~/~({\itshape Pos}~b)~=~{\itshape Pos}~(a~/~b)\\
\texfamily ~~~~({\itshape Neg}~a)~/~({\itshape Neg}~b)~=~{\itshape Pos}~(a~/~b)\\
\texfamily ~~~~({\itshape Pos}~0)~/~({\itshape Neg}~b)~=~{\itshape Pos}~(0~/~b)\\
\texfamily ~~~~({\itshape Pos}~a)~/~({\itshape Neg}~b)~=~{\itshape Neg}~(a~/~b)\\
\texfamily ~~~~({\itshape Neg}~a)~/~({\itshape Pos}~b)~=~{\itshape Neg}~(a~/~b)
\end{tabbing}
\end{minipage}
\section{Negative Binomial Coefficients}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape NegBinom}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Zahl}
\end{tabbing}
}

The aim of this section is to generalise
binomial coefficients to integers using
the new data type \text{\texfamily {\itshape Zahl}}.
There are many ways how to approach this goal,
which perhaps can be grouped into two main
approaches: first, we can look for an
application of such a generalisation in
the ``real world'' and search an appropriate
mathematical tool for this problem.
We started with binomial coefficients
by discussing combinatorial issues,
but, of course, combinatorial problems
like the ways to choose $k$ objects out of $n$
provide no meaningful interpretation for
negative numbers -- what should a negative
number of possibilities mean?
But there are other applications,
such as the multiplication of sums.

The second basic approach is not to look
for applications, but to investigate
the formalism asking ``what happens,
when we change it?''
This may appear much less interesting
at the first sight, since there is no
obvious use case for such an altered
formalism. However, such formal approaches
do not only help deepening the insight
into specific mathematical problems,
but they also lead to new tools, which
may find their applications in the future.
This has happened more than once in the
history of mathematics.
When David Hilbert, the champion of the
formalistic approach, redefined geometry
extending it from the two-dimensional plane
and the three-dimensional space to 
$n$-dimensional manifolds, there was no
concrete application in sight.
It took only a short while, however,
before John von Neumann and others started using
the concepts of Hilbert's geometry to model
quantum physics.

Anyhow, we start with the second approach.
Still, there are many ways to go.
We can start with the formula $\binom{n}{k}$
and ask ourselves what happens if
$n<0$ or $k<0$.
To begin with, let us assume
$n<0$ and $k\ge 0$.
If we just apply the formula
$\frac{n(n-1)\dots (n-k+1)}{k!}$,
we get for $\binom{-n}{k}$
a kind of falling factorial in the numerator
that looks like a rising factorial with minus signs
in front of the numbers, \eg\ $\binom{-6}{3}$ is
$\frac{-6 \times -7 \times -8}{6}$,
which is $-1 \times -7 \times -8 = -56$.
Obviously, the signedness of the result
depends on whether $k$ is even or odd.
Since there are $k$ negative factors 
in the numerator, the product will be positive
if $k$ is even and negative otherwise.

Here is a Haskell implementation
for this version of negative binomial
coefficients:

\ignore{
\begin{tabbing}\texfamily
~~toTheKfalling~::~({\itshape Num}~n,{\itshape Integral}~n)~=>~n~\char'31~n~\char'31~n\\
\texfamily ~~toTheKfalling~n~k~=~go~n~k\\
\texfamily ~~~~{\bfseries where}~~go~m~0~=~m\\
\texfamily ~~~~~~~~~~~go~m~i~=~m~*~go~(m-1)~(i-1)\\
\texfamily \\
\texfamily ~~toTheKrising~~::~({\itshape Num}~n,{\itshape Integral}~n)~=>~n~\char'31~n~\char'31~n\\
\texfamily ~~toTheKrising~n~k~=~go~n~k\\
\texfamily ~~~~{\bfseries where}~~go~m~0~=~m\\
\texfamily ~~~~~~~~~~~go~m~i~=~m~*~go~(m+1)~(i-1)\\
\texfamily \\
\texfamily ~~{\bfseries infix}~>|\\
\texfamily ~~(>|)~::~({\itshape Num}~n,{\itshape Integral}~n)~=>~n~\char'31~n~\char'31~n\\
\texfamily ~~(>|)~=~toTheKfalling\\
\texfamily \\
\texfamily ~~{\bfseries infix}~|>\\
\texfamily ~~(|>)~::~({\itshape Num}~n,{\itshape Integral}~n)~=>~n~\char'31~n~\char'31~n\\
\texfamily ~~(|>)~=~toTheKrising~
\end{tabbing}
}

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~chooseNeg~::~{\itshape Zahl}~\char'31~{\itshape Natural}~\char'31~{\itshape Zahl}\\
\texfamily ~~chooseNeg~n~k~~|~n~\char'35~0~\char'04~k~\char'35~0~~=~~{\itshape Pos}~(choose~(z2n~n)~k)\\
\texfamily ~~~~~~~~~~~~~~~~~|~n~\char'36~k~~~~~~~~~~~~=~~1\\
\texfamily ~~~~~~~~~~~~~~~~~|~k~\char'36~0~~~~~~~~~~~~=~~1\\
\texfamily ~~~~~~~~~~~~~~~~~|~k~\char'36~1~~~~~~~~~~~~=~~n\\
\texfamily ~~~~~~~~~~~~~~~~~|~otherwise~~~~~~~~~=~~(n~>|~({\itshape Pos}~k))~`div`~(fac~({\itshape Pos}~k))
\end{tabbing}
\end{minipage}

This function accepts two arguments,
one of type \text{\texfamily {\itshape Zahl}} and the other of type \text{\texfamily {\itshape Natural}}.
For the moment, we want to avoid negative $k$s
and to rule negative values out right from the beginning,
we choose \text{\texfamily {\itshape Natural}} as data type for $k$.

When both, $n$ and $k$, are positive,
we just use the old \text{\texfamily choose} converting $n$ to \text{\texfamily {\itshape Natural}}.
Then we handle the trivial cases.
Finally, we just implement one of the formulas for
binomial coefficients.
Here are some values:

\text{\texfamily map\Sp (chooseNeg\Sp (-1))\Sp [0..9]}\\
\text{\texfamily [1,-1,1,-1,1,-1,1,-1,1,-1]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg\Sp (-2))\Sp [0..9]}\\
\text{\texfamily [1,-2,3,-4,5,-6,7,-8,9,-10]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg\Sp (-3))\Sp [0..9]}\\
\text{\texfamily [1,-3,6,-10,15,-21,28,-36,45,-55]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg\Sp (-4))\Sp [0..9]}\\
\text{\texfamily [1,-4,10,-20,35,-56,84,-120,165,-220]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg\Sp (-5))\Sp [0..9]}\\
\text{\texfamily [1,-5,15,-35,70,-126,210,-330,495,-715]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg\Sp (-6))\Sp [0..9]}\\
\text{\texfamily [1,-6,21,-56,126,-252,462,-792,1287,-2002]}

We see that the coefficients for each $n$
alternate between positive and negative values
depending on $k$ being even or odd.
We also see that there is no limit anymore
from which on the coefficients are all zero.
In the original definition,
we had $\binom{n}{k}=0$ for $k>n$.
But now we have to give up that rule,
because for $n<0$ and $k\ge 0$,
$k>n$ trivially holds for all cases.
In consequence, we lose the nice symmetry
we had in the original triangle.
Of course, we can restore many of the old
characteristics by changing the definition
of \text{\texfamily chooseNeg} for negative $n$s to
\text{\texfamily (n\Sp |>\Sp ({\itshape Pos}\Sp k))\Sp `div`\Sp (fac\Sp ({\itshape Pos}\Sp k))}.
In this variant, let us call it \text{\texfamily chooseNeg2},
we use the rising factorial,
such that the absolute values of the numbers 
in the numerator equal the numbers in the numerator
for $n\ge 0$.
For instance:

\text{\texfamily map\Sp (chooseNeg2\Sp (-2))\Sp [0..9]}\\
\text{\texfamily [1,-2,1,0,0,0,0,0,0,0]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg2\Sp (-3))\Sp [0..9]}\\
\text{\texfamily [1,-3,3,-1,0,0,0,0,0,0]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg2\Sp (-4))\Sp [0..9]}\\
\text{\texfamily [1,-4,6,-4,1,0,0,0,0,0]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg2\Sp (-5))\Sp [0..9]}\\
\text{\texfamily [1,-5,10,-10,5,-1,0,0,0,0]}\\[12pt]
\text{\texfamily map\Sp (chooseNeg2\Sp (-6))\Sp [0..9]}\\
\text{\texfamily [1,-6,15,-20,15,-6,1,0,0,0]}

The first solution, \text{\texfamily chooseNeg}, however,
is more faithful to the original definition
of the binomial coefficients,
even if its results do not resemble the original results.
One of the characteristics that is preserved
is Pascal's rule:

\begin{equation}
  \binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}.
\end{equation}

Indeed, we could use Pascal's rule
to find negative coefficients in the first place.
If we are interested in the coefficient
$\binom{n-1}{k}$, we just subtract
$\binom{n-1}{k-1}$ from both sides and get

\begin{equation}\label{eq:PascalNegativeN}
  \binom{n-1}{k} = \binom{n}{k} - \binom{n-1}{k-1}.
\end{equation}

We can use this equation to search the unknown
territory of negative coefficients starting
from the well-known territory of positive coefficients
using each result as a base camp for further
expeditions.
We start with $\binom{0}{0}$ and want to know
the value for $\binom{-1}{0}$.
Since coefficients for $k=0$ are defined as 1,
this case turns out to be trivial:
$\binom{-1}{0} = 1$.
The next is $\binom{-1}{1}$, which is
$\binom{0}{1} - \binom{-1}{0}$, hence
$0 - 1 = -1$. The next is 
$\binom{-1}{2} = \binom{0}{2} - \binom{-1}{1}$, which is
$0 - (-1) = 0 + 1 = 1$.
Next: $\binom{-1}{3} = \binom{0}{3} - \binom{-1}{2}$,
which is $0 - 1 = -1$ and so on.
Indeed, the coefficients of $-1$, as we have seen before,
are just alternating
positive and negative 1s:

\text{\texfamily map\Sp (chooseNeg\Sp (-1))\Sp [0..9]}\\
\text{\texfamily [1,-1,1,-1,1,-1,1,-1,1,-1]}

On the basis of this result,
we can investigate the coefficients of $-2$.
We know that $\binom{-2}{0}$ is 1 and continue with
$\binom{-2}{1}$, which is $\binom{-1}{1} - \binom{-2}{0}$,
which is $-1 - 1 = -2$.
The next is $\binom{-2}{2} = \binom{-1}{2} - \binom{-2}{1}$
or $1 - (-2) = 1 + 2 = 3$.
We continue with $\binom{-2}{3} = \binom{-1}{3} - \binom{-2}{2}$,
which is $-1 - 3 = -4$.
It turns out that the coefficients for $n=-2$ are

\text{\texfamily map\Sp (chooseNeg\Sp (-2))\Sp [0..9]}\\
\text{\texfamily [1,-2,3,-4,5,-6,7,-8,9,-10]},

which is a beautiful result.
If we go on this way, we will reproduce the values observed above
using \text{\texfamily chooseNeg}.

Now, what about negative $k$s?
There is no direct way to implement something like \text{\texfamily chooseNeg}
for negative $k$s, because we do not know what
a negative factorial would mean.
Of course, we can apply a trick 
very similar to that we used for \text{\texfamily chooseNeg2},
\ie\ we compute the factorial not as
\text{\texfamily fac\Sp n\Sp =\Sp n\Sp *\Sp fac\Sp (n-1)}, but as
\text{\texfamily fac\Sp n\Sp =\Sp n\Sp *\Sp fac\Sp (n+1)} going up towards zero.
In this case, we also have to take care of
the rising or falling factorial of the numerator.
For instance, we can use 
the falling factorial with the inverse of $k$,
such that the value of the numerator remains
the same independent of $k$ being positive or
negative. The result would resemble that
of \text{\texfamily chooseNeg2}, \ie\ we would have
alternating positive and negative coefficients
for $n > 0$ and positive coefficients for $n < 0$.

More interesting is using Pascal's rule
to create coefficients for negative $k$s.
But we have to be careful.
In equation \ref{eq:PascalNegativeN},
we used a coefficient for $k-1$
to find the coefficient for $k$.
This will not work. When we look for
$\binom{n}{-k}$, we do not yet know 
the value for $\binom{n}{-(k+1)}$,
since we are entering the territory
of negative numbers from above.
Therefore, we need a variant of 
equation \ref{eq:PascalNegativeN}.
Instead of subtracting $\binom{n-1}{k-1}$
from Pascal's rule, we subtract the other
term $\binom{n-1}{k}$ and get

\begin{equation}\label{eq:PascalNegativeK}
\binom{n-1}{k-1} = \binom{n}{k} - \binom{n-1}{k}.
\end{equation}

It is obvious that any coefficient
resulting from a positive $n$
and a negative $k$ in this way equals 0,
since any coefficient with $k=0$ is 1.
So, $\binom{n}{-1} = 1 - 1 = 0$.
However, we also have $\binom{0}{k} = 0$
for any $k$ and $\binom{n}{n} = 1$.
For $\binom{-1}{-2}$, we therefore have
$\binom{0}{-1} - \binom{-1}{-1} = 0 - 1 = -1$.
For $\binom{-1}{-3}$, we get
$0 - \binom{-1}{-2} = 0 - (-1) = 0 + 1 = 1$.
The next coefficient $\binom{-1}{-4}$,
froreseeably, is $0 - \binom{-1}{-3}$,
which is $0 - 1 = -1$.
Again, for $n=-1$, we get a sequence
of alternating negative and positive ones.

For $\binom{-2}{k}$, we get 
$\binom{-2}{-2} = 1$, $\binom{-2}{-3}$
is $\binom{-1}{-2} - \binom{-2}{-2} = -1 - 1 = -2$.
$\binom{-2}{-4}$ then is 
$\binom{-1}{-3} - \binom{-2}{-3} = 1 - (-2) = 3$.
The next coefficient is 
$\binom{-2}{-5} = \binom{-1}{-4} - \binom{-2}{-3}$,
which is $-1 - 3 = -4$ and so on.
The binomial coefficients for -2 with negative $k$s,
thus, is just the sequence $1,-2,3,-4,5,-6,7,-8,9,-10,\dots$,
which we have already seen for positive $k$s above.
The symmetry of the triangle, hence, is reinstalled.
For coefficients with $n=-2$
and $k=-9,-8,\dots,-1,0,1,\dots,7$, 
we get the sequence

$-8,7,-6,5,-4,3,-2,1,0,1,-2,3,-4,5,-6,7,-8$.

To confirm this result, we implement the backward
rule as

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~pascalBack~::~{\itshape Zahl}~\char'31~{\itshape Zahl}~\char'31~{\itshape Zahl}~\\
\texfamily ~~pascalBack~n~0~=~1\\
\texfamily ~~pascalBack~n~k~~|~k~\char'36~n~~~~~=~1\\
\texfamily ~~~~~~~~~~~~~~~~~~|~k~\char'36~0~~~~~=~1\\
\texfamily ~~~~~~~~~~~~~~~~~~|~n~\char'36~0~~~~~=~0\\
\texfamily ~~~~~~~~~~~~~~~~~~|~n~>~~0~\char'04~k~<~0~~~=~0\\
\texfamily ~~~~~~~~~~~~~~~~~~|~n~>~~0~\char'04~k~\char'35~0~~=~{\itshape Pos}~(choose~(z2n~n)~(z2n~k)\\
\texfamily ~~~~~~~~~~~~~~~~~~|~k~<~~0~~~~~=~pascalBack~(n+1)~(k+1)~~-~pascalBack~n~(k+1)\\
\texfamily ~~~~~~~~~~~~~~~~~~|~otherwise~~=~pascalBack~(n+1)~k~~~~~~-~pascalBack~n~(k-1)
\end{tabbing}
\end{minipage}

We first handle the trivial cases
$n=k$, $k=0$ and $n=0$.
When $n > 0$ and $k < 0$, the coefficient is 0.
For $n$ and $k$ both greater 0,
we use \text{\texfamily choose}.
For $k<0$, we use the rule in equation \ref{eq:PascalNegativeK}
and for $n<0$, with $k>0$, we use the rule in
equation \ref{eq:PascalNegativeN}.
Now we map \text{\texfamily pascalBack} for specific $n$s to a range of $k$s:

\text{\texfamily map\Sp (pascalBack\Sp (-1))\Sp [-9,-8..9]}\\
\text{\texfamily [1,-1,1,-1,1,-1,1,-1,1,1,-1,1,-1,1,-1,1,-1,1,-1]}\\[12pt]
\text{\texfamily map\Sp (pascalBack\Sp (-2))\Sp [-9,-8..9]}\\
\text{\texfamily [-8,7,-6,5,-4,3,-2,1,0,1,-2,3,-4,5,-6,7,-8,9,-10]}\\[12pt]
\text{\texfamily map\Sp (pascalBack\Sp (-3))\Sp [-9,-8..9]}\\
\text{\texfamily [28,-21,15,-10,6,-3,1,0,0,1,-3,6,-10,15,-21,28,-36,45,-55]}\\[12pt]
\text{\texfamily map\Sp (pascalBack\Sp (-4))\Sp [-9,-8..9]}\\
\text{\texfamily [-56,35,-20,10,-4,1,0,0,0,1,-4,10,-20,35,-56,84,-120,165,-220]}\\[12pt]
\text{\texfamily map\Sp (pascalBack\Sp (-5))\Sp [-9,-8..9]}\\
\text{\texfamily [70,-35,15,-5,1,0,0,0,0,1,-5,15,-35,70,-126,210,-330,495,-715]}

The symmetry of positive and negative $k$s is not perfect.
The coefficients for $n<0$ and $n < k < 0$ are all 0.
The sequence, in consequence, is mirrowed
with a delay of $|n|-1$ $k$s, such that
the coefficient that corresponds to the coefficient $\binom{n}{k}$
is not $\binom{n}{-k}$, but rather $\binom{n}{n-k}$.
For instance: 
$\binom{-2}{4} = 5 = \binom{-2}{-6}$,
$\binom{-3}{5} = -21 = \binom{-3}{-8}$ and
$\binom{-5}{2} = 15 = \binom{-5}{-7}$.

Looking at the numbers of negative $n$s,
I have the strange feeling that I already
saw those sequences somewhere.
But where? These are definitely not the rows
of Pascal's triangle, but perhaps something else?
Let us look at the triangle once again:

\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
0 &   &   &   &   &    &    &    &    &     &  1 &     &    &    &    &    &   &   &   &   &  \\
1 &   &   &   &   &    &    &    &    &   1 &    &   1 &    &    &    &    &   &   &   &   &  \\
2 &   &   &   &   &    &    &    &  1 &     &  2 &     &  1 &    &    &    &   &   &   &   &  \\
3 &   &   &   &   &    &    &  1 &    &   3 &    &   3 &    &  1 &    &    &   &   &   &   &  \\
4 &   &   &   &   &    &  1 &    &  4 &     &  6 &     &  4 &    &  1 &    &   &   &   &   &  \\
5 &   &   &   &   &  1 &    &  5 &    &  10 &    &  10 &    &  5 &    &  1 &   &   &   &   &  \\   
6 &   &   &   & 1 &    &  6 &    & 15 &     & 20 &     & 15 &    &  6 &    & 1 &   &   &   &  \\
7 &   &   & 1 &   &  7 &    & 21 &    &  35 &    &  35 &    & 21 &    &  7 &   & 1 &   &   &  \\
8 &   & 1 &   & 8 &    & 28 &    & 56 &     & 70 &     & 56 &    & 28 &    & 8 &   & 1 &   & \\
9 & 1 &   & 9 &   & 36 &    & 84 &    & 126 &    & 126 &    & 84 &    & 36 &   & 9 &   & 1
\end{tabular}

Now follow the diagonals from the upper vertex
left and right downwards.
On both sides you see first the 1s,
then the counting numbers $1,2,3,\dots$,
then the sequence we saw with $\binom{-3}{k}$,
then the sequence we saw with $\binom{-4}{k}$
and so on.
In other words, if we turn the triangle
by $90^{\circ}$ counterclockwise, 
we obtain the sequences for negative $n$;
the same also works with a clockwise turn,
but then we read the sequence from right to left.

This boils down to the equations

\begin{equation}\label{eq:NegPos_1}
\left\lvert\binom{-n}{1}\right\rvert 
= \binom{n}{1}
= \binom{n}{n-1}
\end{equation}

and

\begin{equation}\label{eq:NegPos_k}
\left\lvert\binom{-n}{k+1}\right\rvert 
= \binom{n+k-1}{k+1}
= \binom{n+k-1}{n-1}.
\end{equation}

For instance:

\begin{align*}
\left\lvert\binom{-3}{1}\right\rvert &&=&&\binom{3}{1} &&=&&\binom{3}{2} &&=&& 3\\
\left\lvert\binom{-3}{2}\right\rvert &&=&&\binom{4}{2} &&=&&\binom{4}{2} &&=&& 6\\
\left\lvert\binom{-3}{3}\right\rvert &&=&&\binom{5}{3} &&=&&\binom{5}{2} &&=&& 10\\
\left\lvert\binom{-3}{4}\right\rvert &&=&&\binom{6}{4} &&=&&\binom{6}{2} &&=&& 15\\
\left\lvert\binom{-3}{5}\right\rvert &&=&&\binom{7}{5} &&=&&\binom{7}{2} &&=&& 21\\
\left\lvert\binom{-3}{6}\right\rvert &&=&&\binom{8}{6} &&=&&\binom{8}{2} &&=&& 28
\end{align*}

This result may look a bit surprising at the first sight.
But when we look at the formula that actually generates
the value, it is obvious:

\begin{equation}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}.
\end{equation}

When we have a negative $n$, the falling factorial
in the numerator is in fact a rising factorial with
negative numbers:

\[
-n^{\underline{k}} = -n \times -(n+1) \times \dots \times -(n+k-1).
\]

Each number in the product is one less than its predecessor,
but, since the numbers are negative, the absolute value
is greater than its predecessor.
If we eliminate the minus signs, we obtain the rising factorial
for $n$:

\[
n^{\overline{k}} = n \times (n+1) \times \dots \times (n+k-1),
\]

which is just the falling factorial for $n+k-1$:

\[
(n+k-1)^{\underline{k}} = (n+k-1) \times (n+k-2) \times \dots \times n.
\]

We can therefore conclude that

\begin{equation}
\left\lvert\binom{-n}{k}\right\rvert = 
\frac{(n+k-1)^{\underline{k}}}{k!} = \binom{n+k-1}{k}.
\end{equation}

That $\binom{n+k-1}{k}$ also equals
$\binom{n+k-1}{n-1}$ results from the fact
that $n-1$ and $k$ maintain the same distance
from one of the sides of the triangle,
\ie\ from either $\binom{n+k-1}{0}$ or $\binom{n+k-1}{n+k-1}$.
$k$ is trivially $k$ coefficients away from any $\binom{n}{0}$,
whereas $n-1$ is $(n+k-1) - (n-1)$ away from $\binom{n+k-1}{n+k-1}$,
which is $n+k-1-n+1=k$. This is just an implication
of the triangle's symmetry.

Somewhat more difficult is to see the relation to Pascal's rule.
In fact, we have never proven that Pascal's rule follows
from the fraction $\frac{n^{\underline{k}}}{k!}$.
If we can establish this relation, the backward rule
will follow immediately.
So, we definitely should try to prove Pascal's rule.

We want to establish that

\begin{equation}
\binom{n+1}{k} = \binom{n}{k} + \binom{n}{k-1}
\end{equation}

and do this directly using the definitions

\begin{equation}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}
\end{equation}

\begin{equation}
\binom{n}{k-1} = \frac{n^{\underline{k-1}}}{(k-1)!}
\end{equation}

Our claim is that

\begin{equation}\label{eq:Pascal1}
\binom{n+1}{k} = 
\frac{n^{\underline{k}}}{k!} +
\frac{n^{\underline{k-1}}}{(k-1)!}
\end{equation}

In other words, when we can deduce
the left-hand side of this equation
from the right-hand side, we are done.
So let us just add the two fractions on 
the right-hand side.
We first convert them to a common denominator.
We can do this simply by multiplying
the second fraction by $k$:

\[
\frac{kn^{\underline{k-1}}}{k(k-1)! = k!}
\]

We can join the two fractions with the common
denominator and obtain a sum in the numerator:

\[
\frac{n^{\underline{k}} + kn^{\underline{k-1}}}{k!}
\]

If we extend the falling factorials 
in the numerator we get

\[
n(n-1)\dots(n-k+1) + n(n-1)\dots(n-(k-1)+1)k. 
\]

The terms have the same number of factors,
where the last factor in the first term is $n-k+1$
and the one in the second term is $k$.
The factor before the last in the second term
is $n-(k-1)+1$, which is $n-k+1+1 = n-k+2$,
and this term is also the last but second factor
in the first term.
In other words, the factors of the terms are equal
with the exception of the last factor.
We can factor the equal factors out.
If we do this stepwise, this looks like
(using brackets to indicate what remains within
the sum):

\begin{minipage}{\textwidth}
\[
n[(n-1)(n-2)\dots (n-k+1) + (n-1)(n-2)\dots (n-k+2)k]
\]
\[
n(n-1)[(n-2)\dots (n-k+1) + (n-2)\dots (n-k+2)k]
\]
\[
\dots
\]
\[
n(n-1)\dots(n-k+2)[n-k+1+k].
\]
\end{minipage}

The remaining sum can now be simplified:
$n-k+1+k = n+1$ and with this 
we recognise in the whole expression
the falling factorial of $n+1$.
The whole fraction is now 

\[
\frac{(n+1)^{\underline{k}}}{k!},
\]

which is the definition of the binomial coefficient
$\binom{n+1}{k}$ and that concludes the proof.\qed

The proof establishes the relation between
the definition of binomial coefficients and
Pascal's rule. This spares us from going
through the laborious task of establishing
the relation expressed in equation \ref{eq:NegPos_k}
using only Pascal's rule.

We now switch to the first approach mentioned 
at the beginning of this section,
\ie\ trying to find a mathematical
formalism for a practical problem.
The practical problem is multiplication.
We want to know what happens
with negative $a$s or $b$s in products of the form

\[
(a + b)^n.
\]

Negative $n$s are not too interesting here,
since $(a+b)^{-n}$ is just the inverse of
$(a+b)^n$, which is $\frac{1}{(a+b)^n}$,
without any effect on the coefficients themselves.

We could, of course go on by trying out
this formula with concrete numbers $a$ and $b$.
It appears much more promising, however, to choose a 
symbolic approach manipulating strings of the form
\text{\texfamily \char34 a\char34 } and \text{\texfamily \char34 b\char34 }.
The idea is to use string operations to simulate
multiplication and addition.
We do so in two steps: first we combine strings,
then we simplify them in a way mimicking the 
rules of addition and multiplication.
Since we want to see the differences between
positive and negative coefficients, 
we need a means to negate strings simulating
negative numbers.
To this end, we define the simple data type

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries data}~{\itshape Sym}~=~{\itshape P}~{\itshape String}~|~{\itshape N}~{\itshape String}\\
\texfamily ~~~~{\bfseries deriving}~({\itshape Eq},{\itshape Show})
\end{tabbing}
\end{minipage}

where, as you may have guessed,
the \text{\texfamily {\itshape P}}-constructor represents positive strings
and the \text{\texfamily {\itshape N}}-constructor represents negative strings.
We now combine two symbols to simulate
multiplication:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~comb~::~{\itshape Sym}~\char'31~{\itshape Sym}~\char'31~{\itshape Sym}\\
\texfamily ~~comb~~({\itshape P}~a)~~({\itshape P}~b)~~=~~{\itshape P}~~(a++b)\\
\texfamily ~~comb~~({\itshape P}~a)~~({\itshape N}~b)~~=~~{\itshape N}~~(a++b)\\
\texfamily ~~comb~~({\itshape N}~a)~~({\itshape P}~b)~~=~~{\itshape N}~~(a++b)\\
\texfamily ~~comb~~({\itshape N}~a)~~({\itshape N}~b)~~=~~{\itshape P}~~(a++b)
\end{tabbing}
\end{minipage}

You probably realise the pattern
we already used to define the number type \text{\texfamily {\itshape Zahl}}:
two numbers of equal signedness result in a positive number and
two numbers of different signedness result in a negative number.
Multiplication itself is just the concatenation of
the two strings.
\text{\texfamily comb\Sp ({\itshape P}\Sp \char34 a\char34 )\Sp ({\itshape P}\Sp \char34 b\char34 )}, hence, is \text{\texfamily {\itshape P}\Sp \char34 ab\char34 };
\text{\texfamily comb\Sp ({\itshape N}\Sp \char34 a\char34 )\Sp ({\itshape P}\Sp \char34 b\char34 )} is \text{\texfamily {\itshape N}\Sp \char34 ab\char34 }.

We represent addition as lists of strings.
We then can formulate the distributive law as

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~combN~::~{\itshape Sym}~\char'31~[{\itshape Sym}]~\char'31~[{\itshape Sym}]\\
\texfamily ~~combN~x~=~map~(comb~x)
\end{tabbing}
\end{minipage}

where we multiply a number, $x$, with a sum
by multiplying that number with each term of the sum,
mapping the basic combinator operation \text{\texfamily comb}
on the list representing the sum.
Based on \text{\texfamily combN}, we can now define the multiplication
of a sums by itself:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~combine~::~[{\itshape Sym}]~\char'31~[{\itshape Sym}]\\
\texfamily ~~combine~xs~=~concat~[combN~x~xs~|~x~\char'06~xs]
\end{tabbing}
\end{minipage}

Let us look at an example to get a grip on \text{\texfamily combine}:

\text{\texfamily {\bfseries let}\Sp a\Sp =\Sp {\itshape P}\Sp \char34 a\char34 }\\
\text{\texfamily {\bfseries let}\Sp b\Sp =\Sp {\itshape P}\Sp \char34 b\char34 }\\
\text{\texfamily combine\Sp [a,b]\Sp =\Sp concat\Sp [combN\Sp x\Sp [a,b]\Sp |\Sp x\Sp \char'06\Sp [a,b]]}.

The list comprehension will first apply \text{\texfamily combN\Sp a\Sp [a,b]},
resulting in \text{\texfamily [aa,ab]}, and then it will apply
\text{\texfamily combN\Sp b\Sp [a,b]} resulting in \text{\texfamily [ba,bb]}.
These two lists are then merged using \text{\texfamily concat}
resulting in \text{\texfamily [aa,ab,ba,bb]}.
We will later have to simplify this list,
since, as we know, \text{\texfamily ab} and \text{\texfamily ba} are equal
and can be written $2ab$.
We come back to this immediately,
but first we want to implement one more function,
namely a combinator that applies \text{\texfamily combine} $n$ times:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~combinator~::~[{\itshape Sym}]~\char'31~{\itshape Natural}~\char'31~[{\itshape Sym}]\\
\texfamily ~~combinator~~\char95 ~~0~=~[]\\
\texfamily ~~combinator~~xs~n~=~go~xs~(n-1)\\
\texfamily ~~~~{\bfseries where}~~go~ys~0~~=~~ys\\
\texfamily ~~~~~~~~~~~go~ys~n~~=~~go~(concat~[combN~x~ys~|~x~\char'06~xs])~(n-1)
\end{tabbing}
\end{minipage}

Note that this function does not reuse \text{\texfamily combine},
but implements it anew.
The reason is that we do not want to combine
the original input with itself,
but with the result of the previous recursion.
For instance, if $n=3$,
then we start with \text{\texfamily [combN\Sp x\Sp [a,b]\Sp |\Sp x\Sp \char'06\Sp [a,b]]}
resulting in \text{\texfamily [aa,ab,ba,bb]}.
In the next round, we multiply this result
by \text{\texfamily [a,b]}: \text{\texfamily [combN\Sp x\Sp [aa,ab,ba,bb]\Sp |\Sp x\Sp \char'06\Sp [a,b]]}
resulting in \text{\texfamily [aaa,aab,aba,abb,baa,bab,bba,bbb]}
and corresponding to the expression $(a+b)^3$.

Another interesting aspect of \text{\texfamily combinator}
is the base case for $n=0$,
which is just defined as being the empty list.
In this context, the empty list would, hence,
represent the number 1, since 1, as we know,
is the coefficient $\binom{0}{0}$.

Now we want to simplify results, such that
\text{\texfamily [aa,ab,ba,ba]\Sp =\Sp [aa,2*ab,ba]}.
First, we need to express that \text{\texfamily \char34 ab\char34 } and \text{\texfamily \char34 ba\char34 }
are the same thing.
Therefore, we sort the string:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~sortStr~::~{\itshape Sym}~\char'31~{\itshape Sym}\\
\texfamily ~~sortStr~({\itshape P}~a)~~=~{\itshape P}~~(sort~a)\\
\texfamily ~~sortStr~({\itshape N}~a)~~=~{\itshape N}~~(sort~a)
\end{tabbing}
\end{minipage}

leading to a canonical form for strings.
The call \text{\texfamily sortStr\Sp ({\itshape P}\Sp \char34 ba\char34 )} would result in
\text{\texfamily {\itshape P}\Sp \char34 ab\char34 }.

To simplify a complete list, we compare all
symbols in the lists and consolidate
symbols with equal strings resulting
in lists of the type \text{\texfamily ({\itshape Natural},{\itshape Sym})},
where the natural number counts the ocurrences
of that symbol in the list:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~simplify~::~[{\itshape Sym}]~\char'31~[({\itshape Natural},~{\itshape Sym})]\\
\texfamily ~~simplify~xs~=~go~(sort~\$~map~sortStr~xs)~1\\
\texfamily ~~~~{\bfseries where}~~go~[]~~\char95 ~=~[]\\
\texfamily ~~~~~~~~~~~go~[x]~n~=~[(n,x)]\\
\texfamily ~~~~~~~~~~~go~~(({\itshape P}~x):({\itshape P}~y):zs)~~n~~|~x~\char'36~y~~~~~=~go~(({\itshape P}~y):zs)~(n+1)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~(n,{\itshape P}~x)~:~go~(({\itshape P}~y)~:~zs)~1\\
\texfamily ~~~~~~~~~~~go~~(({\itshape N}~x):({\itshape N}~y):zs)~~n~~|~x~\char'36~y~~~~~=~go~(({\itshape N}~y):zs)~(n+1)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~(n,{\itshape N}~x)~:~go~(({\itshape N}~y)~:~zs)~1\\
\texfamily ~~~~~~~~~~~go~~(({\itshape N}~x):({\itshape P}~y):zs)~~n~~|~x~\char'36~y~~~~~=~go~zs~1\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~(n,{\itshape N}~x)~:~go~(({\itshape P}~y)~:~zs)~1\\
\texfamily ~~~~~~~~~~~go~~(({\itshape P}~x):({\itshape N}~y):zs)~~n~~|~x~\char'36~y~~~~~=~go~zs~1\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~(n,{\itshape P}~x)~:~go~(({\itshape N}~y)~:~zs)~1
\end{tabbing}
\end{minipage}
\ignore{$}

We start by first transforming all symbols
into the canonical form sorting their strings.
Then we sort the list of symbols itself.
The idea is that all strings of the same kind
are listed in a row, such that we can easily
count them.
But, of course, we do not want to have the
negative and positive symbols separated,
we want all symbols with the same string in a row,
independently of these symbols being positive
or negative.
We have to implement this notion of comparison
ignoring the sign. To this end, we make \text{\texfamily {\itshape Sym}}
an instance of \text{\texfamily {\itshape Ord}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~{\itshape Ord}~{\itshape Sym}~{\bfseries where}\\
\texfamily ~~~~compare~({\itshape P}~a)~~({\itshape P}~b)~~=~compare~a~b\\
\texfamily ~~~~compare~({\itshape P}~a)~~({\itshape N}~b)~~=~compare~a~b\\
\texfamily ~~~~compare~({\itshape N}~a)~~({\itshape P}~b)~~=~compare~a~b\\
\texfamily ~~~~compare~({\itshape N}~a)~~({\itshape N}~b)~~=~compare~a~b
\end{tabbing}
\end{minipage}

We now \text{\texfamily go} through the list of symbols sorted 
in this sense of comparison
and check on the first and the second of the remaining list
on each step.
If the signedness of first and second are equal
and their strings are equal, we
increment $n$
and store \text{\texfamily (n,x)}, \text{\texfamily x} the head of the list,
whenever they differ
starting the rest of the list with $n=1$.
Otherwise, when the signs are not equal,
but the strings are, then we drop both symbols
and continue with the rest of the list 
after the second.

Applied step for step on the list

\text{\texfamily [aaa,aab,aba,abb,baa,bab,bba,bbb]},

this would advance as follows.
We first sort all strings resulting

\text{\texfamily [aaa,aab,aab,abb,aab,abb,abb,bbb]}.

We next sort the symbols:

\text{\texfamily [aaa,aab,aab,aab,abb,abb,abb,bbb]}.

We then weight according to the number of their appearance:

\text{\texfamily [(1,aaa),(3,aab),(3,abb),(1,bbb)]}.

In this example, we ignore signedness,
since all terms are positive anyway.
The interesting thing thus is still
to commence: the application to sums
with negative terms.
We do this with the simple function

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~powsum~::~[{\itshape Sym}]~\char'31~{\itshape Natural}~\char'31~[({\itshape Natural},{\itshape Sym})]\\
\texfamily ~~powsum~xs~=~simplify~\char'00~combinator~xs
\end{tabbing}
\end{minipage}

We now define two variables,
a positive string $a$ and a negative one $b$:
\text{\texfamily {\bfseries let}\Sp a\Sp =\Sp {\itshape P}\Sp \char34 a\char34 } and \text{\texfamily {\bfseries let}\Sp b\Sp =\Sp {\itshape N}\Sp \char34 b\char34 }
and just apply \text{\texfamily powsum} with increasing $n$s:

\text{\texfamily powsum\Sp [a,b]\Sp 1}\\
\text{\texfamily [(1,{\itshape P}\Sp \char34 a\char34 ),(1,{\itshape N}\Sp \char34 b\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 2}\\
\text{\texfamily [(1,{\itshape P}\Sp \char34 aa\char34 ),(2,{\itshape N}\Sp \char34 ab\char34 ),(1,{\itshape P}\Sp \char34 bb\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 3}\\
\text{\texfamily [(1,{\itshape P}\Sp \char34 aaa\char34 ),(3,{\itshape N}\Sp \char34 aab\char34 ),(3,{\itshape P}\Sp \char34 abb\char34 ),(1,{\itshape N}\Sp \char34 bbb\char34 )]}

It appears that the absolute values of the coefficients 
do not change. We have, for instance,
$\binom{3}{0} = 1$,
$\binom{3}{1} = -3$,
$\binom{3}{2} = 3$,
$\binom{3}{3} = -1$.
What, if we swap the negative sign:
\text{\texfamily {\bfseries let}\Sp a\Sp =\Sp {\itshape N}\Sp \char34 a\char34 } and \text{\texfamily {\bfseries let}\Sp b\Sp =\Sp {\itshape P}\Sp \char34 b\char34 }?

\text{\texfamily powsum\Sp [a,b]\Sp 1}\\
\text{\texfamily [(1,{\itshape N}\Sp \char34 a\char34 ),(1,{\itshape P}\Sp \char34 b\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 2}\\
\text{\texfamily [(1,{\itshape P}\Sp \char34 aa\char34 ),(2,{\itshape N}\Sp \char34 ab\char34 ),(1,{\itshape P}\Sp \char34 bb\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 3}\\
\text{\texfamily [(1,{\itshape N}\Sp \char34 aaa\char34 ),(3,{\itshape P}\Sp \char34 aab\char34 ),(3,{\itshape N}\Sp \char34 abb\char34 ),(1,{\itshape P}\Sp \char34 bbb\char34 )]}

The result is just the same for even exponents.
For odd exponents, the signs are just exchanged:
$\binom{3}{0} = -1$,
$\binom{3}{1} = 3$,
$\binom{3}{2} = -3$,
$\binom{3}{3} = 1$.

What if both terms are negative:
\text{\texfamily {\bfseries let}\Sp a\Sp =\Sp {\itshape N}\Sp \char34 a\char34 } and \text{\texfamily {\bfseries let}\Sp b\Sp =\Sp {\itshape N}\Sp \char34 b\char34 }?

\text{\texfamily powsum\Sp [a,b]\Sp 1}\\
\text{\texfamily [(1,{\itshape N}\Sp \char34 a\char34 ),(1,{\itshape N}\Sp \char34 b\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 2}\\
\text{\texfamily [(1,{\itshape P}\Sp \char34 aa\char34 ),(2,{\itshape P}\Sp \char34 ab\char34 ),(1,{\itshape P}\Sp \char34 bb\char34 )]}\\[12pt]
\text{\texfamily powsum\Sp [a,b]\Sp 3}\\
\text{\texfamily [(1,{\itshape N}\Sp \char34 aaa\char34 ),(3,{\itshape N}\Sp \char34 aab\char34 ),(3,{\itshape N}\Sp \char34 abb\char34 ),(1,{\itshape N}\Sp \char34 bbb\char34 )]}

Now, wonder of wonders,
even exponents lead to positive coefficients,
while odd exponents lead to negative coefficients:
$\binom{3}{0} = -1$,
$\binom{3}{1} = -3$,
$\binom{3}{2} = -3$,
$\binom{3}{3} = -1$.

This result appears quite logical,
since, with even exponents,
we multiply an even number of negative factors,
while, with odd exponents,
we multiply an odd number of negative factors.

To confirm these results with more 
data, we define one more function
to extract the coefficients and, this way,
making the output more readable:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~coeffs~::~[{\itshape Sym}]~\char'31~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~coeffs~xs~=~map~getCoeff~\char'00~powsum~xs
\end{tabbing}
\end{minipage}

where \text{\texfamily getCoeff} is

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~getCoeff~::~({\itshape Natural},{\itshape Sym})~\char'31~{\itshape Natural}\\
\texfamily ~~getCoeff~(n,{\itshape P}~\char95 )~=~n\\
\texfamily ~~getCoeff~(n,{\itshape N}~\char95 )~=~-n
\end{tabbing}
\end{minipage}

We now map \text{\texfamily coeffs} on the numbers \text{\texfamily [0..9]}
and, with $a$ and $b$ still both negative,
see Pascal's triangle with signs alternating
per row:

\begin{minipage}{\textwidth}
\text{\texfamily []}\\
\text{\texfamily [-1,-1]}\\
\text{\texfamily [1,2,1]}\\
\text{\texfamily [-1,-3,-3,-1]}\\
\text{\texfamily [1,4,6,4,1]}\\
\text{\texfamily [-1,-5,-10,-10,-5,-1]}\\
\text{\texfamily [1,6,15,20,15,6,1]}\\
\text{\texfamily [-1,-7,-21,-35,-35,-21,-7,-1]}\\
\text{\texfamily [1,8,28,56,70,56,28,8,1]}\\
\text{\texfamily [-1,-9,-36,-84,-126,-126,-84,-36,-9,-1]}
\end{minipage}

For one of the value $a$ and $b$ positive and
the other negative, we see the coefficients in one row
alternating in signedness.
For $a$ positive and $b$ negative, we see:

\begin{minipage}{\textwidth}
\text{\texfamily []}\\
\text{\texfamily [1,-1]}\\
\text{\texfamily [1,-2,1]}\\
\text{\texfamily [1,-3,3,-1]}\\
\text{\texfamily [1,-4,6,-4,1]}\\
\text{\texfamily [1,-5,10,-10,5,-1]}\\
\text{\texfamily [1,-6,15,-20,15,-6,1]}\\
\text{\texfamily [1,-7,21,-35,35,-21,7,-1]}\\
\text{\texfamily [1,-8,28,-56,70,-56,28,-8,1]}\\
\text{\texfamily [1,-9,36,-84,126,-126,84,-36,9,-1]}
\end{minipage}

The other way round, $a$ negative and $b$ positive,
we see just the same triangle where, for odd exponents,
the minus signs are swapped.
The triangle, hence, is the same as the one before
with each row reversed:

\begin{minipage}{\textwidth}
\text{\texfamily []}\\
\text{\texfamily [-1,1]}\\
\text{\texfamily [1,-2,1]}\\
\text{\texfamily [-1,3,-3,1]}\\
\text{\texfamily [1,-4,6,-4,1]}\\
\text{\texfamily [-1,5,-10,10,-5,1]}\\
\text{\texfamily [1,-6,15,-20,15,-6,1]}\\
\text{\texfamily [-1,7,-21,35,-35,21,-7,1]}\\
\text{\texfamily [1,-8,28,-56,70,-56,28,-8,1]}\\
\text{\texfamily [-1,9,-36,84,-126,126,-84,36,-9,1]}
\end{minipage}


\section{$\mathbb{Q}$}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Quoz}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}\\
\texfamily ~~{\bfseries import}~{\itshape Zahl}\\
\texfamily ~~{\bfseries import}~qualified~{\itshape {\itshape Data}.Ratio}~as~{\itshape R}~(numerator,~denominator)\\
\texfamily ~~{\bfseries import}~{\itshape {\itshape Debug}.Trace}~(trace)
\end{tabbing}
}

We now turn our attention to fractions
and start by implementing a rational data type:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries data}~{\itshape Ratio}~=~{\itshape Q}~{\itshape Natural}~{\itshape Natural}\\
\texfamily ~~~~{\bfseries deriving}~({\itshape Show},{\itshape Eq})
\end{tabbing}
\end{minipage}

A \text{\texfamily {\itshape Ratio}} has a constructor \text{\texfamily {\itshape Q}} that takes
two natural numbers.
The name of the constructor is derived
from the symbol for the set of rational numbers $\mathbb{Q}$
that was introduced by Giuseppe Peano 
and stems from the Italian word \term{Quoziente}.

It would be nice of course to have a function
that creates a rational in its canonical form,
\ie\ reduced to two natural numbers that are
coprime to each other.
This is done by \text{\texfamily ratio}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~ratio~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Ratio}\\
\texfamily ~~ratio~\char95 ~0~=~error~\char34 division~by~zero\char34 \\
\texfamily ~~ratio~a~b~=~reduce~({\itshape Q}~a~b)
\end{tabbing}
\end{minipage}

for which we define the infix \text{\texfamily \%}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries infix}~\%\\
\texfamily ~~(\%)~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Ratio}\\
\texfamily ~~(\%)~=~ratio
\end{tabbing}
\end{minipage}

so that we can create ratios with expressions
like \text{\texfamily 5\Sp \%\Sp 2}, \text{\texfamily 8\Sp \%\Sp 4} and so on.
The function \text{\texfamily reduce} called in \text{\texfamily ratio}
is defined as follows: 

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~reduce~::~{\itshape Ratio}~\char'31~{\itshape Ratio}\\
\texfamily ~~reduce~({\itshape Q}~\char95 ~0)~=~error~\char34 division~by~zero\char34 \\
\texfamily ~~reduce~({\itshape Q}~0~\char95 )~=~{\itshape Q}~0~1\\
\texfamily ~~reduce~({\itshape Q}~n~d)~=~~{\itshape Q}~(n~`div`~gcd~n~d)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~(d~`div`~gcd~n~d)~
\end{tabbing}
\end{minipage}

\ignore{
\begin{tabbing}\texfamily
~~mygcd~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~mygcd~a~0~~=~a\\
\texfamily ~~mygcd~a~b~~=~mygcd~b~(a~`rem`~b)
\end{tabbing}
}

which reduces numerator and denominator 
to the quotient of the greatest common divisor
of numerator and denominator.
If numerator and denominator are coprime,
the \text{\texfamily gcd} is just 1 and the numbers 
are not changed at all.

Useful would be to have access functions
for numerator and denominator. We define them
straight forward as

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~numerator~::~{\itshape Ratio}~\char'31~{\itshape Natural}\\
\texfamily ~~numerator~({\itshape Q}~n~\char95 )~=~n\\
\texfamily ~~denominator~::~{\itshape Ratio}~\char'31~{\itshape Natural}\\
\texfamily ~~denominator~({\itshape Q}~\char95 ~d)~=~d
\end{tabbing}
\end{minipage}

We now make \text{\texfamily {\itshape Ratio}} an instance of \text{\texfamily {\itshape Ord}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~{\itshape Ord}~{\itshape Ratio}~{\bfseries where}\\
\texfamily ~~~~compare~x@({\itshape Q}~nx~dx)~y@({\itshape Q}~ny~dy)~~|~~dx~\char'36~dy~~~=~~compare~nx~ny\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~~otherwise~~=~~{\bfseries let}~(x',y')~=~unify~x~y\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bfseries in}~compare~x'~y'
\end{tabbing}
\end{minipage}

If the denominators are equal,
we just compare the numerators,
\ie\ $\frac{1}{5} < \frac{2}{5} < \frac{3}{5}$
and so on.
Otherwise, if the denominators differ,
we must convert the fractions to a common denominator
before we actually can compare them.
This is done using \text{\texfamily unify}: 

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~unify~::~{\itshape Ratio}~\char'31~{\itshape Ratio}~\char'31~({\itshape Ratio},~{\itshape Ratio})\\
\texfamily ~~unify~({\itshape Q}~nx~dx)~({\itshape Q}~ny~dy)~=~(~~{\itshape Q}~(nx~*~(lcm~dx~dy)~`div`~dx)~(lcm~dx~dy),~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\itshape Q}~(ny~*~(lcm~dx~dy)~`div`~dy)~(lcm~dy~dx))
\end{tabbing}
\end{minipage}

This is the implementation of the logic already described before:
we convert a fraction to the common denominator
defined by the $lcm$ of both denominators
and multiply the numerators by the number
we would have to multiply the denominator by
to get the $lcm$, which trivially is the $lcm$ divided by
the denominator:
$lcm(dx,dy) = dx \times \frac{lcm(dx,dy)}{dx}$.
This may appear a bit complicated,
but it is much faster, whenever the denominators
are not coprime to each other.

The next step is to make \text{\texfamily {\itshape Ratio}} an instance of \text{\texfamily {\itshape Enum}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~{\itshape Enum}~{\itshape Ratio}~{\bfseries where}\\
\texfamily ~~~~toEnum~i~=~{\itshape Q}~(toEnum~i)~(toEnum~1)\\
\texfamily ~~~~fromEnum~({\itshape Q}~n~d)~=~fromEnum~(n~`div`~d)
\end{tabbing}
\end{minipage}

which implies a conversion from and to an integer type.
A plain integral number $i$ is converted to a \text{\texfamily {\itshape Ratio}}
using the denominator 1.
For the backward conversion, \text{\texfamily div} is used
leading to the loss of precision if the denominator
does not divide the numerator.

Now we come to the heart of the data type
making it instance of \text{\texfamily {\itshape Num}}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~{\itshape Num}~{\itshape Ratio}~{\bfseries where}\\
\texfamily ~~~~x@({\itshape Q}~nx~dx)~+~y@({\itshape Q}~ny~dy)~~|~dx~\char'36~dy~~~=~(nx~+~ny)~\%~dx\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~~{\bfseries let}~(x',y')~=~unify~x~y\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bfseries in}~(x'~+~y')\\
\texfamily ~~~~x@({\itshape Q}~nx~dx)~-~y@({\itshape Q}~ny~dy)~~|~x~\char'36~y~=~{\itshape Q}~0~1\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~x~>~~y~\char'04~dx~\char'36~dy~=~(nx~-~ny)~\%~dx\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~x~>~~y~~~~~~~~~~~~~=~~{\bfseries let}~(x',y')~=~unify~x~y\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bfseries in}~~x'~-~y'\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~=~error~\char34 Subtraction~beyond~zero!\char34 \\
\texfamily ~~~~({\itshape Q}~nx~dx)~*~({\itshape Q}~ny~dy)~=~(nx~*~ny)~\%~(dx~*~dy)\\
\texfamily ~~~~negate~a~=~a\\
\texfamily ~~~~abs~~~~a~=~a\\
\texfamily ~~~~signum~({\itshape Q}~0~\char95 )~=~0\\
\texfamily ~~~~signum~({\itshape Q}~\char95 ~\char95 )~=~1\\
\texfamily ~~~~fromInteger~i~~=~{\itshape Q}~(fromIntegral~i)~1
\end{tabbing}
\end{minipage}

We add two fraction with the same denominator
by reducing the result of $\frac{nx+ny}{d}$.
If we add two fractions in canonical form,
such as $\frac{1}{9}$ and $\frac{5}{9}$,
we may arrive at a result that is not in
canonical form like, in this example,
$\frac{6}{9}$, which should be reduced to
$\frac{2}{3}$.
Otherwise, if the denominators differ,
we first convert the fractions
to a common denominator before we add them.

Since we have defined \text{\texfamily {\itshape Ratio}} as a fraction
of two natural numbers (and not of two integers),
we have to be careful with subtraction.
If the two fractions are equal,
the result is zero, which is represented as $\frac{0}{1}$.
If $x > y$, we use the same strategy as with addition.
Otherwise, if $y > x$, subtraction is undefined.

Multiplication is easy: we just reduce the result
of multiplying the two numerators 
and denominators by each other.
The other functions do not add anything new.
We just define \text{\texfamily negate}, \text{\texfamily abs} and \text{\texfamily signum}
as we have done before for plain natural numbers
and we define the conversion from integer
as we have done for \text{\texfamily {\itshape Enum}} already.

The next step, however, is unique:
we define \text{\texfamily {\itshape Ratio}} as an instance of \text{\texfamily {\itshape Fractional}}.
The core of this is to define a division function
and do so defining division as the inverse of multiplication:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~rdiv~::~{\itshape Ratio}~\char'31~{\itshape Ratio}~\char'31~{\itshape Ratio}\\
\texfamily ~~rdiv~({\itshape Q}~nx~dx)~({\itshape Q}~ny~dy)~=~({\itshape Q}~nx~dx)~*~({\itshape Q}~dy~ny)
\end{tabbing}
\end{minipage}

The division of a fraction $\frac{nx}{dx}$
by another $\frac{ny}{dy}$ is just the multiplication
of that fraction with the inverse of the second one,
which is $\frac{dy}{ny}$.
The complete implementation of the \text{\texfamily {\itshape Fractional}} type
then is

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries instance}~{\itshape Fractional}~{\itshape Ratio}~{\bfseries where}\\
\texfamily ~~~~(/)~~~~~~~~~~~~~=~rdiv\\
\texfamily ~~~~fromRational~r~~=~{\itshape Q}~~(fromIntegral~\$~{\itshape R}.numerator~r)~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~(fromIntegral~\$~{\itshape R}.denominator~r)
\end{tabbing}
\end{minipage}

This is not a complete definition of $\mathbb{Q}$, however.
$\mathbb{Q}$ is usually defined on top of the integers
rather than on top of natural numbers.
So, our data type should be signed.
That, however, is quite easy to achieve:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries type}~{\itshape Quoz}~=~{\itshape Signed}~{\itshape Ratio}
\end{tabbing}
\end{minipage}

\ignore{
we need a convenience interface that says
let a = -1 :: Quoz
a
Neg (Q 1 1)
}

\section{Zeno's Paradox}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Zeno}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Quoz}
\end{tabbing}
}

The ancient Greek philosopher Zeno of Elea, who lived
in the $5^{th}$ century \acronym{bc}, devised a number
of paradoxes that came upon us indirectly through
the work of Aristotle and its commentators.
Zeno designed the paradoxes to defend the philosophy
of the \term{One} developed by Zeno's teacher Parmenides.
According to this philosophy everything is One,
undivisible, motionless, eternal, everywhere and nowhere.
That we actually see motion, distinguish and divide
things around us, that everything ``in this world'' is volatile
and that everything has its place, is either here or there,
is, according to Parmenides, just an illusion.

Plato discusses the philosophy of Parmenides in one
of his most intriguing dialogs, ``Parmenides'', 
where young Socrates, Zeno and Parmenides himself
analyse contradictions that arise both in Plato's
\term{theory of forms} as well as in Parmenides' theory of the One.
The Parmenides dialog had a deep influence on 
European philosophy and religion. 
It was the main inspiration for the late-ancient
\term{neoplatonism} and, for many centuries,
it shaped the interpretation of ancient philosophy
by medieval thinkers.
Scholars today, however, are not so sure anymore
what the meaning of this dialog may be.
Some see in it a critical discussion of 
the theory of forms, others hold it is a collection
of exercises for students of Plato's academy,
and again others consider the dialog as highly ironic,
actually criticising Parmenides and other philosophers
for using terms that they know from everyday life 
in a context where the ideas associated with this terms
do not hold anymore -- very similar to the therapeutic
approach of Ludwig Wittgenstein.

It is tempting to relate the philosophy of the One with the
philosophical worldview of mathematical platonism.
Constructivists would see the world as dynamic,
as a chaotic process without meaning in itself or,
pessimistically, as a thermodynamic process that tends to entropy.
It is an effort of human beings to create order in this
dynamic and perhaps chaotic world.
Therefore, prime numbers -- or any other mathematical object --
do not exist, we define them and we have to
invest energy to construct them.
By contrast, mathematical platonists would hold that there
is a static eternal structure that does not change at all.
The mathematical objects are out there, perhaps like in
a gigantic lattice behind the universe.
It would then be absurd to say that we construct prime numbers.
We find them travelling along the eternal metaphysical structure
that is behind of what we can perceive directly with our senses.

Be that as it may,
we are here much more interested in the math
in Zeno's paradoxes.
The most famous one is the race
of Achilles and the tortoise.
Achilles gives the tortoise a lead of, say, hundred meters.
The question now is when Achilles will actually catch up with
the tortoise. Zeno says: never, for it is impossible.
To catch up, he must reach a point where the tortoise has been
shortly before. But when he gets there, the tortoise is already
ahead. Perhaps just a few metres, but definitely ahead.
So, again, to reach that point, Achilles will need some time.
When he reaches the point where the tortoise has been a second before,
the same is already a bit further. 
To reach that point, Achilles again needs some time
and in this time the tortoise again makes some progress
and so it goes on and on. 

A more concrete version of this paradox is given in
the so called \term{Dichotomy} paradox.
It states that, in general, it is impossible 
to move from $A$ to $B$.
Since, to do so, one has first to make half of the way
arriving at a point $C$. To move from $C$ to $B$,
one now has to first make half of the way
arriving at a point $D$. To move from $D$ to $B$,
one now has to first make half of the way
arriving at yet another point and so on and so on.

This paradox appears to be at odds with 
what we observe in the physical world where it
indeed appears to be possible to move from
$A$ to $B$ quite easily.
The paradox, however, draws our attention to
the fact that, between any two rational numbers,
there are infinitely many other rational numbers.
Between 0 and 1, for instance, there is $\frac{1}{2}$.
Between 0 and $\frac{1}{2}$, there is $\frac{1}{4}$.
Between 0 and $\frac{1}{4}$, there is $\frac{1}{8}$.
Between 0 and $\frac{1}{8}$, there is $\frac{1}{16}$
and, in general, between 0 and any number of the form
$\frac{1}{2^k}$, there is a number $\frac{1}{2^{k+1}}$.

So, following these points as in Zeno's paradox,
how close to $B$ would we get after $k$ steps?
The problem can be represented as a sum of the form

\[
\sum_{i=1}^k{\frac{1}{2^i}}
\]

After $k=1$ step, we would be $\frac{1}{2}$ of the way
away from $B$. After $k=2$ steps, the distance would be

\[
\frac{1}{2} + \frac{1}{2^2}.
\]

We convert the fractions to a common denominator
multiplying the first fraction by 2 and arrive at

\[
\frac{2+1}{4} = \frac{3}{4}. 
\]

For $k=3$ steps, we have

\[
\frac{3}{4} + \frac{1}{2^3} =
\frac{6+1}{8} = \frac{7}{8}.
\]

These tests suggest the general formula

\begin{equation}\label{eq:Zeno2}
\sum_{i=1}^k{\frac{1}{2^i}} =
\frac{2^k - 1}{2^k}.
\end{equation}

This equation cries out for an induction proof.
Any of the examples above serves as base case.
We then have to prove that

\begin{equation}
\frac{2^k - 1}{2^k} + \frac{1}{2^{k+1}} = 
\frac{2^{k+1}-1}{2^{k+1}}.
\end{equation}

We convert the fractions to a common denominator
multiplying the first fraction by 2:

\[
\frac{2(2^k - 1) + 1}{2^{k+1}}.
\]

We simplify the numerator: 
$2 \times 2^k = 2^{k+1}$ and
$2 \times (-1) = -2$; we, hence, have in the numerator
$2^{k+1} - 2 + 1$, which can be simplified to
$2^{k+1} - 1$ and leads to the desired result

\[
\frac{2^{k+1} - 1}{2^{k+1}}.\qed
\]

That was easy!
Can we generalise the result for any denominator $n$,
such that

\begin{equation}\label{eq:ZenoGenFalse}
\frac{1}{n^k} + \frac{1}{n^{k+1}} = 
\frac{n^{k+1} - 1}{n^{k+1}}?
\end{equation}

If we went a third of the way on each step
instead of half of it, we had
$\frac{1}{3^k} + \frac{1}{3^{k+1}}$, for instance,
$\frac{1}{3} + \frac{1}{9}$.
We convert the fraction to a common 
denominator multiplying the first by 3:
$\frac{3+1}{9} = \frac{4}{9}$.
So, equation \ref{eq:ZenoGenFalse} seems to be wrong.
The nice and clean result with the denominator 2
appears to be one of those deceptions that are so common
for small numbers, which often behave very differently
from greater numbers.

But let us stop moaning. What actually is the rule
for $n=3$? After the next step, we would have

\[
\frac{4}{9} + \frac{1}{27}.
\] 

We multiply the first fraction by 3 and have

\[
\frac{12+1}{27} =
\frac{13}{27}.
\] 

For $k=4$, we would have

\[
\frac{13}{27} + \frac{1}{81} =
\frac{39 + 1}{81} =  \frac{40}{81}.
\] 

The experiments this time suggest the rule

\begin{equation}\label{eq:Zeno3}
\sum_{i=1}^k{\frac{1}{3^i}} = 
\frac{(3^k - 1) / 2}{3^k}.
\end{equation}

We prove again by induction with any of the examples
serving as base case.
We have to prove that 

\begin{equation}
\frac{(3^k - 1) / 2}{3^k} + \frac{1}{3^{k+1}} =
\frac{(3^{k+1} - 1) / 2}{3^{k+1}}.
\end{equation}

We multiply the first fraction by 3 in numerator and
denominator and get in the numerator
$\frac{3(3^k - 1)}{2} = \frac{3^{k+1} - 3}{2}$. 
We can now add the two fractions:

\[
\frac{(3^{k+1} - 3) / 2 + 1}{3^{k+1}}.
\]

To add 1 to the fraction in the numerator
we have to convert 1 to a fraction with the denominator 2,
which, of course, is $\frac{2}{2}$.
We, hence, have in the numerator
$\frac{3^{k+1} - 3 + 2}{2}$
and this leads to the desired result:

\begin{equation}
\frac{(3^{k+1} - 1) / 2}{3^{k+1}}.\qed
\end{equation}

Before we dare to make a new conjecture
based on equations \ref{eq:Zeno2} and \ref{eq:Zeno3},
let us collect some more data.
Since $n=4$ is closely related to $n=2$,
we will immediately go to $n=5$.
For $k=2$ we have

\[
\frac{1}{5} + \frac{1}{25} =
\frac{5 + 1}{25} = \frac{6}{25}.
\]

For $k=3$ we have

\[
\frac{6}{25} + \frac{1}{125} =
\frac{30+1}{125} = \frac{31}{125}.
\]

For $k=4$ we have

\[
\frac{31}{125} + \frac{1}{625} =
\frac{155+1}{625} = \frac{156}{625}.
\]

In these examples, we see the relation 

\begin{equation}\label{eq:Zeno5}
\sum_{i=1}^k{\frac{1}{5^i}} = 
\frac{(5^k - 1)/ 4}{5^k}.
\end{equation}

We prove easily by induction using any
of the examples as base case.
We have to show that

\begin{equation} 
\frac{(5^k - 1)/4}{5^k} + \frac{1}{5^{k+1}} =
\frac{(5^{k+1} - 1)/4}{5^{k+1}}.
\end{equation} 

We multiply the first fraction by 5, yielding the numerator
$\frac{5^{k+1} - 5}{4}$ and, when adding 1, we get
$\frac{5^{k+1} - 5}{4} + \frac{4}{4}$, which, of course,
leads to the desired result.\qed

To summarise:
with $n=2$, we see $\frac{n^k - 1}{n^k}$;
with $n=3$, we see $\frac{(n^k - 1)/2}{n^k}$;
with $n=5$, we see $\frac{(n^k - 1)/4}{n^k}$.
This suggests the general form

\begin{equation}
\sum_{i=1}^k{\frac{1}{n^i}} =
\frac{(n^k - 1) / (n-1)}{n^k},
\end{equation}

which would nicely explain why we overlooked
the division in the numerator for the case $n=2$,
since, here, $n-1 = 1$ and anything
divided by 1 is just that anything.

It, again, does not appear to be too difficult to prove
the result.
We have a lot of base cases already
and now want to prove that

\begin{equation}
\frac{(n^k - 1) / (n-1)}{n^k} + \frac{1}{n^{k+1}} =
\frac{(n^{k+1} - 1) / (n-1)}{n^{k+1}}.
\end{equation}

We multiply the first fraction by $n$
in numerator and denominator and get 
in the numerator

\[
\frac{n(n^k - 1)}{n-1} = 
\frac{n^{k+1} - n}{n-1}.
\]

We now add 1 represented as the fraction $\frac{n-1}{n-1}$:

\[
\frac{n^{k+1} - n}{n-1} + \frac{n-1}{n-1},
\]

leading to 

\[
\frac{n^{k+1} - n + n - 1}{n-1} =
\frac{n^{k+1} - 1}{n-1},
\]

which is the desired result 

\begin{equation}
\frac{(n^{k+1} - 1)/(n-1)}{n^{k+1}}.\qed
\end{equation}


\section{Systems of Linear Equations}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape LinearSystems}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Quoz}
\end{tabbing}
}

Systems of linear equations provide an excellent topic
to get familar with structures that we will need a lot
in algebra, namely \term{matrices}. Before we get there,
we look at linear equations as such. Linear equations
and the systems made of them belong to the oldest topics
studied in algebra. There is a rich body of knowledge in
Chinese and Indian books dating back to antiquity and
early middle ages (in terms of European history). The
famous ``Nine Chapters of Mathematical Art'', for instance,
dates back to 179 \acronym{ad}. It contains advanced
algorithms to solve systems of linear equations that were
formulated in Europe only in the $19^{th}$ century.

This knowledge was brought to Europe through Arab and
Persian scholars, most famously perhaps al-Hwarizmi,
called Algoritmi in medieval Europe, 
and his book ``Compendium on Calculation and Balancing'',
whose original title contains the word ``al-gabr'',
which was latinised as \term{algebra}.

In this tradition, systems of linear equations were
often worded in terms of \term{bird problems}.
Bird problems are centered around the question of
how many of $n$ different kinds of birds can be
bought for a specific amount of money. A typical
problem is to buy 100 birds for 100 drachme.
There are ducks, chickens and sparrows. 
For 1 drachme, you can either buy one chicken
or 20 sparrows; for 5 drachme, you get a duck.
This translates into the simple
system of equations

\begin{equation}
\begin{array}{lcl}
x + y + z & = & 100\\
5x + \frac{1}{20}y + z & = & 100
\end{array}
\end{equation}

The first equation states that the sum of the number of birds
shall be 100; the second equation states that the sum of the
price of the birds shall be 100 drachme.

One way to solve such equations is 
to \term{eliminate} one of the variables.
In the given system, we can solve for $z$
in both equations:

\begin{equation}
z = 100 - x - y
\end{equation}

and

\begin{equation}
z = 100 - 5x - \frac{1}{20}y.
\end{equation}

We set the right-hand sides of the equations equal, 
subtract 100
and bring $x$ to the left side of the equation
and $y$ to the right side. We get:

\begin{equation}
4x = \frac{19}{20}y
\end{equation}

and divide by 4:

\begin{equation}
x = \frac{19}{80}y.
\end{equation}

From here, we easily find a solution
by assuming that $y=80$, $x=19$ and, in consequence,
$z=1$. The original equations with the variables 
substituted, then, read 

\begin{equation}
\begin{array}{lcl}
19 + 80 + 1 & = & 100\\
5\times 19 + \frac{1}{20}\times 80 + 1 & = & 100,
\end{array}
\end{equation}

which, as you can easily convince yourself, is correct
in both cases.

The final step in the derivation was a mere guess
based on the fact that we expected integer numbers
as results in one of the equations. 
Without that restriction, \ie\ when
we define the system over the field of rational
numbers, would there be a way to solve any such
system? It turns out, there is. Furthermore,
that algorithm is guaranteed to find a single solution to
any well-defined system.

You might remember a similar claim that we proved
for a special kind of systems in the previous chapter,
namely the Chinese Remainder Theorem. Indeed,
Chinese remainders are just a special case of
linear equations in a finite field of modular arithmetic.
For the general case, which includes infinite fields,
such as the rational numbers, we have to restrict
the claim adding the constraint that the system
must be \term{well-defined}.

By this, we mean that the system
is \term{consistent} and contains the same number of 
\term{independent} equations
and unknowns; for the bird problem above this was
not the case, since there were only two equations
for three unknowns ($x$, $y$ and $z$). 
However, the bird problem was restricted to integers,
and we were able to guess the result after some steps. 

Have a look at the following system: 

\begin{equation}
\begin{array}{rcrcr}
 x & + &  y & = & 1\\
2x & + & 2y & = & 2
\end{array}
\end{equation}

There are two unknowns, $x$ and $y$, and two equations.
Unfortunately, the two equations are not independent,
since the second equation is equivalent to the first,
\ie\ it is just the first equation scaled up.
Indeed, whenever one equation can be derived from the others
by algebraic means, it is not independent and, hence,
does not add new information to the system.
A somewhat more subtle example of a system 
with a dependent equation is

\begin{equation}\label{eq_linEqUnder}
\begin{array}{rcrcrcr}
 x & - & 2y & + &  z & = & -1\\
3x & + & 5y & + &  z & = &  8\\
4x & + & 3y & + & 2z & = &  7.
\end{array}
\end{equation}

Here, the third equation is the sum of equations 1 and 2,
so it does not add new information.

Systems of equations that have more unknowns than
independent equations are called \term{underdetermined}.
They usually have no or infinitely many solutions.
If a system has more equations than unknowns, it is
\term{overdetermined} and, usually, has no solution.
The system is then \term{inconsistent}, \ie\ it contains a
contradiction. An inconsistent system is, for instance

\begin{equation}
\begin{array}{rcrcrcr}
 x & - & 2y & + &  z & = & -1\\
3x & + & 5y & + &  z & = &  8\\
4x & + & 3y & + & 2z & = &  5.
\end{array}
\end{equation}

The sum of the left-hand side of equations 1 and 2 results in
the left-hand side of equation 3. 
The right-hand side of equation 3, however,
is not the sum of the right-hand side of equations 1 and 2.
Any try of to solve this system will lead to 
a contradiction of the form $1=0$.

A consistent system, however, that has the same number
of independent equations and unknowns has,
within a field, always a unique
solution and there is an algorithm that finds
this solution.
But before we present and implement the algorithm as such,
we will look at the ideas, on which it is based.

The first approach is \term{elimination}. The idea is
to solve one equation for one of the variables and then
to substitute that variable in the other equations by
the result. A concrete example:

\begin{equation}
\begin{array}{rcrcrcr}
 x & + & 3y & - & 2z & = &  5\\
3x & + & 5y & + & 6z & = &  7\\
2x & + & 4y & + & 3z & = &  8.
\end{array}
\end{equation}

We solve the first equation for $x$.
We just subtract $3y$ from and add $2z$ to both side
to obtain

\begin{equation}
x = 5 - 3y + 2z.
\end{equation}

We substitute this result for $x$ in the other equations
and obtain:

\begin{equation}
\begin{array}{rcrcrcr}
3(5-3y+2z) & + & 5y & + & 6z & = &  7\\
2(5-3y+2z) & + & 4y & + & 3z & = &  8,
\end{array}
\end{equation}

which, after simplication and bringing the constant
numbers to the right-hand side, translates to

\begin{equation}
\begin{array}{rcrcr}
-4y & + & 12z & = & -8\\
-2y & + & 7z & = &  -2.
\end{array}
\end{equation}

Now we repeat the process, solving the first of these equations
for $y$, which yields $-4y = -12z-8$ and, after dividing
both sides by $-4$, $y=3z+2$. We then substitute $y$
into the third equation yielding $-2(3z+2) + 7z = -2$.
Simplifying again leads to $z-4=-2$ and, after adding 4 to both sides,
$z=2$.

Now, we just go backwards, first 
substituting $z$ in the equation solved for $y$ leading to

\begin{equation}
y=3\times 2 + 2 = 8
\end{equation}

and, second, substituting $z=2$ and $y=8$ in the first equation
solved for $x$:

\begin{equation}
x = 5 - 3\times 8 + 2\times 2 = -19 + 4 = -15.
\end{equation}

The complete result, hence, is 

\[
x=-15, y=8, z=2.
\]

Notice that the approach aims to subsequently 
\term{eliminate} variables from the equations.
This way, we simplify a system with $n$ equations 
and unknowns to a system with $n-1$ equations and unknowns
and, then, we just repeat until we are left with
one equation with one unknown.

We can reach this goal in a more direct manner
by adding (or subtracting) one equation to (or from)
the other such that one of the unknowns disappears,
\ie\ reduces to zero. Usually, we have to scale
one of the equations to achieve this.

When we look at the previous system once again

\begin{equation}
\begin{array}{rcrcrcr}
 x & + & 3y & - & 2z & = &  5\\
3x & + & 5y & + & 6z & = &  7\\
2x & + & 4y & + & 3z & = &  8,
\end{array}
\end{equation}

we see that, if we scale the first equation by factor 3
and add it to the second equation, $z$ would fall away:

\begin{equation}\label{eq:linPen1}
\begin{array}{crcrcrcr}
  & 3x & +  & 9y  & - & 6z & = & 15\\
+ & 3x & +  & 5y  & + & 6z & = &  7\\
= & 6x & + & 14y  &   &  & = & 22.
\end{array}
\end{equation}

Likewise, we can scale the third equation by factor 2
and subtract it from the second equation:

\begin{equation}\label{eq:linPen2}
\begin{array}{crcrcrcr}
  & 3x & + & 5y  & + & 6z & = &  7\\
- & 4x & + & 8y  & + & 6z & = & 16\\
= & -x & - & 3y  &   &    & = & -9.
\end{array}
\end{equation}

This way, we obtain two equations with two unknowns.
We can eliminate one more unknown by scaling the second
of these new equations by factor 6 and add it to the
first one:

\begin{equation}
\begin{array}{crcrcr}
  &  6x & + & 14y  & = &  22\\
+ & -6x & - & 18y  & = & -54\\
= &     &   & -4y  & = & -32.
\end{array}
\end{equation}

When we divide both sides of the result by $-4$,
we get $y = 8$, which is the same result we saw
before with the elimination method.

We now can go on and eliminate other unknowns
by scaling and adding. We should not be frightened
to use fractions, when solving equations in a field.
We can, for instance, isolate $x$ by scaling
the resulting equation \ref{eq:linPen2} by the factor
$\frac{14}{3}$ and add it to equation \ref{eq:linPen1}:

\begin{equation}
\begin{array}{crcrcr}
  &             6x & + & 14y  & = &  22\\
+ & -\frac{14}{3}x & - & 14y  & = & -42\\
= &  \frac{4}{3}x  &   &      & = & -20.
\end{array}
\end{equation}

After multiplying by $3$ and dividing by 4 on both sides,
we get $x = -15$, as before.

The generic algorithm is based on
these principles of scaling and adding as well as elimination,
but does so in a systematic way. In our manual process,
we took decisions on which equation to solve and on which
equations to add to or subtract from which other. 
Those decisions 
were driven by human motives, for instance, to avoid
fractions whenever possible. For a systematic algorithm
executed on a machine, such considerations are irrelevant.
The machine has no peference for integers over fractions.

The algorithm is called \term{Gaussian elimination},
although it is known to Chinese and Indian mathematicians
since late antiquity. We will here discuss the basic form
of this algorithm. There is a more advanced form,
called \term{Gauss-Jordan algorithm}, at which we look later.
Interesting, however, is the second eponym of the algorithm,
Wilhelm Jordan (1842 -- 1899), a German geodesist.
This underlines the fact that this method -- 
as well as many other
methods from linear algebra -- has its roots
in applied science rather than in pure mathematics.

Both algorithms are based on a data structure
of fundamental importance in linear algebra, 
the \term{matrix}.
We, here, introduce matrices as a mere tool
that helps us doing calculations. In algebra,
however, matrices are studied as a topic in itself.

Anyway, what is a matrix in the first place?
Well, ``matrix'' is basically a fancy name
for what we all know as ``table''.
A matrix consists of rows and columns
that are identified by a pair of indices $(i,j)$,
where $i$ usually refers to the row and $j$
to the column.

Here we use matrices to represent
systems of equations. Each row contains
one equation. Each column contains one coefficient,
\ie\ the numbers before the unknowns and,
in the last column, we have the constant
value on the right-hand side of the equations
(this is often called an \term{augmented matrix}).
Our equation above can be represented in matrix form as:

\[
\begin{pmatrix}
1 & 3 & -2 & 5\\
3 & 5 &  6 & 7\\
2 & 4 &  3 & 8
\end{pmatrix}
\]

In Haskell, we can define a matrix as a list of lists,
where the inner lists represent rows, for instance:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries data}~{\itshape Matrix}~a~=~{\itshape M}~[[a]]\\
\texfamily ~~~~{\bfseries deriving}~({\itshape Show},{\itshape Eq})
\end{tabbing}
\end{minipage}

We can create a matrix for our system by

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~mysystem~::~{\itshape Matrix}~[{\itshape Natural}]\\
\texfamily ~~mysystem~=~~{\bfseries let}~~e1~~=~[1,3,-2,5]\\
\texfamily ~~~~~~~~~~~~~~~~~~~e2~~=~[3,5,6,7]\\
\texfamily ~~~~~~~~~~~~~~~~~~~e3~~=~[2,4,3,8]\\
\texfamily ~~~~~~~~~~~~~~{\bfseries in}~~~{\itshape M}~[e1,e2,e3]
\end{tabbing}
\end{minipage}

The following functions yield the rows
and, respectively, the columns of the matrix:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~rows~::~{\itshape Matrix}~a~\char'31~[[a]]\\
\texfamily ~~rows~({\itshape M}~rs)~=~rs\\
\texfamily \\
\texfamily ~~cols~::~{\itshape Matrix}~a~\char'31~[[a]]\\
\texfamily ~~cols~({\itshape M}~rs)~=~go~rs\\
\texfamily ~~~~{\bfseries where}~~go~[]~=~[]\\
\texfamily ~~~~~~~~~~~go~rs~~|~~null~(head~rs)~=~[]\\
\texfamily ~~~~~~~~~~~~~~~~~~|~~otherwise~~~~~~=~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~heads~rs~:~go~(tails~rs)\\
\texfamily \\
\texfamily ~~heads~::~[[a]]~\char'31~[a]\\
\texfamily ~~heads~zs~=~[head~z~|~z~\char'06~zs,~\char'05~(null~z)]\\
\texfamily \\
\texfamily ~~tails~::~[[a]]~\char'31~[[a]]\\
\texfamily ~~tails~=~map~tail~z
\end{tabbing}
\end{minipage}

Obtaining the rows is trivial: 
the function just returns the list of lists.
Columns are bit more difficult.
We recursively return the list 
of the heads of the inner lists,
reducing these lists per step to their \text{\texfamily tail}s
until the lists are empty.
This condition is checked on the first
inner list. Since, in a matrix, 
all rows need to have
the same size, the first list can
act as a model for all lists.

Here are two helper functions to compute
the length of one row in the matrix and to compute
the length of one column in the matrix:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
colen~::~[[a]]~\char'31~{\itshape Int}\\
\texfamily colen~=~length\\
\texfamily \\
\texfamily rowlen~::~[[a]]~\char'31~{\itshape Int}\\
\texfamily rowlen~[]~~~~~=~0\\
\texfamily rowlen~[x:\char95 ]~~=~length~x\\
\texfamily \\
\texfamily columnLength~::~{\itshape Matrix}~a~\char'31~{\itshape Int}\\
\texfamily columnLength~({\itshape M}~ms)~=~colen~ms\\
\texfamily \\
\texfamily rowLength~::~{\itshape Matrix}~a~\char'31~{\itshape Int}\\
\texfamily rowLength~({\itshape M}~ms)~=~rowlen~ms
\end{tabbing}
\end{minipage}

The column length is equivalent to the number of rows
in the matrix; the row length is the length of the first row.
Again, in a matrix, all rows shall have
the same length; the first row, hence, serves
as a pattern for the other rows.

Gaussian elimination consists of two steps
(one of the improvements of Gauss-Jordan is
 that it consists of only one step,
 but applies this step with more consequence).
The first step brings the matrix into a special form,
often called \term{echelon} form.
In this form, the matrix contains a triangle
of zeros in the lower-left corner like this:

\[
\begin{pmatrix}
a_{0,0} & a_{0,1} & a_{0,2} & a_{0,3} & a_{0,4} \\
0       & a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
0       & 0       & a_{2,2} & a_{2,3} & a_{2,4} \\
0       & 0       & 0       & a_{3,3} & a_{3,4}
\end{pmatrix}.
\]

The echelon form of our matrix is as follows:

\[
\begin{pmatrix}
1 & 3 & -2  & 5\\
0 & 4 & -12 & 8\\
0 & 0 &   4 & 8
\end{pmatrix}
\]

The echelon form corresponds to a system of equations
where the last equation has been reduced to one unkown;
the last but one to two unknowns and so one until
the first that remains in its original form.

The second step consists in eliminating and
backsubstituting coefficients remaining in
the matrix. But let us first look 
at how to create the echelon form.
In Haskell this may be implemented as follows:\footnote{
This code is based on Matrix.hs, part of the Hugs system}

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~echelon~::~({\itshape Eq}~a,{\itshape Num}~a)~=>~{\itshape Matrix}~a~\char'31~{\itshape Matrix}~a\\
\texfamily ~~echelon~({\itshape M}~ms)~=~{\itshape M}~(go~ms)\\
\texfamily ~~~~{\bfseries where}~~go~::~({\itshape Eq}~a,{\itshape Num}~a)~=>~[[a]]~\char'31~[[a]]\\
\texfamily ~~~~~~~~~~~go~rs~~|~~null~rs~\char'37~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~null~(head~rs)~~=~~rs\\
\texfamily ~~~~~~~~~~~~~~~~~~|~~null~rs2~~~~~~~~=~~map~(0:)~(go~(map~tail~rs))\\
\texfamily ~~~~~~~~~~~~~~~~~~|~~otherwise~~~~~~~=~~piv~:~map~(0:)~(go~rs')\\
\texfamily ~~~~~~~~~~~~~{\bfseries where}~~rs'~~~~~~~~~~~~~~=~~map~(adjustWith~piv)~(rs1++rs3)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~(rs1,rs2)~~~~~~~~=~~span~(\char'10(n:\char95 )~\char'31~n\char'360)~rs\\
\texfamily ~~~~~~~~~~~~~~~~~~~~(piv:rs3)~~~~~~~~=~~rs2\\
\texfamily \\
\texfamily ~~adjustWith~::~({\itshape Num}~a)~=>~[a]~\char'31~[a]~\char'31~[a]\\
\texfamily ~~adjustWith~(m:ms)~(n:ns)~=~zipWith~(-)~~(map~(n*)~ms)~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(map~(m*)~ns)
\end{tabbing}
\end{minipage}

We first look at \text{\texfamily adjustWith}. This function takes two 
lists (of equal length), drops the first elements,
scales each of the lists multiplying by the first element
of the respective other list and
zips the result together by subtracting 
the corresponding elements.
Note that, if we not dropped the first elements,
they would be multiplied by the first element 
of the respective other list; in consequence,
both lists would begin with $nm$. Subracting one list
from the other would result in a list with a leading
zero. The function, instead, just drops the leading element.

Now let us look at how \text{\texfamily adjustWith} is used in \text{\texfamily echelon}.
The main work in \text{\texfamily echelon} is done in the local function \text{\texfamily go}.
This function has two base cases:
\begin{enumerate}
\item
If the input matrix \text{\texfamily rs} is null (it contains no rows)
or if its first element is null (it contains only empty rows),
the input is already in echelon form and we give it back as is.
\item
We look at the local variable \text{\texfamily rs2}. This variable is generated
as the second element of a tuple resulting from 
\text{\texfamily span\Sp (\char'10(n:\char95 )\char'31n\char'360)}, 
\ignore{\)}
i.e. \text{\texfamily rs1} will contain the rows with leading zeros 
and \text{\texfamily rs2} will contain those without leading zeros.
If \text{\texfamily rs2} is the empty list, all rows in \text{\texfamily rs} have 
at least one leading zero; we, therefore, ignore this step
and continue with the tail of all rows, adding the zero
that we ignored here to the final result again.
\end{enumerate}

Now, in the \text{\texfamily otherwise} branch, we use the \text{\texfamily adjustWith} function.
It is used to generate the local variable \text{\texfamily rs'}. Look at how
this variable is generated: \text{\texfamily (adjustWith\Sp piv)} is mapped on 
the concatenation \text{\texfamily rs1++rs3}. We already know the variable
\text{\texfamily rs1}: it contains the rows of \text{\texfamily rs} with leading zeros.
The second list, \text{\texfamily rs3}, is created from \text{\texfamily rs2} as \text{\texfamily (piv:rs3)}.
The pivot (\text{\texfamily piv}), hence, is the first row without leading zero
and \text{\texfamily rs3} consists of all other rows.
In other words: we use one row (the pivot) to eliminate
one variable from all rows.
From here, it is simple: we just apply \text{\texfamily go} 
once again on the result \text{\texfamily rs'}
until one of the base cases applies. On each step,
we insert zero as head to all rows in the result matrix 
and, finally, add one more row: the pivot that now contains
on more column with a value $\neq 0$ than the rows in
the result matrix. The code looks a bit scary on the first sight,
but, after going through it step by step, 
it turns out to be quite simple.
But let us go through an example:
in the first instance of \text{\texfamily go}, we compute

\begin{minipage}{\textwidth}
\text{\texfamily (rs1,rs2)\Sp =\Sp ([],m)}, where $m$ contains all lines of the matrix;\\
\text{\texfamily (piv,rs3)\Sp =\Sp ([1,3,-2,5],rs3)}, where \text{\texfamily rs3} contains the last two lines.
\end{minipage}

For \text{\texfamily adjustWith\Sp piv}, we compute, for the first line of \text{\texfamily rs3}:

\begin{equation}
\begin{array}{crrrr}
  &  3 & 9 & -6  & 15\\
- &  3 & 5 &  6  &  7\\
= &  0 & 4 & -12 &  8
\end{array}
\end{equation}

and for the second:

\begin{equation}
\begin{array}{crrrr}
  &  2 & 6 & -4 & 10\\
- &  2 & 4 &  3 &  8\\
= &  0 & 2 & -7 &  2
\end{array}
\end{equation}.

With these results, we repeat the process computing

\begin{minipage}{\textwidth}
\text{\texfamily (rs1,rs2)\Sp =\Sp ([],m)}, where $m$ now contains the two results computed above;\\
\text{\texfamily (piv,rs3)\Sp =\Sp ([4,-12,8],[[2,-7,2]])}.
\end{minipage}

For \text{\texfamily adjustWith\Sp piv}, we compute:

\begin{equation}
\begin{array}{crrr}
  & 8 & -24 & 16\\
- & 8 & -28 &  8\\
= & 0 &   4 &  8
\end{array}
\end{equation}

Now, going back, we add heading zeros to the rows and,
per recursion, the pivot resulting in the matrix:

\[
\begin{pmatrix}
1 & 3 & -2  & 5\\
0 & 4 & -12 & 8\\
0 & 0 &   4 & 8
\end{pmatrix}
\]

It should be clear, by the way, that \text{\texfamily echelon} just applies
the second method we discussed above:
it systematically scales equations (in \text{\texfamily adjustWith}) 
and subtracts them from each other.
Now you may guess that the second step of the algorithm
applies the first method, \ie\ eliminating variables
by solving and back-substituting -- and you are right:\footnote{
This code is based on \term{Haskell Road}}

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~backsub~::~{\itshape Matrix}~{\itshape Zahl}~\char'31~[{\itshape Quoz}]\\
\texfamily ~~backsub~({\itshape M}~ms)~=~go~ms~[]\\
\texfamily ~~~~{\bfseries where}~~go~~[]~~rs~~=~~rs\\
\texfamily ~~~~~~~~~~~go~~xs~~rs~~=~~go~xs'~(p:rs)\\
\texfamily ~~~~~~~~~~~~~{\bfseries where}~~a~~~~~~~~=~~(last~xs)~!!~((rowlen~xs)-2)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~c~~~~~~~~=~~(last~xs)~!!~((rowlen~xs)-1)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~p~~~~~~~~=~~c~\%~a\\
\texfamily ~~~~~~~~~~~~~~~~~~~~({\itshape M}~xs')~~=~~eliminate~p~\$~{\itshape M}~(init~xs)\\
\texfamily \\
\texfamily ~~eliminate~::~{\itshape Quoz}~\char'31~{\itshape Matrix}~{\itshape Zahl}~\char'31~{\itshape Matrix}~{\itshape Zahl}\\
\texfamily ~~eliminate~r~({\itshape M}~ms)~=~{\itshape M}~(map~(simplify~n~d)~ms)\\
\texfamily ~~~~{\bfseries where}~~n~~=~~numerator~~~r\\
\texfamily ~~~~~~~~~~~d~~=~~denominator~r\\
\texfamily ~~~~~~~~~~~simplify~n~d~row~~=~~init~(init~row')~++~[d*lr~-~al*n]\\
\texfamily ~~~~~~~~~~~~~{\bfseries where}~~lr~~~~~~~=~~last~row\\
\texfamily ~~~~~~~~~~~~~~~~~~~~al~~~~~~~=~~last~(init~row)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~row'~~~~~=~~map~(*d)~row
\end{tabbing}
\end{minipage}
\ignore{$}

Note that, for sake of the topic of this section, 
we have adapted the code
to a specific data type.
The function \text{\texfamily backsub} expects a system with 
integer coefficients and presents a
result of rational numbers.

In \text{\texfamily backsub}, we call the local function \text{\texfamily go},
which receives the input matrix and an empty result set.
When the input matrix is exhausted, we yield the result set.
Otherwise, we create the local variables \text{\texfamily xs'} and \text{\texfamily p}.
The latter is a rational number generated by dividing
the last element of the last row 
by the penultimate element of that same row.
This number, the quotient of the last 
and the last but one element of the last row,
is the first element of the result set.

What does that mean? Well, look at the last line of the matrix.
It reads $0,0,4,8$. That is: it contains only two elements.
The penultimate element, 4, is the coefficient of the last
unknown, $z$, while the last element, 8, is the constant value
on the right-hand side of this equation with one unknown.
The last row can thus be rephrased as:

\[
4z = 8.
\]

That we divide the last element by the last but one
corresponds to the simple manipulation that divides
both sides of the equation by 4, \ie\

\[
z = \frac{8}{4} = 2.
\]

The other variable \text{\texfamily xs'} is generated by eliminating $p$
from the other lines. Eliminating works as follows:
We first multiply all elements in every row
by the denominator of $p$.
This corresponds to the following operation;
the last but one row of the matrix, for instance, is

\[
4y - 12z = 8.
\]

When we substitute $z$ by $\frac{8}{4}$ (which, of course,
is 2, but let us look at the fraction), we get:

\[
4y - \frac{12\times 8}{4} = 8.
\]

We now get rid of the denominator, by multiplying both sides by 4:

\[
16y - 12\times 8 = 32.
\]

In the code above, we start by performing this second step,
\ie\ multiplying by the denominator.
Note, however, that we later continue to compute with
the last and the last but one element of \text{\texfamily row}, not of \text{\texfamily row'}.
In other words, we multiply the denominator only by the elements
that precede the penultimate and 
leave the last two elements as they are.

We then take the last two elements,
multiply the last one by the denominator and
the penultimate one by the numerator and
subtract the latter from the former.
That is, we get rid of the denominator, apply
multiplication of the numerator to the value
that represents $z$ and subtract it from both sides.
In abstract algebraic notation, that would look like:

\[
ay + bz = c.
\]

We know that $z=\frac{n}{d}$, so we can substitute
the second term for $\frac{bn}{d}$.
We multiply by $d$ and get:

\[
ady + bn = cd.
\]

Now, we subtract $bn$ from both sides and get

\[
ady = cd - bn.
\]

Voilà, we have reduced an equation with two unknowns
to an equation with only one unknown, namely $y$.
This elimination step is applied to all rows
(but the last). Then, the process is repeated
using as input the reduced rows.

Let us go through the whole example.
The echelon form of our system is

\[
\begin{pmatrix}
1 & 3 & -2  & 5\\
0 & 4 & -12 & 8\\
0 & 0 &   4 & 8
\end{pmatrix}
\]

We look at the last line $0,0,4,8$.
We set 

\[
p = \frac{8}{4} = \frac{2}{1}.
\]

We then call \text{\texfamily eliminate\Sp p} on the first two lines
of the matrix. Processing the first line,
we compute \text{\texfamily map\Sp (*1)\Sp row}, which we can ignore.
We then set

\[
lr = 1\times 5,
al = 2\times -2
\]

and compute $lr - al$, which is 9.
The complete result for the first row,
hence, is $1,3,9$.

For the second row $0,4,-12,8$, \text{\texfamily eliminate} computes

\[
lr = 1\times 8,
al = 2\times -12
\] 

and further computes $lr - al$, \ie\ $8 + 24 = 32$.
The complete result for the second row, hence, is
$0,4,32$. After application of \text{\texfamily eliminate}, \text{\texfamily xs'}
is thus:

\[
\begin{pmatrix}
1 & 3 & 9\\
0 & 4 & 32
\end{pmatrix}
\]

Now, in \text{\texfamily backsub}, we repeat the process with this result.
We, again, look at the last line, which now is $0,4,32$.
We set 

\[
p = \frac{32}{4} = 8.
\]

This goes into the result set and, as you may remember,
is the result for $y$.

We apply \text{\texfamily eliminate} on the remaining row, which is $1,3,9$.
We set

\[
lr = 1\times 9 = 9,
al = 8\times 3 = 24
\]

and compute $lr - al$, \ie\ $9-24=-15$.
The complete result for this instance of \text{\texfamily eliminate}, hence, is
$1,-15$.

We, again, repeat the \text{\texfamily backsub} process with this result.
There is only one row left and from this line we compute
$p$ as 

\[
p = \frac{-15}{1} = -15,
\]

which, as you may remember, is the result for $x$.
Since \text{\texfamily init\Sp xs} is now \text{\texfamily []}, \text{\texfamily eliminate} will return \text{\texfamily []}
and this terminates the process with the correct result 
\text{\texfamily [-15,8,2]}.
\section{Binomial Coefficients are Integers}
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape BinomInt}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}\\
\texfamily ~~{\bfseries import}~{\itshape Fraction}\\
\texfamily ~~{\bfseries import}~{\itshape Binom}
\end{tabbing}
}

To prove that binomial coefficients are integers
is quite easy. 
We will make our argument for
coefficients of the form
$1 \le k \le n$ in $\binom{n}{k}$. 
For cases outside of this range,
the coefficients are defined as 1 for $k=0$,
and as 0 for $k > n$.
So, there is nothing to prove in these cases.

We have the equation

\begin{equation}
\binom{n}{k} = \frac{n^{\underline{k}}}{k!}.
\end{equation}

We can prove that $\binom{n}{k}$ is an integer
for any $n$ and any $k \le \frac{n}{2}$ by 
induction. 
The induction argument holds, until we have $k > \frac{n}{2}$.
At this moment, the factors in the numerator and denominator
begin to intersect and the factors now common in
numerator and denominator are cancelled out
leading to a corresponding case
in the lower half of $k$s, namely the case $k=n-k$.
You can easily convince yourself by trying 
some examples that this is the reason for 
the symmetry in Pascal's triangle.

Since, in the proof, 
we have already handled the cases 
in the lower half of $k$s by induction up to $\frac{n}{2}$,
there is nothing that still needs to be proven.
The proof, therefore, consists in the induction argument
that if $\binom{n}{k}$ is an integer,
then $\binom{n}{k+1}$ is an integer too
for $k \le \frac{n}{2}$.

We first handle the trivial case
$k=1$. Here, we have $\frac{n}{1!=1}$,
which, trivially, is an integer.
Note that, for this case, the falling factorial,
which is defined as the product of the
consecutive numbers $n$ to $n-k+1$, 
is just $n$, since $n-1+1 = n$.

For $k=2$, we have

\[
\frac{n(n-1)}{2}.
\]

In the numerator we have 
a product of two consecutive numbers,
one of which must be even. 
We, hence, divide the even one
and the denominator by 2 and have an integer.

For $k=3$, we have

\[
\frac{n(n-1)(n-2)}{6}.
\]

We now have three consecutive numbers
as factors in the numerator and $3! = 6$
in the denominator. One of three consecutive 
numbers must be divided by 3, since there are only
two numbers between any two multiples of 3.
If, for instance, $n=11$, then $n$ and $n-1 = 10$
are not divided by 3, but $n-2=9$ is.
So we reduce the problem to $k-1=2$.
But note that there is a difference 
between the previous case $k=2$
and the case $k-1$ at which we are arriving now:
In the previous case, we had two consecutive numbers
in the numerator, but now we have three numbers
that are not necessarily consecutive anymore.
It may have been the middle number, $n-1$,
that we divided by 3; then, if $n$ is odd,
$n-2$ is odd as well.
However, if $n$ and $n-2$ are not even,
then $n-1$ must have been and then it must
have been divisible by 3 and 2.
In consequence, even though the numbers
are not consecutive anymore,
the divisibility argument still holds.

This is how the induction argument works:
for any $k+1$, we have $k+1$ consecutive
factors in the numerator and we have
$(k+1)\times k!$ in the 
denominator. Since there are $k+1$ consecutive numbers
in the numerator, all greater $k+1$, 
one of them must be divided by $k+1$.
By dividing this number and the denominator by $k+1$,
we reduce the problem to $k$ with $k$ factors
in the numerator and $k!$ in the denominator.
Now, the factors are not consecutive anymore,
but that does not affect the argument:
either the number that was reduced by dividing
by $k+1$ is divided by $k$ as well
and then we have reduced the problem to $k-2$ already,
or, if it is not divided by $k$, then 
one of the other numbers must be,
because we started with $k+1$ numbers in the first place.\qed

Let us quickly look at the next example, $k=4$,
just to illustrate the argument once again.
With $k=4$ we have the fraction

\[
\frac{n(n-1)(n-2)(n-3)}{24}.
\]

There are four consecutive numbers in the numerator,
one of which must be divided by 4. We divide this number
by 4 and the problem is reduced to the case $k=3$.
Again, the numbers are not consecutive anymore.
But if the number that we reduce by dividing by 4
is divisible by 3, then we would have reduced
the problem to $k=2$ already. Otherwise, that
number was just a number between two multiples of
3 and the argument does not suffer.

A concrete example makes the argument entirely clear.
Consider $\binom{11}{4} = \frac{11\times 10 \times 9 \times 8}{24}$.
One of the numbers in the numerator must divide 4.
11 does not, 10 and 9 either, but 8 does.
We divide numerator and denominator by 4 and,
with that, reduce the problem to 
$\frac{11\times 10 \times 9 \times 2}{6}$.
There are 3 consecutive numbers in the numerator,
one of which must be divided by 3.
11 and 10 are not divided by 3,
but 9 is and, hence, we reduce the problem to
$\frac{11\times 10 \times 3 \times 2}{2}$,
which was our base case. 
We now have the choice to either cancel 2
in the numerator and 2 in the denominator
or to divide 10 by 2 in the numerator and
cancel 2 in the denominator.
If we choose the latter,
we divide 10 by 2 and obtain
$\frac{11\times 5 \times 3 \times 2}{1} = 11 \times 5 \times 3 \times 2$,
which is $55 \times 6 = 330$.

When calculating binomial coefficients by hand,
we see that the main activity in this process
is to cancel numbers in the numerator and the
denominator.
The question arises if we could not spare
a lot of computing by using numbers
that are already reduced before we start working with them.
In particular, we see that the reduction
of the numerator tends towards the prime
factors of the binomial coefficient.
We know that finding the prime factors
is a very hard problem.
But could there not be a 
shortcut to the binomial coefficient
by using prime factors?

It turns out there is.
The solution, however,
sounds a bit surreal,
since it combines facts
that, on the first sight, are completely
unrelated.
The point is that there is a way
to quickly decide for any
prime number $p$ whether it is part
of the prime factorisation of a 
binomial coefficient and
to even determine how often it occurs
in its prime factorisation by counting
the number of $borrows$ we have to make
subtracting $k$ from $n$ in the numeral system
of base $p$.

To determine how often (or if at all) 
2 appears in the prime factorisation of $\binom{6}{2}$,
we would perform the subtraction $6-2$ in
binary format.
6 in binary format is 110 and
2 is just 10.
So, we subtract:

\begin{tabular}{r r r r}
    & 1 & 1 & 0 \\
$-$ &   & 1 & 0 \\
$=$ & 1 & 0 & 0
\end{tabular}

The final result 100 is 4
in decimal notation and, hence, correct.
Since we have not borrowed once,
2 is not a factor of $\binom{6}{2}$.
We check the next prime number 3.
6 in base 3 is 20 and 2 is just 2:

\begin{tabular}{r r r}
    & 2 & 0 \\
$-$ &   & 2 
\end{tabular}

Now we have to borrow
to compute 0 - 2,
so we have:

\begin{tabular}{r r r}
    & 2             & 0 \\
$-$ & \underline{1} & 2 \\
$=$ & 1             & 1 
\end{tabular}

11 base 3 is 4 in the decimal system,
so the result is correct.
Furthermore, we had to borrow once
to compute this result.
We, therefore, claim that 3 appears once
in the prime factorisation of $\binom{6}{2}$.

We look at the next prime, 5.
6 in base 5 is 11 and 2 in base 5 is just 2 again.
So we have:

\begin{tabular}{r r r}
    & 1 & 1 \\
$-$ &   & 2 
\end{tabular}

Again, we have to borrow
to compute 1 - 2:

\begin{tabular}{r r r}
    & 1             & 1 \\
$-$ & \underline{1} & 2 \\
$=$ & 0             & 4 
\end{tabular}

Since 4 in base 5 is just decimal 4,
the result, again, is correct.
To reach it, we had to borrow once
and we, therefore, claim that 5
appears once in the prime factorisation of
$\binom{6}{2}$.

The next prime would be 7,
but we do not need to go on,
since 7 is greater than $n=6$.
Because we multiply $n$ only by values
less than $n$ (namely: $n-1,n-2,\dots,n-k+1$),
7 cannot be a factor of such a number.
Our final result, thus, is:
$\binom{6}{2} = 3^1 \times 5^1 = 3 \times 5 = 15$.
Let us check this result against our usual method
to compupte the binomial coefficient:
$\frac{6 \times 5}{2} = 3 \times 5 = 15$.
The result is correct.

But what, on earth, has the factorisation
of binomial coefficients to do with 
the borrows in $n-k$? 
The link is the following theorem:

\begin{equation}
\binom{n}{k} \equiv \prod_{i=0}^{r}{\binom{a_i}{b_i}} \pmod{p},
\end{equation}

where the $a$s and $b$s are the coefficients
in the representation of $n$ and $k$ 
in base $p$:

\begin{equation}
n = a_rp^r + a_{r-1}p^{r-1} + \dots + a_1p + a_0
\end{equation}

and

\begin{equation}
k = b_rp^r + b_{r-1}p^{r-1} + \dots + b_1p + b_0.
\end{equation}

The theorem, hence, claims that
$\binom{n}{k}$ is congruent
to the product of the coefficients
in the representation base $p$ 
modulo that $p$.
We are back to congruences and
modular arithmetic!

The theorem is a corollary of \term{Lucas' theorem},
which we will now introduce as a lemma to prove
the theorem above.
Lucas' theorem states that

\begin{equation}\label{eq:lucas1}
\binom{n}{k} \equiv \binom{\lfloor n/p\rfloor}{\lfloor k/p\rfloor}
                    \binom{n \bmod p}{k \bmod p} \pmod{p},
\end{equation}

which is exceptionally beautiful, since
it decomposes $n$ and $k$
into the two parts of the Euclidian division,
the quotient $\lfloor n/p \rfloor$ and
the remainder $n \bmod p$.
Let us rename the quotient and remainder
of $n$ and $k$, because we will refer
to them quite often in this section:
let $u = \lfloor n/p \rfloor$ and
    $v = n \bmod p$, such that
$n = up + v$, and
let $s = \lfloor k/p \rfloor$ and
    $t = k \bmod p$, such that
$k = sp + t$.
We can now rewrite the 
usual computation of the coefficient

\begin{equation}
\binom{n}{k} = \frac{n}{k} \times \frac{n-1}{k-1} \times \dots \times \frac{n-k+1}{1}
\end{equation}

as

\begin{equation}
\binom{n}{k} = \frac{up + v}{sp+t} \times \frac{up+v-1}{sp+t-1} \times \dots 
        \times \frac{up+v-k+1}{1}.
\end{equation}

This formula leads to a
cyclic repetition of denominators of the form

\[
sp + t - 1, sp + t - 2, \dots, sp + t - t.
\]

We have to be careful with the denominators
of the form $sp + t - t = sp$, since, modulo $p$,
they are just zero and the corresponding
fraction is thus undefined.
But before we get into it, 
let us look at the $t$ very first numbers,
that is the fractions, before the formula
reaches a multiple of $p$ for the first time.
These fractions are:

\[
\frac{up+v}{sp+t} \times
\frac{up+v-1}{sp+t-1} \times \dots \times
\frac{up+v-t+1}{sp+t-t+1},
\]

which modulo $p$ is 

\[
\frac{v}{t} \times
\frac{v-1}{t-1} \times \dots \times
\frac{v-t+1}{1}.
\]

This, in its turn, is just
the usual way to define the
binomial coefficient for $v$ and $t$:

\begin{equation}
\binom{v}{t} = 
\frac{v}{t} \times
\frac{v-1}{t-1} \times \dots \times
\frac{v-t+1}{1}.
\end{equation}

But $v$ and $t$ are
$n \bmod p$ and $k \bmod p$ respectively
and substituting back these values for $v$ and $t$
in the equation leads to

\begin{equation}
\binom{n \bmod p}{k \bmod p} = 
\frac{n \bmod p}{k \bmod p} \times
\frac{(n \bmod p)-1}{(k \bmod p)-1} \times \dots \times
\frac{(n \bmod p)-(k \bmod p)+1}{1}
\end{equation}

and we conclude 

\begin{equation}\label{eq:lucasX}
\binom{n}{k} \equiv \binom{n \bmod p}{k \bmod p} X \pmod{p},
\end{equation}

where $X$ is the rest of the product after 
the first $t$ factors we are looking at right now.

Consider the example $\binom{90}{31}$ and
the prime 7. For $n$, we get
$u = 90 / 7 = 12$ and, since $12 \times 7 = 84$,
we have $v = 90 \bmod 7 = 6$.
For $k$, we get
$s = 31 / 7 = 4$ and, since $4 \times 7 = 28$,
we have $t = 31 \bmod 7 = 3$.
The first $t$ factors, we have looked at so far
are

\[
\frac{90 \times 89 \times 88}{31 \times 30 \times 29}.
\]

We take all factors in numerator
and denominator modulo 7:

\[
\frac{6 \times 5 \times 4}{3 \times 2 \times 1},
\]

which, as you can see, is just
$\binom{6}{3} = \binom{90 \bmod 7}{31 \bmod 7}$.

Now we will look at the ominous $X$.
Since $X$ is the product with the first
$k \bmod p$ factors cut off and
the number of factors in the entire product is $k$,
the number of the remaining factors 
is a multiple of $p$.
These factors fall into $\frac{k}{p}$ groups
each of which contains 
in the numerator and the denominator
one multiple of $p$ and $p-1$ remainders of $p$.
Let us look at the denominators of such a group: 

\[
\frac{\dots}{sp} \times \frac{\dots}{sp+1} \times \dots \times
\frac{\dots}{sp+p-1}.
\]

Since the whole is a product,
these values are multiplied with each other
and, as we know from Wilson's theorem,
the factorial of $p-1$ is $(p-1)! \equiv p-1 \pmod{p}$.
We certainly have the same remainders in the numerators
modulo $p$, which, again according to Wilson,
are $p-1$ modulo $p$ and, therefore, cancel out.
We are then left with the factors that are multiples
of $p$.

Continuing the example, the first of such groups would be

\[
\frac{87 \times 86 \times 85 \times 84 \times 83 \times 82 \times 81}
     {28 \times 27 \times 26 \times 25 \times 24 \times 23 \times 22}
\]

Here, 84 in the numerator and 28 in the denominator are 
multiples of 7. All other numbers are remainders.
In the denominator, we have a complete group of remainders
$22\dots 27$, which are $1\dots 6$ modulo 7.
These multiplied with each other are,
according to Wilson's theorem, congruent to 6 modulo 7.
In the numerator, we do not see the whole group at once.
Instead, we see two different parts of two groups
separated by 84, the multiple of 7, in the middle:
$85,86,87$, which are congruent to 1, 2 and 3 modulo 7,
and $81,82,83$, which are congruent to 4, 5 and 6 modulo 7.
Multiplying these remainders is, again
according to Wilson's theorem, congruent to 6 modulo 7.
So, we cancel all these values in numerator and denominator
and keep only

\[
\frac{84}{28}.
\]

As already observed, we have 
$\lfloor k/p\rfloor = \lfloor 31/7\rfloor = 4$ of such groups.
We are therefore left with 4 fractions with a multiple of 7 
in the numerator and the denominator, namely the factors:

\[
\frac{84}{28} \times \frac{77}{21} \times \frac{70}{14} \times \frac{63}{7}.
\]

When we divide numerator and denominator by 7,
we get

\[
\frac{12}{4} \times \frac{11}{3} \times \frac{10}{2} \times \frac{9}{1}.
\]

and see by this simple trick of black magic that the result is 

\[
\binom{12}{4} =
\binom{\lfloor 90/7\rfloor}{\lfloor 31/7\rfloor} =
\binom{\lfloor n/p\rfloor}{\lfloor k/p\rfloor}.
\]

Unfortunately, there are very few scholars left
who would accept magic as proof and so
we must continue with the abstract reasoning.
Note again that we have taken the first $t=k \bmod p$ terms
out in the previous step. The denominators 
we are left with, when we arrive at fractions with
multiples of $p$ in the numerator and denominator, are therefore
$k - (k \bmod p), k - (k \bmod p) - p, k - (k \bmod p) - 2p, \dots, 1$.
In the example above, 28 corresponds to $k - (k \bmod p)$:
$31 - 3 = 28$, 21 corresponds to $k - (k \bmod p) - p$ and so on.

The numerators are not so clean, but very similar:
$n - (k \bmod p) - x, n - (k \bmod p) - x - p, \dots$
The $x$ in this formula results from the fact
that $n - (k \bmod p)$ does not necessarily result
in a multiple of $p$. For instance, $90 - 3 = 87$ is not
a multiple of $p$. $x$ in this case is 3, since
$90 - 3 - 3 = 84$, which is a multiple of $p$.
In fact, we can determine the value of $x$ more specifically
as $(n \bmod p) - (k \bmod p)$, which is $6 - 3 = 3$,
but we do not need to make use of this fact.
It is sufficient to realise that each value
must be divisible by $p$ and, hence, $(k \bmod p) + x < p$.
When we now divide by $p$, we get for each factor

\[
\frac{\lfloor (n - (k \bmod p) - x_i - a_ip) / p\rfloor}
     {\lfloor (k - (k \bmod p) - b_ip) / p\rfloor},
\]

where the $a$s and $b$s run from 0 to the number of groups we have minus 1,
\ie\ $\lfloor k/p \rfloor - 1$.

Since the second term of the differences in 
numerator and denominator are remainders of $p$ that,
together with $n$ and, respectively, $k$, are
multiples of $p$, this is just the same as saying

\[
\frac{\lfloor (n-a_ip)/p\rfloor}{\lfloor (k-b_ip)/p\rfloor},
\]

which of course is

\[
\frac{\lfloor n/p\rfloor - a_i}{\lfloor k/p\rfloor - b_i}.
\]

Since the $a$s and $b$s run from 0 to the number of the last group,
we get this way the product

\[
\frac{\lfloor n/p\rfloor}{\lfloor k/p\rfloor} \times
\frac{\lfloor n/p\rfloor - 1}{\lfloor k/p\rfloor - 1} \times
\frac{\lfloor n/p\rfloor - 2}{\lfloor k/p\rfloor - 2} \times \dots \times
\frac{\lfloor n/p\rfloor - \lfloor k/p\rfloor + 1}
     {\lfloor k/p\rfloor - \lfloor k/p\rfloor + 1}, 
\]

which we immediately recognise as the computation for 

\[
\binom{\lfloor n/p\rfloor}
      {\lfloor k/p\rfloor}.
\]

You, hopefully, remember that this 
is the $X$, we left over in equation \ref{eq:lucasX}.
Substituting for $X$ we derive the intended result:

\begin{equation}
\binom{n}{k} \equiv \binom{n \bmod p}{k \bmod p} 
                    \binom{\lfloor n/p\rfloor}
                          {\lfloor k/p\rfloor}\pmod{p}
\end{equation}

and this completes the proof.\qed

But we have to add an important remark.
Binomial coefficients with $k > n$
are defined to be zero.
The equation, thus, tells us that
the prime $p$ divides the coefficient
if $k \bmod p > n \bmod p$.
For instance $\binom{8}{3}$, which is 
$\frac{8\times 7 \times 6}{6} = 8 \times 7 = 56$,
is divided by 7, since 7 appears as a factor
in the numerator and, indeed:
$\binom{8 \bmod 7}{3 \bmod 7} = \binom{1}{3}$.
This would also work with $\binom{9}{3}$,
which is $\frac{9\times 8 \times 7}{6} = 3 \times 4 \times 7 = 84$,
where 7, again, appears as a factor in the numerator
and $\binom{9 \bmod 7}{3 \bmod 7} = \binom{2}{3}$.
It does not work with $\binom{9}{2}$,
which is $\frac{9\times 8}{2} = 9 \times 4 = 36$,
since $\binom{9 \bmod 7}{2 \bmod 7} = \binom{2}{2} = 1$ and,
indeed: $36 \bmod 7 = 1$.
Let us memorise this result:
a prime $p$ divides a binomial coefficient $\binom{n}{k}$,
if $k \bmod p > n \bmod p$.

We, finally, come to the corollary,
which we wanted to prove in the first place.
We need to prove that

\begin{equation}
\binom{n}{k} \equiv \prod_{i=0}^{r}{\binom{a_i}{b_i}} \pmod{p},
\end{equation}

where the $a$s and $b$s are the coefficients in the
representation of $n$ and $k$ base $p$.
We now calculate $u,v,s$ and $t$, as we have done
before, as 
$u = \lfloor n/p\rfloor$,
$v = n \bmod p$, which is just the last coefficient 
in the $p$-base representation of $n$ $a_0$,
$s = \lfloor k/p\rfloor$ and
$t = k \bmod p$, which is just the last coefficient $b_0$.

The $p$-base representations of $n$ and $k$ are
$n = a_rp^r + \dots + a_1p + a_0$ and
$k = b_rp^r + \dots + b_1p + b_0$. 
If we divide those by $p$, we get
$u = a_rp^{r-1} + \dots + a_1$ and
$s = b_rp^{r-1} + \dots + b_1$
with $a_0$ and $b_0$ as remainders.

From Lucas' theorem we conclude that

\begin{equation}
\binom{n}{k} \equiv \binom{u}{v}\binom{a_0}{b_0} \pmod{p}.
\end{equation}

Now we just repeat the process for $u$ and $v$:

\begin{equation}
\binom{n}{k} \equiv \binom{\lfloor u/p\rfloor}
                          {\lfloor v/p\rfloor}
                    \binom{a_1}{b_1}
                    \binom{a_0}{b_0} \pmod{p}
\end{equation}

and continue until we have

\begin{equation}\label{eq:lucasCor}
\binom{n}{k} \equiv \binom{a_r}{b_r}
                    \dots
                    \binom{a_1}{b_1}
                    \binom{a_0}{b_0} \pmod{p},
\end{equation}

which then concludes the proof.\qed

We see immediately that $p$ divides
$\binom{n}{k}$, when at least one
digit of $k$ in the $p$-base representation
is greater than the corresponding
digit of $n$, because, in this case,
the corresponding binomial coefficient
is zero and, in consequence, the whole
product modulo $p$ becomes zero.

That a digit of $k$ is greater than
the corresponding digit of $n$ implies
that, on subtracting $k$ from $n$,
we have to borrow from the next place.
Therefore, if we have to borrow
during subtraction, then $p$ divides
$\binom{n}{k}$ and, thus, is a prime
factor of that number.

So, we divide $\binom{n}{k}$ by $p$
leading to the product in equation \ref{eq:lucasCor}
with one pair of digits removed
and search again for a pair of digits
where $k > n$.
If we find one, the number
$\binom{n}{k}$ is divided twice by $p$,
so $p$ appears twice in the factorisation
of that number.
We again divide by $p$ and repeat the process
until we do not find a pair $k > n$ anymore.
Then we know how often $p$ appears in
the prime factorisation of $\binom{n}{k}$.
If we do this for all primes $\le n$,
we learn the complete prime factorisation
of $\binom{n}{k}$.

We now will implement this logic in Haskell.
We start with the notion of borrows:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~borrows~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~borrows~~\char95 ~0~\char95 ~\char95 ~\char95 ~=~0\\
\texfamily ~~borrows~~p~u~v~s~t~~|~v~<~t~=~1~+~~borrows~~p~~(u~`div`~p)~(u~`rem`~p)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(s~`div`~p)~((s~`rem`~p)~+~1)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~|~otherwise~=~~borrows~~p~~(u~`div`~p)~(u~`rem`~p)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(s~`div`~p)~(s~`rem`~p)
\end{tabbing}
\end{minipage}

The function \text{\texfamily borrows} takes five arguments
all of our old type \text{\texfamily {\itshape Natural}}.
The first argument is the prime;
the next four arguments are $u$, $v$, $s$ and $t$,
that is 
$u = \lfloor n/p \rfloor$,
$v = n \bmod p$,
$s = \lfloor k/p \rfloor$ and
$t = k \bmod p$.

If $u=0$, we are through and no more borrows
are to be found.
Otherwise, if $v < t$, we have to borrow.
The borrow is actually seen in the recursive
call of borrows, where we increment $s \bmod p$ by 1.
We also add 1 to the overall result.
Otherwise, we call borrows with the quotients
and remainders of $u$ and $s$.
The recursion implements the logic we see
in equation \ref{eq:lucasCor}:
we reduce the product factor by factor
by dividing by $p$;
on each step, we check if a borrow occurs
and continue with the next step.

This is the heart of our algorithm.
However, we can improve on this.
There are cases we can decide 
immediately without going through 
the whole process.
Consider a prime $p \le n-k$.
Since the numerator in the computation
of the binomial coefficient
runs from $n\dots (n-k+1)$,
this prime will not appear directly
in the numerator. It could of course
still be a factor of one of the numbers
$n,n-1,\dots,n-k+1$ and, as such, be
a factor of the resulting number. But then it
must be less than or at most equal to
the half one of those numbers. Otherwise,
there would be no prime number by which
we could multiply $p$ to obtain one
of those number.
In consequence, when $p \le n-k$ and
$p > n/2$, $p$ cannot divide $\binom{n}{k}$.

On the other hand, if $p > n-k$,
then it appears in the numerator;
if also $p > k$, then it will not
appear in the denominator.
In consequence, if $p > k$ and $p > n-k$,
then it will not be cancelled out
and is therefore prime factor of
the binomial coefficient.
Furthermore,
it can appear only once in the numerator.
There are only $k$ consecutive numbers
in the numerator and $p > k$.
If we assume there is a factor
$ap$ with $a \ge 1$, then 
we will reach $n$ in one direction
and $n-k+1$ in the other direction,
before we reach either $(a+1)p$ or $(a-1)p$.
Therefore, $a=1$ or, in other words,
$p$ appears only once
in the prime factorisation.

Finally, if $n \bmod p < k \bmod p$,
we know for sure that $p$ divides
the number at least once.
If $p^2 > n$, then we know
that $p$ divides $\binom{n}{k}$
exactly once.

We will implement these one-step decisions
as filters in a function that calls \text{\texfamily borrows}:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~powOfp~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~powOfp~n~k~p~~|~~p~\char'34~n~-~k~\char'04~p~>~n~`div`~2~~~~~~~=~0\\
\texfamily ~~~~~~~~~~~~~~~~|~~p~>~k~\char'04~p~>~n~-~k~~~~~~~~~~~~~~~~=~1\\
\texfamily ~~~~~~~~~~~~~~~~|~~p*p~>~n~\char'04~n~`rem`~p~<~k~`rem`~p~~=~1\\
\texfamily ~~~~~~~~~~~~~~~~|~~otherwise~=~borrows~p~~(n~`div`~p)~(n~`rem`~p)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(k~`div`~p)~(k~`rem`~p)
\end{tabbing}
\end{minipage}

Now we implement a new variant of \text{\texfamily choose}
making use of borrows:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~choose3~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~choose3~n~k~~~=~product~(map~f~ps)\\
\texfamily ~~~~{\bfseries where}~~ps~~~=~~takeWhile~(\char'34~n)~allprimes\\
\texfamily ~~~~~~~~~~~f~p~~=~~p\char94 (powOfp~n~k~p)
\end{tabbing}
\end{minipage}

The implementation is quite simple and
there is certainly room for optimisation.
It just maps $p^{(\text{powOfp}~n~k~p)}$ on all primes
up to $n$ and builds the product of the result. 
A possible improvement is to use \text{\texfamily fold} instead of \text{\texfamily map}
and not to add primes to the result for which \text{\texfamily powOfp} yields 0.
That would reduce the size of the resulting
list of primes drastically. With \text{\texfamily map},
there are a lot of 1s, in fact, most of the elements
are 1 in most cases.

But let us investigate the running time
of \text{\texfamily choose3} compared to that of 
$\frac{n^{\underline{k}}}{k!}$
in more general terms.
The running time of the fraction is a function
of $k$, such as $2k-1$, since we have $k-1$ multiplications
in numerator and denominator and one division,
as already discussed in a previous chapter.
Since we are not too much interested in the details
of the operations, \ie\ the cost of single
multiplications and divisions and the cost
of \text{\texfamily borrows} for one prime, we will use the
\term{big-O} notation, for instance: $\mathcal{O}(2k-1)$.
This notation tells us that there is a function
of the form $2k-1$, which is a limit for our function,
the running time of the fraction.
In other words, for huge input values,
the function within the $\mathcal{O}$
is equal to or greater than our function.

The running time of \text{\texfamily choose3}, 
by contrast, is a function
of $\Pi(n)$, that is the number of primes up to $n$,
approxmimately $n/\ln{}n$.
In big-O notation, we state that the running time
of \text{\texfamily choose3} is $\mathcal{O}(n/\ln{}n)$.

For large $n$s and small $k$s, 
\eg\ $\binom{1000000}{2}$, the fraction appears to be much better.
Of course, the multiplications in the numerator are heavy,
since the complexity of multiplication grows with the number
of digits of the factors, but there are still only few such multiplications.
The multiplications in the denominator, in compensation, 
are trivial with small $k$s.

The complexity of \text{\texfamily choose3} for this specific number
is $1000000/\ln{}1000000 \approx 72382$
and, as such, of course much worse.
With larger $k$s, however, the picture changes.
Even though we can reduce the complexity of the fraction
for cases where $k > n/2$ to cases with $k < n/2$,
as discussed before,
there is sufficient room for $k$s around $n/2$
that makes \text{\texfamily choose3} much more efficient than the fraction.
In general, we can say that \text{\texfamily choose3} is more efficient,
whenever $\frac{n}{2\ln{}n} + 1 < k < n - \frac{n}{2\ln{}n} - 1$.
For the specific example of $n=1000000$, 
\text{\texfamily choose3} is more efficient for 
$36192 < k < 963807$. 

The fact underlying the algorithm,
the relation between the number of occurrences
of a prime $p$ in the factorisation of a
binomial coefficient and the number of
borrows in the subtraction of $n$ and $k$
in base-$p$, was already known in the $19^{th}$
century, but was apparently forgotten during
the $20^{th}$ century.
It was definitely mentioned by German mathematician
Ernst Kummer (1810 -- 1893), but 
described as an algorithm apparently only
in 1987 in a short, but nice computer science paper
by French mathematician
Pascal Goetgheluck who rediscovered the relation
by computer analysis.

Lucas' theorem is named for Édouard Lucas (1842 -- 1891),
a French mathematician who worked mainly in number theory,
but is also known for problems and solutions in
recreational math. The \term{Tower of Hanoi}
is of his invention.
Lucas died of septicemia in consequence of a household accident
where he was cut by a piece of broken crockery. 
\section{Euler's Totient Function} % Carmichael Theorem
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Totient}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}\\
\texfamily ~~{\bfseries import}~{\itshape Gen}\\
\texfamily ~~{\bfseries import}~{\itshape Quoz}
\end{tabbing}
}

The constructor function \text{\texfamily ratio}
of our \text{\texfamily {\itshape Ratio}} data type
reduces fractions to their canonical form
by dividing numerator and denominator by
their \text{\texfamily gcd}.
If the \text{\texfamily gcd} is just 1, numerator and denominator
do not change. If the \text{\texfamily gcd} is not
1, however, the numbers actually do change.
For instance, the fraction $\frac{1}{6}$ is
already in canonical form. The fraction
$\frac{3}{6}$, however, is not,
since $gcd(6,3)$ is 3 and so
we reduce: $\frac{3/3=1}{6/3=2} = \frac{1}{2}$.

This leads to the observation that not all
fractions possible with one denominator
manifest with the \text{\texfamily {\itshape Ratio}} number type.
For the denominator 6, for example,
we have only $\frac{1}{6}$ and $\frac{5}{6}$.
For other numerators, we have
$\frac{2}{6} = \frac{1}{3}$,
$\frac{3}{6} = \frac{1}{2}$ and
$\frac{4}{6} = \frac{2}{3}$.
We could write a function that shows 
all proper fractions for one denominator,
\eg\:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~fracs1~::~{\itshape Natural}~\char'31~[{\itshape Ratio}]\\
\texfamily ~~fracs1~n~=~[x~\%~n~|~x~\char'06~[1..n-1]]
\end{tabbing}
\end{minipage}

\text{\texfamily fracs1\Sp 6}, for instance, gives:

\text{\texfamily [{\itshape Q}\Sp 1\Sp 6,{\itshape Q}\Sp 1\Sp 3,{\itshape Q}\Sp 1\Sp 2,{\itshape Q}\Sp 2\Sp 3,{\itshape Q}\Sp 5\Sp 6]},

which is easier to read in mathematical notation:

\[
\frac{1}{6},
\frac{1}{3},
\frac{1}{2},
\frac{2}{3},
\frac{5}{6}.
\]

How many proper fractions are there for a specific
denominator? In the example above, there are only two
proper fractions with the denominator 6.
We devise a function to filter out the fractions
that actually preserve the denominator 6:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~fracs2~::~{\itshape Natural}~\char'31~[{\itshape Ratio}]\\
\texfamily ~~fracs2~n~=~filter~(\char'10~({\itshape Q}~\char95 ~d)~\char'31~d~\char'36~n)~(fracs1~n)
\end{tabbing}
\end{minipage}

This function, again applied to 6 would give
(in mathematical notation):

\[
\frac{1}{6},
\frac{5}{6}.
\]

Applied to 12, it would give:

\[
\frac{1}{12},
\frac{5}{12},
\frac{7}{12},
\frac{11}{12}.
\]

It is easy to see that the numerators of the fractions
with a given denominator $n$, correspond to the group of numbers
$g < n$ coprime to $n$.
We, hence, could also create a function that just
finds the coprimes from the range $0\dots n-1$:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~coprimes~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~coprimes~n~=~[x~|~x~\char'06~[0..n-1],~gcd~n~x~\char'36~1]
\end{tabbing}
\end{minipage}

With little surprise, we see that \text{\texfamily coprimes\Sp 12}
gives

\[
1,5,7,11.
\]

Mathematicians like to quantify things
and so it is no wonder that there is a well
known function, Euler's \term{totient} function,
often denoted $\varphi(n)$,
that actually counts the coprimes.
This is easily implemented:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~tot~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~tot~=~fromIntegral~\char'00~length~\char'00~coprimes
\end{tabbing}
\end{minipage}

Let us have a look at the results of the totient function
for the first 20 or so numbers: 

\begin{tabular}{r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & $\dots$\\\hline
1 & 1 & 2 & 2 & 4 & 2 & 6 & 4 & 6 &  4 & 10 &  4 & 12 &  6 &  8 &  8 & 16 &  6 & 18 & $\dots$
\end{tabular}

We first see that the totient of a prime $p$ is $p-1$:
$\varphi(2) = 1$,
$\varphi(3) = 2$,
$\varphi(5) = 4$,
$\varphi(7) = 6$,
$\varphi(11) = 10,\dots$
Of course, that is the group of remainders of that prime --
the previous chapter was dedicated almost entirely to that group!

The totients of composites are different.
They slowly increase, but many numbers have the same totient.
Since the difference between primes and composites
lies in the fact that composites have divisors
different from 1 and themselves,
it is only natural to suspect that
there is a relation between 
the totients of composites and their divisors.
For instance, 2 has one divisor: 1.
The totien of 1 is $\varphi(1) = 1$ and that of 2,
$\varphi(2)$ is 1 as well.
It may just be one of those peculiarities 
we see with small numbers,
but what we see is the curious relation
$\varphi(1) + \varphi(2) = 2$.
We could formulate the hypothesis that 
the sum of the totients of the divisors of a number $n$
is that number $n$.
$n = \varphi(1) + \varphi(2) + \dots + \varphi(n)$ or,
more elegantly:

\begin{equation}
n = \sum_{d|n}{\varphi(d)}.
\end{equation}

Let us check this hypothesis.
From 2, we go on to 3, but 3 is prime
and has only the divisors 1 and 3
and, trivially, $\varphi(1) + \varphi(3) = 3$,
since, for any prime $p$: $\varphi(p) = p-1$.
We go on to 4.
The divisors of 4 are 1,2 and 4:
$\varphi(1) + \varphi(2) + \varphi(4) = 1 + 1 + 2 = 4$.
The next number, 5, again is prime
and we go on to 6, which has the divisors
1, 2, 3, 6:
$\varphi(1) + \varphi(2) + \varphi(3) + \varphi(6) = 1 + 1 + 2 + 2 = 6$.
Until here, the equation is confirmed. 
Let us jump a bit forward: 
12 has the divisors 1, 2, 3, 4, 6, 12:
$\varphi(1) + \varphi(2) + \varphi(3) + \varphi(4) + \varphi(6) + \varphi(12)$
$ = 1 + 1 + 2 + 2 + 2 + 4 = 12$.
The equation appears to be true.
But can we prove it?

In fact, it follows from a number of fundamental,
but quite simple theorems that one would probably
tend to take for granted on first encounter.
One of these theorems is related to cardinality of
set union. The theorem states that

\begin{equation}
|S1 \cup S2| = |S1| + |S2| - |S1 \cap S2|
\end{equation}

that is: the cardinality of the union of two sets
equals the sum of the cardinalities of the two sets
minus the cardinality of the intersection of the two sets.

Proof: The intersection of two sets $S1$ and $S2$
contains all elements that are both in $S1$ and $S2$.
The union of two sets contains all elements of $S1$ and
$S2$. But those elements that are in both sets
will appear only once in the union, since this is
the definition of the very notion of \term{set}.
We can therefore first build a collection of
all elements in the sets including the duplicates
and then, in a second step, remove the duplicates.
The elements that we remove, however, 
are exactly those 
that are also elements of the intersection.
The number of elements in the union, hence,
is exactly the sum of the numbers of elements
of the individual sets minus the number of duplicates.\qed

There are two corollaries that immediately follow
from this theorem.
First, for two disjoint sets $S1$ and $S2$, 
\ie\ sets for which $S1 \cap S2 = \varnothing$,
the equation above simplifies to:

\begin{equation}
|S1 \cup S2| = |S1| + |S2|. 
\end{equation}

This is trivially true, since $|\varnothing| = 0$.

Second, for sets that are pairwise disjoint
(but only for those!), we can derive the
general case:

\begin{equation}\label{eq:CardinalUnion}
\left|\bigcup_{i=1}^n{S_i}\right| = \sum_{i=1}^n{|S_i|}, 
\end{equation}

where $\bigcup$ is the union operator for $n$ sets,
where $n$ is not necessarily 2.
It, hence, does for $\cup$ what $\sum$ does for $+$.
Systems of such operators that are applied to 
an arbitrary number of operands are called
$\sigma$-algebras. But, for the time being,
that is just a fancy word.

The next fundamental theorem states
that the sum of the divisors of a number $n$
equals the sum of the fractions $\frac{n}{d_i}$,
where $d_i = d_1,d_2,\dots,d_r$ are the divisors of $n$.
More formally, the theorem states:

\begin{equation}\label{eq:SumOfDiv}
\sum_{d|n}{d} = \sum_{d|n}{\frac{n}{d}}.
\end{equation}

The point, here, is to see that if $d$ is a divisor of $n$,
then $\frac{n}{d}$ is a divisor too. That $d$ is a divisor
means exactly that: $n$ divided by $d$ results in another
integer $m$, such that $d = \frac{n}{m}$ and $dm=n$.
Since the set of divisors of $n$ contains all divisors
of $n$ and the set of quotients $\frac{n}{d}$ contains
quotients with all divisors, the two sets are equal.
The only aspect that changes, when we see these sets
as sequences of numbers, is the order.
Since order has no influence on the result
of the sum, the two sums are equal.\qed

For the example $n=12$, the divisors are
$1,2,3,4,6,12$. The quotients generated by
dividing $n$ by the divisors are
$12,6,4,3,2,1$.
The sum of the first sequence is $1+2+3+4+6+12 = 28$.
The sum of the second sequence is $12+6+4+3+2+1 = 28$.

It remains to note that, when we have a sum
of functions, then still 
$\sum_{d|n}{f(\frac{n}{d})} = \sum_{d|n}{f(d)}$,
since the values to which the function is applied
are still the same in both sets.

Equipped with these simple tools, we return
to the sum of the totients of the divisors.
We start by defining a set $S_d$ that contains
all numbers, whose \text{\texfamily gcd} with $n$ is $d$:

\begin{equation}  
S_d = \lbrace m \in \mathbb{N}: 1 \le m \le n, \gcd(n,m) = d\rbrace.
\end{equation}

In Haskell this would be:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~s~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~s~n~d~=~[m~|~m~\char'06~[1..n],~gcd~n~m~\char'36~d]
\end{tabbing}
\end{minipage}

When we map this function with $n=12$
on the divisors of 12,
\text{\texfamily map\Sp s\Sp 12\Sp [1,2,3,4,6,12]}, we get:

\begin{minipage}{\textwidth}
\text{\texfamily [1,5,7,11]}\\
\text{\texfamily [2,10]}\\
\text{\texfamily [3,9]}\\
\text{\texfamily [4,8]}\\
\text{\texfamily [6]}\\
\text{\texfamily [12]}.
\end{minipage}

We see six pairwise disjoint sets whose union
equals the numbers $1\dots 12$.
The first set contains the coprimes of 12,
since we ask for $m$, such that $\gcd(12,m) = 1$.
The next set contains the numbers, such that
$\gcd(12,m) = 2$, the next, the numbers,
such that $\gcd(12,m) = 3$ and so on.
In other words, these lists together
contain all numbers $1\dots 12$
partitioned according to their greatest common divisor
with $n=12$.
Note that the lists together necessarily contain 
all the numbers in the range
$1\dots n$, since, either a number does not have
common divisors with $n$, then it is in the first
set for $\gcd(n,m) = 1$, or it has a common divisor
with $n$. Then it is in one of the other sets.
This is just what the set $S_d$ mapped on the divisors
of $n$ is about.

The sets are also necessarily disjoint from each other,
since no number $m$ would, on one occasion,
have a \text{\texfamily gcd} $d_1$ with $n$ and, on another,
a distinct \text{\texfamily gcd} $d_2$ with the same $n$.
It either shares $d_1$ as greatest common divisor with $n$
or divisor $d_2$.
It, hence, is either in set $S_{d_1}$ or 
in set $S_{d_2}$.

But there is more. The set for divisor 2
contains 2 and 10. These numbers divided
by 2 give 1 and 5. $\frac{12}{2}$ is 6
and 1 and 5 are the coprimes of 6.
The set for divisor 3 contains 3 and 9;
these numbers divided by 3 are 1 and 3.
$\frac{12}{3}$ is 4 and 1 and 3 are the 
coprimes of 4 and so on. 
In other words, the sets that we see above
contain numbers $m$ that, divided by the corresponding
divisor $d$, $\frac{m}{d}$, are the coprimes
of $\frac{n}{d}$.
This results from the fact that these numbers
and the divisor are related to each other by the $\gcd$.
When we have two numbers $m$ and $n$ and we
compute their $\gcd$: $d = \gcd(n,m)$,
then $\frac{n}{d}$ is coprime to $\frac{m}{d}$,
since we divide them by the biggest number
that divides both.
Therefore, all numbers in the set $S_d$
are necessarily coprimes of $\frac{n}{d}$.

Can there be a coprime of $\frac{n}{d}$
(less than $\frac{n}{d}$)
that is not in the set $S_d$?
We created the list of coprimes by first computing
$m$, such that $\gcd(n,m) = d$, and then $c=\frac{m}{d}$.
Now, let us assume that there is a coprime $c$
that escapes this filter. In other words,
there is another number $k \neq d$, such that
$\gcd(n,m) = k$ and $c = \frac{m}{k}$.
To be a coprime of interest, we must have
$\frac{m}{k} < \frac{n}{d}$.
Since $\frac{dn}{d} = n$, we must have
$\frac{dm}{k} < n$. This number must therefore
appear in one of the $S_{d_i}$.
We can ask: in which one?
The answer is $\gcd(n,\frac{dm}{k})$.
There are two candidates: $d$ and $\frac{m}{k}$.
But $\frac{m}{k}$ cannot be a divisor of $n$,
since $k$ is the greatest divisor $m$ and $n$
have in common. They do not share any other divisor,
not even $\frac{m}{k}$. Therefore, $d$ must be
the greatest common divisor of $n$ and $\frac{dm}{k}$.
But then this number appears in $S_d$ and
$\frac{m}{k}$ does not escape our filter.

\ignore{
Since $k$ is not $d$, we have either
$k<d$ or $k>d$. 

If $k>d$, then, for sure, $\frac{n}{k} < \frac{n}{d}$ and,
since $m<n$, $\frac{m}{k} < \frac{n}{d}$.
We can even further deduce that $\frac{dm}{k} < n$
and can now ask in which set this number
$\frac{dm}{k}$ would appear or, in other words,
what is $\gcd(n,\frac{dm}{k})$?
The result is either $d$ or $\frac{m}{k}$,
since both are divisors of $n$
(it is not $\frac{d}{k}$, since, for sure: 
$d > \frac{d}{k}$).
If it is $d$, then $\frac{m}{k}$ is coprime
of $\frac{n}{d}$, as required, but it would then not
escape the filter, since, as we have just assumed,
$\gcd(n,\frac{dm}{k}) = d$.

If, otherwise, $\frac{m}{k}$ is the result,
then $\frac{dm}{k}$ appears in the set
$S_{m/k} = S_c$ and, in consequence,
$d$ is coprime to $\frac{n}{c}$.
But we also want $c$ to be coprime to $\frac{n}{d}$.
So, we have two pairs of numbers,
$\frac{n}{c}$ and $c$, on one hand,
and $\frac{n}{d}$ and $d$, on the other.
$\frac{n}{c}$ and $d$ do not share divisors,
\ie\ they do not share prime factors, and
$\frac{n}{d}$ and $c$ do not share
prime factors either.
We want the products of the pairs $\frac{cn}{c}=n$
and $\frac{dn}{d}=n$
result in the same number $n$.
But that is not possible.
Assume the prime factorisation of $n$ were $\lbrace p,q,r,s,t\rbrace$
and that of $d$, $\lbrace p,q\rbrace$.
Then $\frac{n}{d} = \lbrace r,s,t\rbrace$.
We want $\frac{n}{c}$ coprime to $d$,
so it must not contain the primes $p$ and $q$.
Its prime factorisation is thus a subset of $\lbrace r,s,t\rbrace$,
and $c$ is the complement of this set.
For $\frac{n}{c} = \lbrace r,s\rbrace$,
$c$ would be $\lbrace p,q,t\rbrace$.
But then $c$ is not coprime to $\frac{n}{d}$,
because it shares the divisor $t$ with that number.
To avoid that these numbers share divisors,
$c$ must not contain $r$, $s$ and $t$
and to avoid that the other two numbers share
divisors, $c$ must contain $p$ and $q$.
In consequence, $c$ and $d$ are equal and then
$\frac{n}{d}$ and $\frac{n}{c}$ are equal as well.

For the case $k<d$, it is not necessarily
the case that $\frac{m}{k} < \frac{n}{d}$,
but depends on $m$.
If it is the case, then we proceed as above;
otherwise, we look for the group of
$\frac{cn}{d}$, which,
as you can assure yourself, will 
lead to a similar situation.
}
\ignore{$}

It follows that each of the sets $S_d$
contains exactly those numbers that divided by $d$
are the coprimes of $\frac{n}{d}$.
The size of each of these sets
is thus the totient number of $\frac{n}{d}$:

\begin{equation}
|S_d| = \varphi\left(\frac{n}{d}\right).
\end{equation}

To complete the proof, we now have
to extend the relation between one of those sets and
the totient of one $\frac{n}{d}$ to that between the union of
all the $S_{d_i}$ and the sum of the totient numbers
of the divisors.
From cardinality of disjoint sets (equation \ref{eq:CardinalUnion})
we know that the cardinality of the union of disjoint sets
is the sum of the cardinality of each of the sets,
so we have:

\begin{equation}
\left|\bigcup_{d|n}{S_d}\right| = \sum_{d|n}{\varphi\left(\frac{n}{d}\right)}.
\end{equation}

From sum of divisors (equation \ref{eq:SumOfDiv})
we know even further that the sum of $\frac{n}{d}$ 
equals the sum of $d$, therefore: % function missing in proof!

\begin{equation}\label{eq:SdPhi}
\left|\bigcup_{d|n}{S_d}\right| = \sum_{d|n}{\varphi(d)}.
\end{equation}

We have seen that the union of the $S_{d_i}$ for a given $n$
contains all numbers in the range $1\dots n$:

\begin{equation}
\bigcup_{d|n}{S_d} = \lbrace 1\dots n\rbrace.
\end{equation}

Since the set $\lbrace 1\dots n\rbrace$ contains $n$ 
numbers, we can conclude that

\begin{equation}
\left|\bigcup_{d|n}{S_d}\right| = n,
\end{equation}

from which, together with equation \ref{eq:SdPhi}, 
we then can conclude that

\begin{equation}
\sum_{d|n}{\varphi(d)} = n.\qed
\end{equation}

We could define a recursive function
very similar to Pascal's rule that exploits this relation.
We first define a function to get the divisors

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~divs~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~divs~n~=~[d~|~d~\char'06~[1..n],~rem~n~d~\char'36~0]
\end{tabbing}
\end{minipage}

Then we add up the totients of these numbers
(leaving $n$ out, because that is the one we want to compute)
and subtract the result from $n$ and, this way,
obtain the totient number of $n$:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~divsum~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~divsum~1~=~1\\
\texfamily ~~divsum~2~=~1\\
\texfamily ~~divsum~n~=~n~-~sum~[divsum~d~|~d~\char'06~divs~n,~d~<~n]
\end{tabbing}
\end{minipage}

\ignore{
This algorithm, however, is less efficient
than the original one implemented with the \text{\texfamily tot} function.
Though computing the remainder is less complex
in computation than computing the $\gcd$, which calls the 
remainder several times,
we now have a lot of recursion steps, 
when computing big numbers, before we get to the base cases.
}

Another property of the totient function is
multiplicity of totients of coprimes, that is

\begin{equation}
\varphi(a)\times\varphi(b) = \varphi(ab), \text{if} \gcd(a,b) = 1.
\end{equation}

For instance, the coprimes of 3 are 1 and 2;
those of 5 are 1, 2, 3 and 4.
$\varphi(3)$, hence, is 2 and $\varphi(5)$ is 4.
$\varphi(3\times 5 = 15)$ is 8, which also is $2\times 4$.
Indeed, the coprimes of 15 are 1, 2, 4, 7, 8, 11, 13 and 14.
An example of two coprimes that are not both primes
is 5 and 8. $\varphi(5) = 4$ and $\varphi(8) = 4$.
$\varphi(5\times 8 = 40) = 16$, which also is $4\times 4$.

This property might look surprising at the first sight,
but is becomes almost trivial in the light
of the Chinese Remainder theorem.
For two coprimes $a$ and $b$ and their sets of coprimes
$A$ and $B$, we can, for any $a_i \in A$ and $b_j \in B$ create
congruence systems of the form 

\begin{align*}
x & \equiv a_i \pmod{a}\\
x & \equiv b_j \pmod{b}
\end{align*}

The Chinese Remainder theorem guarantees
that, for every case, their is a solution 
that is unique modulo $ab$,
\ie\ there are no two different systems with 
the same solution and there is no system without
a solution.
Since the solutions are unique modulo $ab$,
there must be exactly one number in the group 
of coprimes of $ab$ for any combination of $a_i$
and $b_j$. Since there are $|A| \times |B|$
combinations of all elements of $A$ and $B$,
$\varphi(ab)$ must be $\varphi(a) \times \varphi(b)$.

To illustrate that, we can create all the combinations
of $a$s and $b$s and then apply the Chinese remainder
on all of them.
First we create all combinations of $a$s and $b$s:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~consys~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~[[{\itshape Natural}]]\\
\texfamily ~~consys~a~b~=~concatMap~mm~(coprimes~b)\\
\texfamily ~~~~{\bfseries where}~mm~y~=~[[x,y]~|~x~\char'06~coprimes~a]
\end{tabbing}
\end{minipage}

The result for \text{\texfamily consys\Sp 5\Sp 8}, for instance, is:

\begin{minipage}{\textwidth}
\text{\texfamily \Sp [1,1],[2,1],[3,1],[4,1],}\\
\text{\texfamily \Sp [1,3],[2,3],[3,3],[4,3],}\\
\text{\texfamily \Sp [1,5],[2,5],[3,5],[4,5],}\\
\text{\texfamily \Sp [1,7],[2,7],[3,7],[4,7]\Sp }
\end{minipage}

Now we map \text{\texfamily chinese} on this: 

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~china~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~china~a~b~=~map~(esenihc~[a,b])~(consys~a~b)~\\
\texfamily ~~~~{\bfseries where}~esenihc~=~flip~chinese
\end{tabbing}
\end{minipage}

Note that, to map \text{\texfamily chinese} on \text{\texfamily consys},
we have to flip it.
\text{\texfamily chinese} expects first the congruences
and then the moduli, but we need it
the other way round.

This is the result for \text{\texfamily china\Sp 5\Sp 8}:

\text{\texfamily 1,11,21,31,17,27,37,7,33,3,13,23,9,19,29,39}

and this is the result for \text{\texfamily coprimes\Sp 40}:

\text{\texfamily 1,3,7,9,11,13,17,19,21,23,27,29,31,33,37,39}.

When you sort the result of \text{\texfamily china\Sp 5\Sp 8}, you will
see that the results are the same.

We are now approaching the climax of this section.
There is a little fact we need, before we can
go right to it, which may appear a tiny curiosity.
This curiosity is yet another property of
the totient function concerning the totient
of powers of prime numbers:

\begin{equation}
\varphi(p^k) = p^k - p^{k-1}.
\end{equation}

That is, if $p$ is prime, then
the totient of $p^k$ equals
the difference of this number $p^k$
and the previous power of that prime $p^{k-1}$.
An example is 27, which is $3^3$.
We compute $27 - 3^2 = 27 - 9 = 18$,
which is indeed $\varphi(27)$.

The proof is quite simple.
Since $p$ is prime, its powers 
have only one prime factor,
namely $p$. When we say \term{prime power},
we mean exactly this: a number whose factorisation
consists of one prime raised to some $k\ge 1$: $p^k$.
Therefore, the only numbers that share divisors
with $p^k$ are multiples of $p$ less than
or equal to $p^k$: $p$, $2p$, $3p$, $\dots$, $p^k$.
The 9 numbers that share divisors with 27 are:

\[
3,6,9,12,15,18,21,24,27.
\]

How many multiples of $p$ less than or equal to $p^k$ are there?
There are $p^k$ numbers in the range $1\dots p^k$,
every $p^{th}$ number of which is a multiple of $p$.
There, hence, are $\frac{p^k}{p} = p^{k-1}$ numbers that divide $p^k$.
The number of coprimes in this range 
is therefore $p^k$ minus that number and, thus,
$p^k - p^{k-1}$.\qed

We can play around a bit with this formula.
The most obvious we can do is to factor $p^{k-1}$ out
to get $p^{k-1}(p-1)$.
So, we could compute $\varphi(27) = 9\times2 = 18$.
Even more important for the following, however,
is the formula at which we arrive by factoring
$p^k$ out. We then get

\begin{equation}\label{eqTotPrimePower1}
\varphi\left(p^k\right) = p^k \left(1-\frac{1}{p}\right)
\end{equation}

This formula leads directly to a closed form
for the totient function, namely \term{Euler's product formula}.
Any number can be represented as a product of prime powers:
$n = p_1^{k_1}p_2^{k_2}\dots$
Since the $p$s in this formula are all prime,
the resulting prime powers are for sure coprime to each other.
That means that the multiplicity property of 
the totient function applies, \ie\ 

\[
\varphi(n) = \varphi\left(p_1^{k_1}\right)\varphi\left(p_2^{k_2}\right)\dots.
\]

We now can substitute the totient computations 
of the prime powers on the right-hand side
by the formula in equation \ref{eqTotPrimePower1}
resulting in 

\[
\varphi(n) = p_1^{k_1}\left(1-\frac{1}{p_1}\right)
             p_2^{k_2}\left(1-\frac{1}{p_2}\right)
             \dots
\]

We regroup the formula a bit to get:

\[
\varphi(n) = p_1^{k_1}p_2^{k_2}\dots
             % \left(1-\frac{1}{p_1}\right) % this has stopped working, why???
             (1-\frac{1}{p_1})
             \left(1-\frac{1}{p_2}\right)
             \dots
\]

and see that we have all the prime factors of $n$
and then the differences. The prime factors
multiplied out result in $n$, so we can simplify
and obtain Euler's product formula:

\begin{equation}\label{eq:prodForm1}
\varphi(n) = n \prod_{p|n}{\left(1-\frac{1}{p}\right)}.
\end{equation}

Consider again the example $n=12$.
The formula claims that 

\[
\varphi(12) = 12 \times \left(1-\frac{1}{2}\right)
                        \left(1-\frac{1}{3}\right),
\]

since the prime factors of 12 are 2 and 3.
Let us see: $1-\frac{1}{2}$ is just $\frac{1}{2}$,
$1-\frac{1}{3}$ is $\frac{2}{3}$.
We therefore get 
$12 \times \frac{1}{2} \times \frac{2}{3}$.
$12 \times \frac{1}{2} = 6$ and
$6 \times \frac{2}{3} = \frac{12}{3} = 4$,
which, indeed, is $\varphi(12)$.

We can use this formula to implement
a variant of \text{\texfamily tot} in Haskell:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~ptot~::~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~ptot~n~=~~{\bfseries let}~~r~~~=~ratio~n~1~\\
\texfamily ~~~~~~~~~~~~{\bfseries in}~~~{\bfseries case}~r~*~product~[ratio~(p-1)~p~|~p~\char'06~nub~(trialfact~n)]~{\bfseries of}\\
\texfamily ~~~~~~~~~~~~~~~~~{\itshape Q}~t~1~~\char'31~t\\
\texfamily ~~~~~~~~~~~~~~~~~\char95 ~~~~~~\char'31~error~\char34 Totient~not~an~integer\char34 
\end{tabbing}
\end{minipage}

Note that we use the \text{\texfamily {\itshape Ratio}} number type.
To convert it back to \text{\texfamily {\itshape Natural}},
we check if the denominator is 1,
\ie\ if the resulting fraction is indeed
an integer. If so, we return the numerator.
Otherwise, we create an error, which, of course, 
should not occur if Euler was right.

But what is so special about this function?
Well, it leads us quickly to an old friend.
Consider a squarefree number $n$, that is a number
where the exponents in the prime factorisation
are all 1: $n = p_1p_2\dots p_r$.
We perform some simple transformations on 
\ref{eq:prodForm1}.
First, we transform the factors $1-\frac{1}{p}$
to $\frac{p}{p} - \frac{1}{p} = \frac{p-1}{p}$.
Then we have:

\begin{equation}
\varphi(n) = n\frac{p-1}{p}\frac{q-1}{q}\dots
\end{equation}

As you may have noticed,
this is the form in which we implemented Euler's formula in \text{\texfamily ptot}.
Now we multiply this out:

\begin{equation}
\varphi(n) = \frac{n(p-1)(q-1)\dots}{pq\dots}
\end{equation}

Since $n$ is squarefree, the denominator,
the product of the prime factors, equals $n$.
$n$ and the entire denominator, hence,
cancel out. Now we have:

\begin{equation}
\varphi(n) = (p-1)(q-1)\dots
\end{equation}

That is, the totient number of a squarefree number
is the product of the prime factors, each one
reduced by one, a result that follows
from multiplicity of totients of numbers that are coprime
to each other.

The formula is quite interesting.
It is very close to the formula we used for the
\acronym{rsa} algorithm to find a number $t$
for which

\[
a^t \equiv 1 \pmod{n}.
\]

It will probably not come as a surprise 
that there is indeed \term{Euler's theorem},
which states that, for $a$ and $n$ coprime
to each other:

\begin{equation}
a^{\varphi(n)} \equiv 1 \pmod{n},
\end{equation}

which, as you will at once recognise,
is a generalisation of Fermat's little theorem.
Fermat's theorem expresses the same congruence
for a prime number. Since, as we know,
the totient number of a prime number $p$
is $p-1$, Fermat's theorem can be reduced
to Euler's theorem.
In fact, Euler's theorem is nothing new at all.
We already know from the previous chapter
that the powers of $a$ up to $a^k$,
such that $a^k \equiv 1 \pmod{n}$,
establish a group or subgroup of the numbers
coprime to $n$.
Since $\varphi(n)$ is the size of the group
of coprimes of $n$, any subgroup of $n$
has either size $\varphi(n)$ or a size
that divides $\varphi(n)$.
But for sure, for any coprime $a$ the group
is at most $\varphi(n)$, since there are only
$\varphi(n)$ elements in the group.

For the example 12, the groups are all trivial.
The coprimes of 12 are 1, 5, 7 and 11.
1 establishes the trivial group $\lbrace 1\rbrace$.
5 is in the likewise trivial group $\lbrace 1,5\rbrace$,
since $5^2 = 25$ and, hence, $5^2 \equiv 1 \pmod{12}$.
Since $7^2 = 49 \equiv 1 \pmod{12}$, the group of
7 is also trivial. Finally, since $11^2 = 121 \equiv 1 \pmod{12}$,
11 is in a trivial group as well.

A more interesting case is 14.
Let us look at the groups of 14 using \text{\texfamily generate},
which we defined in the previous chapter,
like \text{\texfamily map\Sp (generate\Sp 14)\Sp (coprimes\Sp 14)}:

\text{\texfamily [1]}\\
\text{\texfamily [3,9,13,11,5,1]}\\
\text{\texfamily [5,11,13,9,3,1]}\\
\text{\texfamily [9,11,1]}\\
\text{\texfamily [11,9,1]}\\
\text{\texfamily [13,1]}

We see four different groups.
The trivial groups 1 and $n-1$ and
the non-trivial groups $\lbrace 1,3,5,9,11,13\rbrace$,
identical to the coprimes of 14, and 
$\lbrace 1,9,11\rbrace$, a subgroup
with three elements.

From these examples it should be clear
that not for all coprimes $a$ $\varphi(n)$
is the first number for which the congruence
in Euler's theorem is established.
In fact, in many cases, there are smaller numbers $k$
that make $a^k \equiv 1 \pmod{n}$.
For $\varphi(n)$, however, it is guarenteed
for any $a$ coprime to $n$ that the congruence
holds.

But, still, $\varphi(n)$ is not necessarily
the smallest number that guarantees 
the congruence.
In some cases, there is a smaller number
that does the job and this number
can be calculated by the \term{Carmichael function},
of which we have already used a part,
when we discussed \acronym{rsa}.

The Carmichael function is usually denoted
$\lambda(n)$ (but has nothing to do with
the $\lambda$-calculus!).
It is a bit difficult to give its definition
in words. It is much easier, actually,
to define it in Haskell:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~lambda~::~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~lambda~~2~=~tot~2\\
\texfamily ~~lambda~~4~=~tot~4\\
\texfamily ~~lambda~~n~~|~twopower~~~n~~=~(tot~n)~`div`~2~\\
\texfamily ~~~~~~~~~~~~~|~primepower~n~~=~tot~n\\
\texfamily ~~~~~~~~~~~~~|~even~n~\char'04~primepower~(n~`div`~2)~=~tot~n\\
\texfamily ~~~~~~~~~~~~~|~otherwise~~~~~=~~{\bfseries let}~ps~=~map~lambda~(simplify~\$~trialfact~n)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bfseries in}~foldl'~lcm~1~ps\\
\texfamily ~~~~{\bfseries where}~simplify~=~map~product~\char'00~group~\char'00~sort
\end{tabbing}
\end{minipage}

There are two base cases stating
that $\lambda(2)$ and $\lambda(4)$
equal $\varphi(2)$ and $\varphi(4)$ respectively.
Otherwise, if $n$ is a power of 2,
then $\lambda(n)$ equals the half of $\varphi(n)$.
An example is 8. We would expect that any
group generated with coprimes of 8
has at most two members, since
$\varphi(8) = 4$, and $\lambda(8) = 2$.
We generate the groups 
with \text{\texfamily map\Sp (generate\Sp 8)\Sp (coprimes\Sp 8)} and see:

\begin{minipage}{\textwidth}
\text{\texfamily [1]}\\
\text{\texfamily [3,1]}\\
\text{\texfamily [5,1]}\\
\text{\texfamily [7,1]}.
\end{minipage}

The prediction, hence, is correct.
We saw a similar result for 12,
but that has other reasons
as we will see below.

The function \text{\texfamily twopower}, by the way,
is defined as

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~twopower~::~{\itshape Natural}~\char'31~{\itshape Bool}\\
\texfamily ~~twopower~~1~=~{\itshape True}\\
\texfamily ~~twopower~~2~=~{\itshape True}\\
\texfamily ~~twopower~~n~~|~even~n~~~~~=~twopower~(n~`div`~2)\\
\texfamily ~~~~~~~~~~~~~~~|~otherwise~~=~{\itshape False}
\end{tabbing}
\end{minipage}

The next line states that for any primepower $n$,
$\lambda(n) = \varphi(n)$.

The function \text{\texfamily primepower} is defined as

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~primepower~::~{\itshape Natural}~\char'31~{\itshape Bool}\\
\texfamily ~~primepower~n~=~length~(nub~\$~trialfact~n)~\char'36~1
\end{tabbing}
\end{minipage}

In other words, a primepower is a number
whose prime factorisation consists of only
one prime (which itself, however, may appear more than once).
Since powers of 2 are handled in the previous guard,
we are dealing here only with powers of odd primes.
An example is 9, which is $3^2$.
The coprimes of 9 are 1, 2, 4, 5, 7 and 8.
The totient number, hence, is 6.
The groups are \text{\texfamily map\Sp (generate\Sp 9)\Sp (coprimes\Sp 9)}:

\begin{minipage}{\textwidth}
\text{\texfamily [1]}\\
\text{\texfamily [2,4,8,7\Sp 5,1]}\\
\text{\texfamily [4,7,1]}\\
\text{\texfamily [5,7,8,4,2,1]}\\
\text{\texfamily [7,4,1]}\\
\text{\texfamily [8,1]}.
\end{minipage}

We see four different groups.
The two trivial groups and two
groups with 3 and 6 members respectively.
Again, the prediction is correct.

The next line states that,
if $n$ is even and half of $n$
is a primepower, then again $\lambda(n) = \varphi(n)$.
An example is 18, since 18 is even and
the half of 18 is 9, which is a power of 3.
$\varphi(18)$ is 6, so we would expect to see
groups with at most 6 elements.
Here is the result for \text{\texfamily map\Sp (generate\Sp 18)\Sp (coprimes\Sp 18)}:

\begin{minipage}{\textwidth}
\text{\texfamily [1]}\\
\text{\texfamily [5,7,17,13,11,1]}\\
\text{\texfamily [7,13,1]}\\
\text{\texfamily [11,13,17,7,5,1]}\\
\text{\texfamily [13,7,1]}\\
\text{\texfamily [17,1]}.
\end{minipage}

We see, again, four groups,
the two trivial groups 1 and $n-1$ and
two non-trivial groups with 3 and 6 members respectively.

We come to the \text{\texfamily otherwise} guard.
If $n$ is not 2 or 4, not a power of 2
nor a power of another prime and not
twice a power of a prime, then
we do the following: 
we compute the factorisation,
order the factors, group by equal factors,
compute the primepower that corresponds
to each group of factors and map $\lambda$
on the resulting numbers. Then we
compute the \text{\texfamily lcm} of the results.
In short: $\lambda(n)$ is the \text{\texfamily lcm}
of the $\lambda$ mappend to the prime powers
in the prime factorisation of $n$.

An example for a number that is not a primepower
nor twice a primepower is 20.
The factorisation of 20 is $\lbrace 2,2,5\rbrace$.
We compute the primepowers resulting in $\lbrace 4,5\rbrace$.
When we map $\lambda$ on them, we should
get $\lambda(4) = \varphi(4) = 2$ and
$\lambda(5) = \varphi(5) = 4$.
The \text{\texfamily lcm} of 2 and 4 is 4 and, hence,
$\lambda(20) = 4$.
We, thus, should not see a group
with more than 4 elements. We call
\text{\texfamily map\Sp (generate\Sp 20)\Sp (coprimes\Sp 20)} and see:

\begin{minipage}{\textwidth}
\text{\texfamily [1]}\\
\text{\texfamily [3,9,7,1]}\\
\text{\texfamily [7,9,3,1]}\\
\text{\texfamily [9,1]}\\
\text{\texfamily [11,1]}\\
\text{\texfamily [13,9,17,1]}\\
\text{\texfamily [17,9,13,1]}\\
\text{\texfamily [19,1]}.
\end{minipage}

We see 6 different groups, the two trivial groups
1 and $n-1$ and four non-trivial groups
with 2 and, respectively, 4 members.

The factorisation of 12 is $\lbrace 2,2,3\rbrace$,
so we apply $\lambda$ on the numbers
4 and 3, which for both cases is 2.
The \text{\texfamily lcm} of 2 is just 2 and, therefore,
we do not see groups with more than 2 members
with the coprimes of 12.

Now, as you may have guessed,
\term{Carmichael's theorem} states that,
if $a$ and $n$ are coprime to each other, then

\begin{equation}
a^{\lambda(n)} \equiv 1 \pmod{n}.
\end{equation}

For primes, the theorem is identical to
Fermat's little theorem. For powers of
odd primes, it reduces to Euler's theorem.
The \text{\texfamily lcm} of primepowers under the \text{\texfamily otherwise}-guard
is a consequence of the Chinese Remainder theorem 
and the very notion of the \text{\texfamily lcm}. 
We know that, 
if $x \equiv 1 \pmod{n}$, then also $x \equiv 1 \pmod{mn}$.
However, $mn$ is not necessarily
the first multiple of $n$ and $m$ that establishes
the congruence. Any number that is a multiple of both,
$n$ and $m$, would have the same effect and the first number
that is a multiple of both is $lcm(m,n)$.

The totient number of twice the power of an odd prime,
$2p^k$,
is the same as the totient number of that odd prime power,
$p^k$: $\varphi(p^k) = \varphi(2p^k)$.
The coprimes of $p^k$ are all numbers
from 1 to $p^k$ that are not multiples of $p$,
including all even numbers.
Since twice that primepower is an even number,
the even numbers are not part of the coprimes of that number.
So, the coprimes of $2p^k$ in the range 
$1\dots p^k$, are exactly half of the coprimes of $p^k$.
But now, there are the coprimes in the second half
$p^k\dots 2p^k$. 
Since the interval is the same in size
and we eliminate the same number of numbers in that range
as in the first half, namely the even numbers and
the multiples of $p$, we end up with two sequences,
each containing half as many numbers as the original
sequence of coprimes of $p^k$. The two halfs together,
therefore, make for the same amount of coprimes
of $p^k$ and $2p^k$.
So, we can handle these cases in the same way.

The general rule, however, would produce the same result.
According to the general rule, we would first
compute $\lambda$ for the individual primepowers
and then the \text{\texfamily lcm} of these values.
The factorisation of a number that is twice a primepower 
contains the factor 2 and the primepower.
The value for $\lambda(2)$ is $\varphi(2)$,
which is 1. The \text{\texfamily lcm} of 1 and another number
is that other number. There, hence,
is no difference between this rule and
the general rule.

Now, what about the powers of 2 greater 4?
To show that the greatest group of a power of 2
is half the totient of that number 
is quite an interesting exercise in group theory.
The coprimes of a power of 2 have a quite peculiar
structure, namely

\[
1, \dots, m_1, m_2, \dots, n-1.
\]

Interesting are the middle numbers $m_1$ and $m_2$.
They both are their own inverses, such that
$m_1m_1 \equiv 1 \pmod{n}$ and
$m_2m_2 \equiv 1 \pmod{n}$.
The set of coprimes, therefore, consists of two symmetric halves,
each starting and ending with a number that is its own inverse:
$1\dots m_1$ is the first half,
$m_2\dots n-1$ is the second half.

The number of coprimes is of course even 
since they consist of all odd numbers
$1\dots 2^k-1$. Therefore, we do not have one central number,
but the two middle numbers $m_1$ and $m_2$, which are one off
the half of $2^k$, that is
$m_1 = 2^{k-1}-1$ and
$m_2 = 2^{k-1}+1$.
The following calculation shows that 
both $m_1$ and $m_2$ squared
are immediately 1 modulo $2^k$,
for any $2^k$ with $k>2$.
For $m_1$ we have:

\[
(2^{k-1}-1)(2^{k-1}-1).
\]

When we multiply this out we get the terms
$2^{k-1+k-1}$, which simplifies to $2^{2k-2}$,
$-2^{k-1}-2^{k-1}$, which simplifies to $-2^k$,
and $1$:

\[
2^{2k-2}-2^k+1.
\]

We can factor $2^k$ out of the the first two terms:

\[
2^k(2^{k-2}-1)+1,
\]

and see clearly that the first remaining term 
is divided by $2^k$ and, thus,
disappears modulo $2^k$. We are left with 1 and this shows
that $m_1$ is its own inverse.

For $m_2$, the proof is very similar, with the difference
that we are left over with $2^k(2^{k-2}+1)$ for the first term.
However, this term is a multiple of $2^k$ as well, and we are
again left with 1.

Now, we can select a random generator of the group, say, $a$
and look what it generates. For explicitness, we consider
the case of $2^4 = 16$, whose coprimes are

\[
1,3,5,7,9,11,13,15
\]

This group has the form 
(with arbitrary placement of the inverses of $a$ and $b$):

\[
1,a,b,m_1,m_2,b',a',n-1.
\]

We see at once that no generator will create a sequence
with 8 elements. Any sequence generated by exponentiation
of $a$ can contain only one of the elements $m_1$, $m_2$ and
$n-1$. Since, if $a^k = m_2$, $a^{2k} = 1$ and, afterwards,
the whole cycle repeats. If some $a^l$, for $k < l < 2k$,
was $m_1$ or $n-1$, then $a^{2l}$ would be 1 again.
But that cannot be, since $2l > 2k$ and, therefore,
$a^{2l}$ is part of the second cycle, which has to be
exactly the same as the first. But, obviously,
there was only one 1 in the first cycle, namely at $a^{2k}$
and there must be only one 1 in the second cycle, namely at
$a^{4k}$. Therefore, there can be only one of the elements
$m_1$, $m_2$ and $n-1$ in the sequence and this reduces
the longest possible sequence for this example to 6, for instance:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c}
 1 &  2 &  3   &  4 &  5 &  6 \\\hline
 a &  b & $m_2$& b' & a' &  1 
\end{tabular}
\end{center}

Until here it looks fine.
But observe that we now have one set with six numbers,
the group generated by $a$, which we will call $G$,
and its complement relative to the set of coprimes,
which contains 2 elements. For the group above
that containts $m_2$, the complement 
consists of $m_1$ and $n-1$.

Now, we will construct what is called a \term{coset}.
A coset of $G$, in our context here, is a set of numbers
resulting from one element of the
complement of $G$, multiplied by all numbers of $G$.
Let us say, this element is $m_1$. Then the coset
of $G$ created by $m_1$ denoted $m_1G$ is

\begin{equation}
m_1G = \lbrace m_1a, m_1b, m_1m_2, m_1b', m_1a', m_1\rbrace
\end{equation}

Note that this set contains six numbers.
These numbers are necessarily different from
all numbers in $G$, since the numbers in $G$
form a group, the product of two members of which
result in another member of it and,
for each pair of members of the group $c$ and $d$, 
there is one number $x$ in the group, such that
$xc = d$.
$m_1$, however, is not member of the group
and, if $m_1$ multiplied by $c$
resulted in another member $d$, then we would
have the impossible case that $d$ is the result 
of multiplications of $c$ for 
two different numbers: $m_1$ and $x$.
Therefore, no number in $m_1G$ can possibly equal
any number in $G$.

But we do not have six numbers!
We only have two numbers, namely $m_1$ 
and $n-1$.
Therefore, no group that we create on a
set of coprimes with such a structure
can be greater than half of the number of coprimes.
In our example that is four. With four numbers
in $G$ and four elements in the complement of
$G$, we would have no problem at all.
But, definitely, a group with six members does 
not work.\qed

A corollary of this 
simple but important argument is that
the order of any subgroup of a group of
coprimes must divide the number of coprimes.
This extends
the proof of Lagrange's theorem for prime groups
to composite groups. We will extend it even further
in the future. For the moment, however,
we can be satisfied with the result.
We have proven Carmichael's theorem.
\section{How many Fractions?} % Cantor's Diagonal 1 + Calkin-Wilf
\ignore{
\begin{tabbing}\texfamily
{\bfseries module}~{\itshape Cantor1}\\
\texfamily {\bfseries where}\\
\texfamily ~~{\bfseries import}~{\itshape {\itshape Data}.List}~(nub,sort)\\
\texfamily ~~{\bfseries import}~{\itshape {\itshape Data}.Tree}\\
\texfamily ~~{\bfseries import}~{\itshape Fact}\\
\texfamily ~~{\bfseries import}~{\itshape Binom}\\
\texfamily ~~{\bfseries import}~{\itshape Natural}\\
\texfamily ~~{\bfseries import}~{\itshape Zahl}\\
\texfamily ~~{\bfseries import}~{\itshape Quoz}
\end{tabbing}
}

How many negative numbers are there?
That is a strange question! 
How do you want me to answer?
I can tell you how many numbers
of a specific kind are there only
in relation to another kind of numbers.
The words ``how many'' clearly indicate
that the answer is again a number.

Let us try to state the question more pecisely:
are there more or fewer negative numbers
than natural numbers or are
there exactly as many negative numbers
as natural numbers?

To answer the question, 
I would like to suggest
a way to compare two sets.
To compare the size of two sets, $A$ and $B$, 
we create a third set,
which consists of pairs $(a,b)$,
such that $a\in A$ and $b \in B$.
The sets are equal, if and only if
every $a \in A$ appears exactly once
in the new set and every $b \in B$
appears exactly once in the new set.
If there is an $a \in A$
that does not appear in the new set,
but all $b \in B$ appear exactly once,
then $B$ is greater than $A$.
If there is a $b \in B$
that does not appear, but all $a \in A$
appear, then $A$ is greater than $B$.

Furthermore, I suggest a way of counting a set $A$.
We count a set by creating a new set
that consists of pairs $(a,n)$,
such that $a \in A$ and $n \in \mathbb{N}$.
For $n$, we start with 0 and, for each element
in $A$, we increase $n$ by 1 before we put it
into the new set, like this:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~count~::~[a]~\char'31~[(a,{\itshape Natural})]\\
\texfamily ~~count~=~go~0\\
\texfamily ~~~~{\bfseries where}~~go~\char95 ~[]~~~~~=~[]\\
\texfamily ~~~~~~~~~~~go~n~(x:xs)~=~(x,n+1)~:~go~(n+1)~xs
\end{tabbing}
\end{minipage}

The greatest number $n$, we find in the pairs
of this set is the number of elements in $A$.
Let us see if we can count the negative numbers
in this manner.
We count them by creating the set 
$\lbrace (-1,1), (-2,2), (-3,3), \dots\rbrace$.
Do we ever run out of negative or natural numbers?
I don't think so. 
Should we ever feel that we run out of negative
numbers, then we just take the current natural number
and put a minus sign before it.
Should we ever feel that we run out of natural numbers,
then we simply take the current negative number
and remove the negative sign.
This proves, I guess, that there is a way
to assign each negative number to exactly one
natural number and vice versa.
There are hence as many negative numbers
as natural numbers.

Well, how many numbers are that?
That are $|\mathbb{N}|$ numbers.
If you want a word for that, call it 
\term{aleph-zero} and write it like this: $\aleph_0$.
A set with this cardinality is infinite,
but countable. Calling \text{\texfamily count} on it,
we will never get a final answer.
But we will have a partial result at any given step.

What about all the integers?
There should be twice as much as natural numbers, right?
Let us see. We first create a set to count the 
natural numbers:

\[
\lbrace(1,1),(2,2),(3,3),\dots\rbrace
\]

Then we insert a negative number before or behind
each positive number:

\[
\lbrace(1,1),(-1,2),(2,3),(-2,4),(3,5),(-3,6),\dots\rbrace
\]

Again, it appears that we do not run out of natural numbers
to count all the integers. The set of integers,
hence, has cardinality $\aleph_0$ too.

What about fractions?
On the first sight, fractions look very different.
There are infinitely many of them between
any two natural numbers as we have seen 
with Zeno's paradox.
But now comes Cantor and his first
diagonal argument to show that fractions
are countable and, therefore, that the set of fractions
has cardinality $\aleph_0$.

Cantor's proof, his first diagonal argument,
goes as follows. He arranged the fractions
in a table, such that the first column contained
the integers starting from 1 in the first row
and counting up advancing row by row.
The integers correspond to fractions with 1
as denominator. So, we could say,
the first column of this table is dedicated
to denominator 1. The second column, correspondingly,
is dedicated to denominator 2; the third
to denominator 3 and so on.
Then, the rows are dedicated likewise to numerators.
The first row contains numerator 1, the second
contains numerator 2, the third numerator 3
and so on.
Like this:

\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|c|c|c|c|c|c|c}\hline
$\frac{1}{1}$ & $\frac{1}{2}$ & $\frac{1}{3}$ & $\frac{1}{4}$ & $\frac{1}{5}$ & $\frac{1}{6}$ & $\dots$\\\hline
$\frac{2}{1}$ & $\frac{2}{2}$ & $\frac{2}{3}$ & $\frac{2}{4}$ & $\frac{2}{5}$ & $\frac{2}{6}$ & $\dots$\\\hline
$\frac{3}{1}$ & $\frac{3}{2}$ & $\frac{3}{3}$ & $\frac{3}{4}$ & $\frac{3}{5}$ & $\frac{3}{6}$ & $\dots$\\\hline
$\frac{4}{1}$ & $\frac{4}{2}$ & $\frac{4}{3}$ & $\frac{4}{4}$ & $\frac{4}{5}$ & $\frac{4}{6}$ & $\dots$\\\hline
$\frac{5}{1}$ & $\frac{5}{2}$ & $\frac{5}{3}$ & $\frac{5}{4}$ & $\frac{5}{5}$ & $\frac{5}{6}$ & $\dots$\\\hline
$\frac{6}{1}$ & $\frac{6}{2}$ & $\frac{6}{3}$ & $\frac{6}{4}$ & $\frac{6}{5}$ & $\frac{6}{6}$ & $\dots$\\\hline
$\dots$       & $\dots$       & $\dots$       & $\dots$       & $\dots$       & $\dots$       & $\dots$
\end{tabular}
\endgroup
\end{center}

This table is a brute-force approach
to list all fractions in two dimensions.
Obviously, this table contains all possible fractions,
since, for any possible pair of numbers, it contains
a fraction, which, however, is not necessarily
in canonical form, so the table contains duplicates.

Then, using this table, he created a sequence.
He started with the first cell containing $\frac{1}{1}$.
From there he went to the next row $\frac{2}{1}$.
Then he applied the rule \text{\texfamily up},
that is he went up following a diagonal line to the first row,
so he would eventually reach $\frac{1}{2}$.
He went one to the right and then applied rule \text{\texfamily down},
that is he went down following a diagonal line
to the first column, so he would eventually reach $\frac{3}{1}$.
Now he continued to the next row and repeated 
the process of going \text{\texfamily up} and \text{\texfamily down} in
diagonal lines infitely adding the number of each
cell he crossed to the sequence.

The sequence evolves as follows:
Cantor starts with $\frac{1}{1}$, adds $\frac{2}{1}$,
applies rule \text{\texfamily up} and adds $\frac{1}{2}$,
goes to the right, adds $\frac{1}{3}$ and
applies rule \text{\texfamily down} adding $\frac{2}{2}$;
then he goes to the next row adding $\frac{4}{1}$
and goes \text{\texfamily up} again adding $\frac{3}{2}$,
$\frac{2}{3}$ and $\frac{1}{4}$ and so he
goes on forever.

We can reformulate this rule in Haskell,
which will make the process clearer:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~cantor1~::~[{\itshape Ratio}]\\
\texfamily ~~cantor1~=~(1\%1):go~2~1\\
\texfamily ~~~~{\bfseries where}~~go~~~~n~1~~~=~up~~~~n~1~~++~go~1~(n+1)~\\
\texfamily ~~~~~~~~~~~go~~~~1~d~~~=~down~~1~d~~++~go~(d+1)~1\\
\texfamily ~~~~~~~~~~~down~~n~1~~~=~[n\%1]\\
\texfamily ~~~~~~~~~~~down~~n~d~~~=~(n\%d)~:~down~~(n+1)~(d-1)\\
\texfamily ~~~~~~~~~~~up~~~~1~d~~~=~[1\%d]\\
\texfamily ~~~~~~~~~~~up~~~~n~d~~~=~(n\%d)~:~up~~~~(n-1)~(d+1)
\end{tabbing}
\end{minipage}

When we look at rule \text{\texfamily up}, starting at the bottom
of the code, we see the base case where the numerator
is 1. In this case, we just yield \text{\texfamily [1\%d]}.
Otherwise, we call \text{\texfamily up} again with the numerator \text{\texfamily n}
decremented by 1 and the denominator incremented by 1.
\text{\texfamily up\Sp 4\Sp 1}, thus, is processed as follows:

\begin{minipage}{\textwidth}
\text{\texfamily up\Sp 4\Sp 1\Sp =\Sp (4\%1):\Sp up\Sp 3\Sp 2\Sp }\\
\text{\texfamily up\Sp 3\Sp 2\Sp =\Sp (3\%2):\Sp up\Sp 2\Sp 3\Sp }\\
\text{\texfamily up\Sp 2\Sp 3\Sp =\Sp (2\%3):\Sp up\Sp 1\Sp 4\Sp }\\
\text{\texfamily up\Sp 1\Sp 4\Sp =\Sp [1\%4]}
\end{minipage}

yielding the sequence
$\frac{4}{1}, 
 \frac{3}{2}, 
 \frac{2}{3}, 
 \frac{1}{4}$. 
\text{\texfamily go}, after calling \text{\texfamily up}, proceeds with 
\text{\texfamily go\Sp 1\Sp (n+1)}.
We, hence, would continue with 
\text{\texfamily go\Sp 1\Sp 5}, which calls \text{\texfamily down}.
The base case of \text{\texfamily down} is the case
where the denominator is 1.
Otherwise, we increment the numerator by 1
and decrement the denominator by 1.
\text{\texfamily down\Sp 1\Sp 5}, hence, is processed as follows:

\begin{minipage}{\textwidth}
\text{\texfamily down\Sp 1\Sp 5\Sp =\Sp (1\%5):\Sp down\Sp 2\Sp 4\Sp }\\
\text{\texfamily down\Sp 2\Sp 4\Sp =\Sp (2\%4):\Sp down\Sp 3\Sp 3\Sp }\\
\text{\texfamily down\Sp 3\Sp 3\Sp =\Sp (3\%3):\Sp down\Sp 4\Sp 2\Sp }\\
\text{\texfamily down\Sp 4\Sp 2\Sp =\Sp (4\%2):\Sp down\Sp 5\Sp 1\Sp }\\
\text{\texfamily down\Sp 5\Sp 1\Sp =\Sp [5\%1]}
\end{minipage}

yielding the sequence
$\frac{1}{5}, 
 \frac{2}{4}, 
 \frac{3}{3}, 
 \frac{4}{2},
 \frac{5}{1}$. 
When we put the sequence together,
including the first steps,
we see

\[
 \frac{1}{1},
 \frac{2}{1}, 
 \frac{1}{2}, 
 \frac{1}{3}, 
 \frac{2}{2}, 
 \frac{3}{1}, 
 \frac{4}{1}, 
 \frac{3}{2}, 
 \frac{2}{3}, 
 \frac{1}{4}, 
 \frac{1}{5}, 
 \frac{2}{4}, 
 \frac{3}{3}, 
 \frac{4}{2},
 \frac{5}{1},
 \dots
\] 

When we reduce all fractions to canonical form,
we see a lot of repetitions:

\[
 \frac{1}{1},
 \frac{2}{1}, 
 \frac{1}{2}, 
 \frac{1}{3}, 
 \frac{1}{1}, 
 \frac{3}{1}, 
 \frac{4}{1}, 
 \frac{3}{2}, 
 \frac{2}{3}, 
 \frac{1}{4}, 
 \frac{1}{5}, 
 \frac{1}{2}, 
 \frac{1}{1}, 
 \frac{2}{1},
 \frac{5}{1},
 \dots
\] 

We see the numbers 
$\frac{1}{1} = 1$,
$\frac{2}{1} = 2$ and
$\frac{1}{2}$ repeated several times.
They will continue to appear over and over again and,
even worse, other numbers will start to reappear too.
That is, before we can use this sequence to count
fractions, we need to filter duplicates out
leading to the sequence

\[
 \frac{1}{1},
 \frac{2}{1}, 
 \frac{1}{2}, 
 \frac{1}{3}, 
 \frac{3}{1}, 
 \frac{4}{1}, 
 \frac{3}{2}, 
 \frac{2}{3}, 
 \frac{1}{4}, 
 \frac{1}{5}, 
 \frac{5}{1},
 \dots
\]

But now we see that we can enumerate, that is count,
the fractions creating the sequence

\[
 \left(\frac{1}{1},1\right),
 \left(\frac{2}{1},2\right), 
 \left(\frac{1}{2},3\right), 
 \left(\frac{1}{3},4\right), 
 \left(\frac{3}{1},5\right), 
 \left(\frac{4}{1},6\right), 
 \left(\frac{3}{2},7\right), 
 \left(\frac{2}{3},8\right), 
 \left(\frac{1}{4},9\right), 
 \dots
\]

This clearly shows that the cardinality 
of the set of fractions is $\aleph_0$.\qed

This result may feel a bit odd on the first sight.
We clearly have the feeling that there must
be more fractions than integers, because,
between any two integers, there are infinitely many
fractions. 
When we think of a visualisation with a pair of balances,
with the fractions being in one balance and the integers
in the other, then, what we would see at any given instance, 
clearly indicates that there must be more fractions than integers.
However, our feeling betrays us, when it comes
to infinity. Indeed, our feeling was not made for infinity.
Therefore, at least if we accept the notions
of comparison and counting outlined above,
then we have to accept the result of Cantor's
argument.
Even further, I would say that the fact that this argument
shows things in a way that contradicts our spontaneous
way to see these things, underlines
the extraordinary quality of this argument.
Cantor lets us see things that are usually
hidden from our perception.
This makes Cantor, who was seen by his
contemporary opponents as a kind of sorcerer,
a true magus.

It is, by the way, quite simple
to extend the argument to negative fractions.
We just have to insert behind each number
its additive inverse, resulting in
the sequence:

\[
 \left(\frac{1}{1},1\right),
 \left(-\frac{1}{1},2\right),
 \left(\frac{2}{1},3\right), 
 \left(-\frac{2}{1},4\right), 
 \left(\frac{1}{2},5\right), 
 \left(-\frac{1}{2},6\right), 
 \left(\frac{1}{3},7\right), 
 \left(-\frac{1}{3},8\right), 
 \dots
\]

Indeed, there appears to be a lot of room
for new numbers, once we are dealing with
infinity.
This led the great David Hilbert to the
analogy of the hotel, which is today 
named after him \term{Hilbert's Hotel}.
In this analogy, there is a hotel with the
uncommon property of having infinitely
many rooms. This hotel will never run
out of rooms for new guests.
If a new guest arrives and there are
already infinitely many guests,
the manager just asks the guests
to move one room up, \ie\ the guest
currently in room 1 moves to room 2,
the guest in room 2 moves to room 3
and so on. 
At the end of the process,
room 1 is free for the new arriving guest.
Since there are infinitely
many rooms, there is no guest,
even though there are infinitely many of them already,
who would not find a room with a room number
one greater than his previous room number.

This approach works for any number of finitely many
guests arriving.
But even when infinitely many new guests arrive,
the manager, still, has resources.
In this case, he could ask the guests to move to
a room with a number twice the number of his
current room, \eg\ the guest in room 1
would move to room 2, the guest in room 2
would move to room 4, the guest in room 3
would move to room 6 and so on
leaving infinitely many rooms
with odd room numbers unoccupied
and making room for infinitely many new guests.

But let us go back to more technical stuff.
In spite of its ingenuity, Cantor's argument
is not perfect.
In particular, the sequence and how it is
created is quite uggly,
but, apparently, nobody, for more than hundred years,
cared too much about that. 
Then, in \num{2000}, Neil Calkin and Herbert Wilf
published a paper with a new sequence with 
a bunch of interesting properties that make
this sequence for enumerating the fractions
much more attractive than Cantor's original
sequence. The beginning of the sequence is

\[
 \frac{1}{1},
 \frac{1}{2}, 
 \frac{2}{1}, 
 \frac{1}{3}, 
 \frac{3}{2}, 
 \frac{2}{3}, 
 \frac{3}{1}, 
 \frac{1}{4}, 
 \frac{4}{3}, 
 \frac{3}{5}, 
 \frac{5}{2},
 \frac{2}{5},
 \frac{5}{3},
 \frac{3}{4},
 \frac{4}{1},
 \dots
\]

The sequence, as we will show in a minute,
corresponds to a \term{binary tree} of the form

\begin{center}
\begin{tikzpicture}
% root
\node (A1) at ( 6,  0) {$\frac{1}{1}$};

% first level
\node (A2) at ( 3,-1 ) {$\frac{1}{2}$};
\node (A3) at ( 9,-1 ) {$\frac{2}{1}$};

% kids of A2
\node (A4) at (1.5,-2 ) {$\frac{1}{3}$};
\node (A5) at (4.5,-2 ) {$\frac{3}{2}$};

% kids of A3
\node (A6) at (7.5,-2 ) {$\frac{2}{3}$};
\node (A7) at (10.5,-2 ) {$\frac{3}{1}$};

% kids of A4
\node (A8) at (0.75,-3 ) {$\frac{1}{4}$};
\node (A9) at (2.25,-3 ) {$\frac{4}{3}$};

% kids of A5
\node (A10) at (3.75,-3 ) {$\frac{3}{5}$};
\node (A11) at (5.25,-3 ) {$\frac{5}{2}$};

% kids of A6
\node (A12) at (6.75,-3 ) {$\frac{2}{5}$};
\node (A13) at (8.25,-3 ) {$\frac{5}{3}$};

% kids of A7
\node (A14) at (9.75,-3 ) {$\frac{3}{4}$};
\node (A15) at (11.25,-3 ) {$\frac{4}{1}$};

% kids of A8
\node (A16) at (0.375,-4 ) {$\frac{1}{5}$};
\node (A17) at (1.125,-4 ) {$\frac{5}{4}$};

% kids of A9
\node (A18) at (1.875,-4 ) {$\frac{4}{7}$};
\node (A19) at (2.625,-4 ) {$\frac{7}{3}$};

% kids of A10
\node (A20) at (3.375,-4 ) {$\frac{3}{8}$};
\node (A21) at (4.125,-4 ) {$\frac{8}{5}$};

% kids of A11
\node (A22) at (4.875,-4 ) {$\frac{5}{7}$};
\node (A23) at (5.625,-4 ) {$\frac{7}{2}$};

% kids of A12
\node (A24) at (6.375,-4 ) {$\frac{2}{7}$};
\node (A25) at (7.125,-4 ) {$\frac{7}{5}$};

% kids of A13
\node (A26) at (7.875,-4 ) {$\frac{5}{8}$};
\node (A27) at (8.625,-4 ) {$\frac{8}{3}$};

% kids of A14
\node (A28) at ( 9.375,-4 ) {$\frac{3}{7}$};
\node (A29) at (10.125,-4 ) {$\frac{7}{4}$};

% kids of A15
\node (A30) at (10.875,-4 ) {$\frac{4}{5}$};
\node (A31) at (11.625,-4 ) {$\frac{5}{1}$};

% connect root
\connect {A1} {A2};
\connect {A1} {A3};

% connect A2
\connect {A2} {A4};
\connect {A2} {A5};

% connect A3
\connect {A3} {A6};
\connect {A3} {A7};

% connect A4
\connect {A4} {A8};
\connect {A4} {A9};

% connect A5
\connect {A5} {A10};
\connect {A5} {A11};

% connect A6
\connect {A6} {A12};
\connect {A6} {A13};

% connect A7
\connect {A7} {A14};
\connect {A7} {A15};

% connect A8
\connect {A8} {A16};
\connect {A8} {A17};

% connect A9
\connect {A9} {A18};
\connect {A9} {A19};

% connect A10
\connect {A10} {A20};
\connect {A10} {A21};

% connect A11
\connect {A11} {A22};
\connect {A11} {A23};

% connect A12
\connect {A12} {A24};
\connect {A12} {A25};

% connect A13
\connect {A13} {A26};
\connect {A13} {A27};

% connect A14
\connect {A14} {A28};
\connect {A14} {A29};

% connect A15
\connect {A15} {A30};
\connect {A15} {A31};

\end{tikzpicture}
\end{center}

When you have a closer look at the tree,
you see that the kids of each node
are created by a simple formula.
If the current node has the form
$\frac{n}{d}$, then the left kid
corresponds to $\frac{n}{n+d}$ and
the right kid corresponds to $\frac{n+d}{d}$.
For instance, the kids of $\frac{1}{1}$
are $\frac{1}{1+1}$ and $\frac{1+1}{1}$.
The kids of $\frac{3}{2}$ are
$\frac{3}{3+2}$ and $\frac{3+2}{2}$.

We can easily create this tree in Haskell.
First, we need a data type to represent
the tree:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~{\bfseries type}~{\itshape CalWiTree}~=~{\itshape Tree}~{\itshape Ratio}
\end{tabbing}
\end{minipage}

The \text{\texfamily {\itshape Tree}} data type is in fact not a
binary tree, but a generic tree with
an arbitrary number of nodes.
But it is simple to implement and
serves our purpose.
The data type is parametrised,
so we define a specialised data type
\text{\texfamily {\itshape Tree}\Sp {\itshape Ratio}} and the type synonym
\text{\texfamily {\itshape CalWiTree}} referring to this type.
Now we create the tree with:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~calWiTree~::~{\itshape Zahl}~\char'31~{\itshape Ratio}~\char'31~{\itshape CalWiTree}\\
\texfamily ~~calWiTree~1~r~~~~~~~~~~=~{\itshape Node}~r~[]\\
\texfamily ~~calWiTree~i~r@({\itshape Q}~n~d)~~=~{\itshape Node}~r~[~~calWiTree~(i-1)~(n~\%~(n+d)),\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~calWiTree~(i-1)~((n+d)~\%~d)]
\end{tabbing}
\end{minipage}

The function takes two arguments,
a \text{\texfamily {\itshape Zahl}} and a \text{\texfamily {\itshape Ratio}}.
The \text{\texfamily {\itshape Ratio}} is the starting point
and the \text{\texfamily {\itshape Zahl}} is the number of generations
we want to create.
Often we do not want the function to create
the entire sequence -- for that a lot of patience
and memory resources would be necessary --
but only a tiny part of it.
In this case, we set the \text{\texfamily {\itshape Zahl}} to $i\ge 1$.
If $i$ reaches 1, we create the current node
without kids.
If we want the function to create the entire
infinite tree, we just assign a value $i < 1$.
If $i\neq 1$,
we create a node with \text{\texfamily r} as data and 
the kids resulting from calling \text{\texfamily calWiTree} on
$\frac{n}{n+d}$ and $\frac{n+d}{d}$
with $i$ decremented by 1.

This shows that there is a very simple algorithm
to generate the tree.
We will now show that 
Calkin-Wilf tree and Calkin-Wilf sequence 
are equivalent.
We do so by creating an algorithm that 
converts the tree to the sequence.

We may be tempted to do this with a typical
recursive function that does something with the current node
and then adds the result of the operation to
the partial sequences that result from recursively
calling the function on the left and the right kid.
This approach, however, is \term{depth-first}.
The resulting sequences would follow the branches
of the tree. It would create partial sequences
like, for instance, 
$\frac{1}{1},
 \frac{1}{2},
 \frac{1}{3},
 \frac{1}{4},
 \dots$
But what we need is partial sequences
that cover generation by generation, \ie\
$\frac{1}{1},
 \frac{1}{2},
 \frac{2}{1},
 \frac{1}{3},
 \frac{3}{2},
 \frac{2}{3},
 \frac{3}{1}$ and so on.
In other words, we need a \term{breadth-first} approach.

Since this is a generic problem,
we can define a function on the level
of the generic \text{\texfamily {\itshape Tree}} data type
that creates a sequence composed
of subsequences corresponding to tree generations:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~getKids~::~{\itshape Natural}~\char'31~{\itshape Tree}~a~\char'31~[a]\\
\texfamily ~~getKids~1~~({\itshape Node}~r~~\char95 )~~~~~~~=~~[r]\\
\texfamily ~~getKids~n~~({\itshape Node}~r~~[])~~~~~~=~~[]\\
\texfamily ~~getKids~n~~({\itshape Node}~r~~(x:xs))~~=~~getKids~(n-1)~x~++~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~getKids~n~({\itshape Node}~r~xs)~
\end{tabbing}
\end{minipage}

The function receives two arguments,
a \text{\texfamily {\itshape Natural}}, which determines the generation we want to obtain,
and the tree on which we are operating.
If the generation is 1, we just give back the data
of the current node.
Otherwise, we distinguish two cases:
If the current node has no kids, then the result 
is the empty list. This indicates that we have
exhausted the current (finite) tree.
Otherwise, we advance recursively on the head and tail
of the list of kids.
Decisive is that we do not add anything 
to the resulting sequence, before we have reached
the intended depth $n$.
This way, the function produces a sequence
containing all kids on level $n$.
We now just apply \text{\texfamily getKids} to all
generations in the tree:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~calWiTree2Seq~::~{\itshape CalWiTree}~\char'31~[{\itshape Ratio}]\\
\texfamily ~~calWiTree2Seq~t~=~go~1\\
\texfamily ~~~~{\bfseries where}~go~n~=~~{\bfseries case}~getKids~n~t~{\bfseries of}\\
\texfamily ~~~~~~~~~~~~~~~~~~[]~~\char'31~[]\\
\texfamily ~~~~~~~~~~~~~~~~~~sq~~\char'31~sq~++~go~(n+1)
\end{tabbing}
\end{minipage}

We have shown that there is a simple algorithm
to generate the tree and 
that there is a simple algorithm to convert
the tree into the sequence.
The latter is quite useful,
since it means that tree and sequence are equivalent.
This allows us to prove some crucial properties
of the sequence using the tree,
which is much simpler than proving them
on the sequence directly.

Already a quick glance at the tree
reveales some interesting properties,
for instance, that, in all cases,
the left kid is smaller and the right kid
is greater than the parent node.
This, of course, is just a consquence
of the generating algorithm.
We also see that the integers are all
in the right-most branch, which equals
the first column in Cantor's table.
The left-most branch equals the first row
in Cantor's table: it contains all fractions
with 1 in the numerator.
We also see that all fractions are in
canonical form, different from Cantor's table.
Also, no fraction repeats and, as far as we can tell,
the fractions appear to be complete.

The crucial properties, those that we need
to show that the Calkin-Wilf sequence contains
exactly all fractions and, thus, can be used
for Cantor's argument in place of the old sequence,
are:

\begin{enumerate}
\item All fractions in the tree are in canonical form;
\item Every possible fraction (in canoncial form) 
      appears in the tree;
\item Every fraction appears exactly once.
\end{enumerate}

The first property is simple to prove.
We observe that
the first fractions appearing in the tree 
that do not have
1 in numerator or denominator have
there either 2 or 3. 2 and 3 a coprime to each other.
Summing two coprimes, $a$ and $b$, will lead to a number
that again is coprime to both $a$ and $b$.
Therefore, if we have at a node $n$
a fraction whose numerator and denominator
are coprime to each other, the whole subtree
below $n$ will contain fractions whose
numerator and denominator are coprime to each other.
Therefore, the subtrees below $\frac{3}{2}$ and
$\frac{2}{3}$ will only contain fractions in canonical form.

The fractions that actually have 1 in 
numerator or denominator, however,
will necessarily lead to a fraction
whose numerator and denominator are coprime
to each other, since no number $n$, except $n=1$,
shares any divisor with $n+1$.
Since we start the tree with 
$\frac{1}{1}$, the whole tree can only contain
fractions in canonical form.\qed 

We prove the second property by contradiction.
Let us assume that there are fractions 
that do not appear in the tree. 
Then, there is a number of fractions that have
the smallest denominator and, among those,
there is one with the smallest numerator.
Let $\frac{n}{d}$ be this fraction.
If $n>d$, then $\frac{n-d}{d}$ cannot appear either, since
$\frac{n}{d}$ would be its right kid.
But that is a contradiction to our assumption that
$\frac{n}{d}$ was the nonappearing fraction
with the smallest numerator among those fractions
with denominator $d$, but, obviously, $n-d$
is an even smaller numerator.
If $n<d$, then $\frac{n}{d-n}$ cannot appear either, since,
$\frac{n}{d}$ would be its left kid.
But that again is a contradiction, since the denominator
is smaller than the denominator of one of the fractions 
we assumed to be those with the smallest denominator.
The only way to solve this dilemma is
to assume that $n$ and $d$ are equal.
Then, indeed, $\frac{n-d}{d} = 0$ and
$\frac{n}{n-d} = \bot$ would not be in the tree.
But such a fraction with $n=d$ is irrelevant,
since it reduces to $\frac{1}{1}$, 
which already is in the tree.\qed

To prove the third property, we first observe
that 1 is the root of the tree. 
Since any fraction below 1 is either
$\frac{n}{n+d}$ or $\frac{n+d}{d}$,
there cannot be a fraction with $n=d$.
With this out of the way, we can argue 
as we did for the second property:
we assume there are fractions that appear
more than once.
From all these fractions, there is a group 
that shares the smallest denominator and,
among this group, one with the smallest numerator.
But this fraction is then either the left kid
of two fractions of the form $\frac{n}{d-n}$,
making the denominator of these fractions even smaller,
or the right kid of two fractions of the form
$\frac{n-d}{d}$, making the numerator of these fractions
even smaller. In both cases we arrive at a contradiction.\qed

We can also prove directly -- but the argument 
may be more subtle or, which is the same,
almost self-evident.
We know that all nodes are of either of the forms
$\frac{n}{n+d}$ or $\frac{n+d}{d}$.
Since, except for the root node, we never have
$n=d$, no node derived from one of those fractions
can ever equal one derived from the other.
To say that two fractions
derived from such nodes are equal, would mean that
we could have two numbers $n$ and $d$, such that
$n=n+d$ and $d=n+d$. That would only work if
$n=0$ and $d=0$. But that case cannot occur.\qed

That taken all together shows that the Calkin-Wilf sequence
contains all fractions exactly once.
Since we can enumerate this sequence, we can 
enumerate all fractions and, hence, $|\mathbb{Q}| = \aleph_0$.\qed

There is still another advantage of this sequence
over Cantor's original one.
There is a simple, yet quite exciting way
to compute which is the $n^{th}$ fraction.
The key is to realise that we are dealing
with a structure, namely a binary tree, 
that stores sequences of binary decisions.
At any given node in the tree,
we can go either right or left.
We could, therefore, describe 
the $n^{th}$ position as a trajectory
through the tree, where at each node,
we take a binary decision:
going right or going left.
An efficient way to encode a sequence
of binary decisions is a binary number,
and, indeed, the position in the sequence,
represented as a binary number
leads to the fraction at that position.
Here is a function that,
given a natural number $n$,
returns the fraction in the Calkin-Wilf sequence
at position $n$:

\ignore{
\begin{tabbing}\texfamily
~~toBaseN~::~{\itshape Natural}~\char'31~{\itshape Natural}~\char'31~[{\itshape Int}]\\
\texfamily ~~toBaseN~b~=~reverse~\char'00~go\\
\texfamily ~~~~{\bfseries where}~go~x~=~~~{\bfseries case}~x~`quotRem`~b~{\bfseries of}\\
\texfamily ~~~~~~~~~~~~~~~~~~~(0,~r)~\char'31~[fromIntegral~r]\\
\texfamily ~~~~~~~~~~~~~~~~~~~(q,~r)~\char'31~(fromIntegral~r)~:~go~q\\
\texfamily \\
\texfamily ~~fromBaseN~::~{\itshape Natural}~\char'31~[{\itshape Int}]~\char'31~{\itshape Natural}\\
\texfamily ~~fromBaseN~b~=~go~0~\char'00~map~fromIntegral~\char'00~reverse\\
\texfamily ~~~~{\bfseries where}~~go~\char95 ~[]~~~~~=~0\\
\texfamily ~~~~~~~~~~~go~x~(r:rs)~=~r*(b\char94 x)~+~go~(x+1)~rs~\\
\texfamily \\
\texfamily ~~toBinary~::~{\itshape Natural}~\char'31~[{\itshape Int}]\\
\texfamily ~~toBinary~=~toBaseN~2\\
\texfamily \\
\texfamily ~~fromBinary~::~[{\itshape Int}]~\char'31~{\itshape Natural}\\
\texfamily ~~fromBinary~=~fromBaseN~2
\end{tabbing}
}

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~calwiR~::~{\itshape Natural}~\char'31~{\itshape Ratio}\\
\texfamily ~~calwiR~=~go~(0~\%~1)~\char'00~toBinary~\\
\texfamily ~~~~{\bfseries where}~~go~r~[]~~~~~~~~~~~~~~=~r\\
\texfamily ~~~~~~~~~~~go~r@({\itshape Q}~n~d)~(0:xs)~~=~go~(n~\%~(n+d))~xs\\
\texfamily ~~~~~~~~~~~go~r@({\itshape Q}~n~d)~(1:xs)~~=~go~((n+d)~\%~d)~xs
\end{tabbing}
\end{minipage}

We start by converting $n$ to binary format
(\ie\ a list of 0s and 1s).
Then we call \text{\texfamily go} starting with 0, since,
for the first number 1, we want position 1.
If we have exhausted the binary number,
the result is just $r$, the rational number we pass
to \text{\texfamily go}. Otherwise, we distinguish two cases:
the head of the binary number being 0 or 1.
If it is 0, we follow the left branch,
which has the fraction $\frac{n}{n+d}$
at its top; if it is 1, we follow 
the right branch with the fraction $\frac{n+d}{d}$.

Consider $n=25$ as an example.
25 in binary format is $11001$.
We go through the steps:

\text{\texfamily go\Sp ({\itshape Q}\Sp 0\Sp 1)\Sp [1,1,0,0,1]\Sp \Sp =\Sp go\Sp (1\%1)\Sp [1,0,0,1]}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 1\Sp 1)\Sp [1,0,0,1]\Sp \Sp \Sp \Sp =\Sp go\Sp (2\%1)\Sp [0,0,1]}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 1)\Sp [0,0,1]\Sp \Sp \Sp \Sp \Sp \Sp =\Sp go\Sp (2\%3)\Sp [0,1]}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 3)\Sp [0,1]\Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp =\Sp go\Sp (2\%5)\Sp [1]}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 5)\Sp [1]\Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp =\Sp go\Sp (7\%5)\Sp []}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 7\Sp 5)\Sp []\Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp \Sp =\Sp {\itshape Q}\Sp 7\Sp 5}

Let us check if this is true;
\text{\texfamily take\Sp 1\Sp \$\Sp drop\Sp 24\Sp \$\Sp calWiTree2Seq\Sp (calWiTree\Sp 5\Sp (1\%1))} gives

\text{\texfamily [{\itshape Q}\Sp 7\Sp 5]},

which corresponds to the correct result $\frac{7}{5}$.
With this we can create the sequence much simpler
without deviating through the tree:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~calwis~::~[{\itshape Ratio}]\\
\texfamily ~~calwis~=~map~calwiR~[1..]
\end{tabbing}
\end{minipage}

We can also do the opposite:
compute the position of any given fraction.
For this, we just have to turn the logic
described above around:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~calwiP~::~{\itshape Ratio}~\char'31~{\itshape Natural}\\
\texfamily ~~calwiP~=~fromBinary~\char'00~reverse~\char'00~go~\\
\texfamily ~~~~{\bfseries where}~~go~({\itshape Q}~0~\char95 )~~=~[]\\
\texfamily ~~~~~~~~~~~go~({\itshape Q}~n~d)~~|~n~\char'35~d~~~~~=~1~:~go~((n~-~d)~\%~d)\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~=~0~:~go~(n~\%~(d-n))
\end{tabbing}
\end{minipage}

We look at the fraction and, if $n\ge d$,
then we add 1 to the digits of the 
resulting binary number, otherwise,
we add 0.
Note that there is only one case where $n = d$
(as we have proven above), namely the root node
of the tree. In all other cases, we either have
$n>d$ or $n<d$.
In the first case, we know that the fraction
is a right kid and in the second case,
we know it is a left kid.
When we call this function on $\frac{7}{5}$,
we see the steps:

\begin{minipage}{\textwidth}
\text{\texfamily go\Sp ({\itshape Q}\Sp 7\Sp 5)\Sp =\Sp 1\Sp :\Sp go\Sp (2\Sp \%\Sp 5)}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 5)\Sp =\Sp 0\Sp :\Sp go\Sp (2\Sp \%\Sp 3)}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 3)\Sp =\Sp 0\Sp :\Sp go\Sp (2\Sp \%\Sp 1)}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 2\Sp 1)\Sp =\Sp 1\Sp :\Sp go\Sp (1\Sp \%\Sp 1)}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 1\Sp 1)\Sp =\Sp 1\Sp :\Sp go\Sp (0\Sp \%\Sp 1)}\\
\text{\texfamily go\Sp ({\itshape Q}\Sp 0\Sp 1)\Sp =\Sp []},
\end{minipage}

which leads to the list \text{\texfamily 1:0:0:1:1:[]}.
This evaluated and reversed is \text{\texfamily 1,1,0,0,1},
which, converted to decimal representation, is 25.

Surprisingly or not, the Calkin-Wilf sequence
is not completely new. A part of it was already studied
in the $19^{th}$ century by 
German mathematician Moritz Stern (1807 -- 1894),
successor of the early deceased sucessor of Gauss
at the University of Göttingen, Lejeune-Dirichlet, 
and professor of Bernhard Riemann.
The numerators of the Calkin-Wilf sequence correspond to 
\term{Stern's diatomic sequence}.
Using the Calkin-Wilf sequence,
we can produce Stern's sequence with the function

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~stern~::~[{\itshape Natural}]\\
\texfamily ~~stern~=~map~numerator~calwis\\
\texfamily ~~~~{\bfseries where}~numerator~({\itshape Q}~n~\char95 )~=~n
\end{tabbing}
\end{minipage}

\text{\texfamily take\Sp 32\Sp stern} shows:

\[
  1,1,2,1,3,2,3,1,4,3,5,2,5,3,4,1,5,4,7,3,8,5,7,2,7,5,8,3,7,4,5,1.
\]

Mapping \text{\texfamily denominator} defined as \text{\texfamily denominator\Sp ({\itshape Q}\Sp \char95 \Sp d)\Sp =\Sp d}
on the Calkin-Wilf sequence would give a very similar result:
the Stern sequence one ahead, \ie:

\[
  1,2,1,3,2,3,1,4,3,5,2,5,3,4,1,5,4,7,3,8,5,7,2,7,5,8,3,7,4,5,1,6.
\]

Edsgar Dijkstra, the great pioneer of the art of computer programming,
studied this sequence not knowing that it had already been studied before. 
He called it the \term{fusc} sequence and generated it with
the function

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~fusc~::~{\itshape Natural}~\char'31~{\itshape Natural}\\
\texfamily ~~fusc~0~~=~0\\
\texfamily ~~fusc~1~~=~1\\
\texfamily ~~fusc~n~~|~even~n~~~~~=~~fusc~(n~`div`~2)\\
\texfamily ~~~~~~~~~~|~otherwise~~=~~{\bfseries let}~k~=~(n-1)~`div`~2~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~{\bfseries in}~fusc~k~+~fusc~(k+1)
\end{tabbing}
\end{minipage}

From the definition of the \text{\texfamily fusc} function,
we can read some of the many properties
of Stern's sequence (and the Calkin-Wilf tree).
First, an even number has the same value
as half of that number, for instance
\text{\texfamily fusc\Sp 3} is 2 and so is \text{\texfamily fusc\Sp 6},
\text{\texfamily fusc\Sp 12}, \text{\texfamily fusc\Sp 24} and so on.
Even numbers in binary format
end on zeros. For instance,
3 in binary notation is 11.
$2 \times 3$ is 110,
$2 \times 6$ is 1100,
$2 \times 12$ is 11000 and so on.
The binary format clearly indicates that, after having
reached the number before the trail of zeros
at the end, we go down in a straight line
following the left branch of that node
in the Calkin-Wilf tree.
Since, in the left path, the numerator
never changes, the result of 
$fusc(n)$ equals the result of $fusc(2n)$.

We also see that for any power of 2,
\text{\texfamily fusc} equals \text{\texfamily fusc\Sp 1}, which is 1;
\text{\texfamily map\Sp fusc} \text{\texfamily [2,4,8,16,32,} \text{\texfamily 64,128,256]}, hence,
gives \text{\texfamily [1,1,1,1,1,1,1,1]}.
Note that, looking at the Calkin-Wilf tree,
this is immediate obvious, since powers of 2
in binary representation are numbers of the form
$1,10,100,1000,\dots$
Those numbers indicate that 
we navigate through the tree in a straight line
following the left branch of the root node $\frac{1}{1}$.

The \text{\texfamily fusc} results of powers of two minus one ($1,11,111,1111,\dots$)
equal the number of digits of this number in binary form.
This is the right outer branch
of the tree with the integers.

The \text{\texfamily fusc} results of powers of two plus one ($1,11,101,1001,\dots$)
also equal the number of digits
in the binary representation of that number.
These numerators appear in the immediate neighbours
of the powers of two in the left outer branch of the tree,
for instance 
$\frac{3}{2},
 \frac{4}{3},
 \frac{5}{4},
 \frac{6}{5}, \dots$

What about numbers with an alternating sequence
of 1s and 0s, like 101010101?
Those numbers are not in the outer branches
and not even close to them. Indeed, they 
tend to the horizontal centre of the tree.
The first 1 leads to node $\frac{1}{1}$.
We now go left, that is,
we add the numerator to the denominator leading
to $\frac{1}{2}$; we then add the denominator
to the numerator leading to $\frac{3}{2}$;
then we add the numerator to the denominator again
leading to $\frac{3}{5}$ 
and so we go on and obtain the fractions
$\frac{8}{5},
 \frac{8}{13},
 \frac{21}{13},
 \frac{21}{34},
 \frac{55}{34},\dots$
Do you see the point?
All the numerators and denominators are Fibonacci numbers!
Well, what we did above,
adding the two numbers we obtained before
starting with the pair $(1,1)$,
is just the recipe
to create the Fibonacci numbers.

An amazing property of \text{\texfamily fusc},
found by Dijkstra, is the fact that
two numbers whose binary representations
are the reverse of each other
have the same \text{\texfamily fusc} result. 
25, for instance, is 11001.
The reverse, 10011, is 19,
and \text{\texfamily fusc\Sp 19\Sp =\Sp fusc\Sp 25\Sp =\Sp 7}.

For the Calwin-Wilf tree this means that,
when we have two trajectories through the tree,
where each step after the root node
is the opposite of the simultaneous step
of the other one, we arrive at two fractions
with the same numerator.
The trajectory defined by the binary sequence
$1,1,0,0,1$ leads, first, to the root node $\frac{1}{1}$,
then through 
$\frac{2}{1}$,
$\frac{2}{3}$ and
$\frac{2}{5}$ to
$\frac{7}{5}$.
The reverse of this sequence, $1,0,0,1,1$ leads,
first, to the root node $\frac{1}{1}$ and then through
$\frac{1}{2}$,
$\frac{1}{3}$ and
$\frac{4}{3}$ to
$\frac{7}{3}$.

A similar is true for two numbers,
whose binary sequences can be transformed
into one another by inverting the inner bits.
For instance, 11001, 25, can be transformed into
10111, inverting all bits, but the first and the last one.
10111 is 23 and \text{\texfamily fusc\Sp 19\Sp =\Sp fusc\Sp 23\Sp =\Sp fusc\Sp 25}.
What about the bit inverse of 19?
That is 11101, the reverse of 10111 and 29 in decimal notation.
Therefore \text{\texfamily fusc\Sp 19\Sp =\Sp fusc\Sp 23\Sp =\Sp fusc\Sp 25\Sp =\Sp fusc\Sp 29}.

We can reformulate this result in terms of group theory.
We have three basic transformations $i$, the identity,
$\rho$, the reverse, and $\beta$, the bit inverse.
We add one more transformation, the composition of
$\rho$ and $\beta$ and call it $\sigma = \rho \cdot \beta$.
The operation defined over this set is composition.
We see that the identity is part of the set;
for each transformation, its inverse is in the set, too,
because $\rho \cdot \rho = i$, $\beta \cdot \beta = i$ and
$\sigma \cdot \sigma = i$.
To illustrate the logic of this group with the numbers above,
we define it on the base string 19, which is 10011:

$i = 10011$\\
$\rho = 11001$\\
$\beta = 11101$\\
$\sigma = 10111$.

Now we can play around and see that we will
never generate a string that is not already 
in the group:

$\rho  \cdot i     = 11001$\\
$\rho  \cdot \rho  = i = 10011$\\
$\beta \cdot \beta = i = 10011$\\
$\sigma \cdot \sigma = i = 10011$\\
$\rho  \cdot \beta = \sigma = 10111$\\
$\beta \cdot \rho  = \sigma = 10111$\\
$\sigma \cdot \rho  = \beta = 11101$\\
$\sigma \cdot \beta = \rho  = 11001$\\
$\dots$

All elements of one group are in the same generation
of the Calkin-Wilf tree,
since they all have the same number of digits.
Numbers with a symmetric binary representation,
such that $\rho = i$, lead to groups with only 
two distinguishable members, for instance
$fusc(2^n+1) = fusc(2^{n+1}-1)$.
The same is true for numbers with a binary representation
such that $\rho = \beta$, for instance, 
101011 (43) = 110101 (53) and $fusc(43) = fusc(53) = 13$.

There are infinitely many numbers
with the same \text{\texfamily fusc} result.
Most of these numbers have trailing zeros
and, as such, are in the long shadow thrown 
by one of the original odd numbers with the same result.
One example of such a shadow is the outer left branch,
which maintains the numerator of the root node $\frac{1}{1}$
and also maintains the leading 1 in the binary representation
of its positions, merely adding more and more 0s to it.

How many ``original'' numbers in this sense are there
for a given \text{\texfamily fusc} result $n$?
The answer is simple if we consider two facts:
1. The fractions in the Calkin-Wilf tree are in
canonical form, \ie\ numerator and denominator
do not share divisors, and
2. Any position number whose binary representation
ends with 1, points to a right kid and, for all fractions
$\frac{n}{d}$ that are right kids:
$n > d$. Binary numbers, however, that end with 0,
point to a left kid and, therefore, $n < d$.
In other words, the number of original numbers
for a given numerator $n$ is $\varphi(n)$, the totient number of $n$.
The denominators of the original fractions are 
the coprimes of $n$.

The numerator 7, for instance, appears in six positions:
19, 23, 25, 29, 65 and 127.
The denominators of the fractions at those positions are
3, 2, 5, 4, 6 and 1.
For numerator 8, there are only four such numbers:
21, 27, 129 and 255.
The denominators at those positions are 5, 3, 7 and 1.
Note that 21 in binary format is 10101, 
which is its own reverse,
and 27 is the bit inverse of 21, namely 11011,
which also is its own reverse.

Furthermore, those numbers appear in groups
with 2 or 4 members, depending on the properties 
of the binary representation. The number of such groups,
hence, is $\frac{\varphi(n)}{k}$, 
where $k$ is some integer that divides $\varphi(n)$.
For 7: $k=3$, since there are two groups,
one containing 4 elements, the other containing 2.
For 8: $k=2$, since there are two groups, 
both containing 2 elements.

\ignore{
The $k$ values for the numbers 1-20 are

1,1,2,2,2,2,3,2,3,2,5,2,4,6,4,4,5,6,4,4

which, up to my knowledge, is not a known integer sequence.
}

The last group is the one consisting of
binary numbers with $n$ bits, \ie\ $2^{n-1}+1$ and $2^n-1$.
The other groups appear in generations of the Calkin-Wilf tree
before the generation with that final group.
For 7, the generation of the group
with four members is the fifth generation and
the generation with the final group is of course the seventh generation.
In other cases,
the groups can be many generations apart.
The numerator 55, for instance, appears for the first time
in generation 10, namely in the fraction $\frac{55}{34}$
(both Fibonacci numbers).
This is far off from generation 55 
with the group consisting of $\frac{55}{1}$ and $\frac{55}{54}$.

Interestingly, Dijkstra was not aware of the relation
of the \text{\texfamily fusc} algorithm to the Stern sequence, and
the Calkin-Wilf tree was not even around at that time.
Dijkstra describes \text{\texfamily fusc} as a state automaton that
parses strings consisting of 1s and 0s.
The parsing result would be a number, 
namely the result of \text{\texfamily fusc}.
We could now say that the Calkin-Wilf tree
is a model that gives meaning to the strings
in terms of trajectories through the tree.

A final remark relates to the product of one generation in the tree.
Each generation consists of fractions 
whose numerators and denominators were created
by adding the numerators and denominators 
of the fractions of the previous generation.
We start with the fraction $\frac{1}{1}$.
In consequence, in any generation, 
there is for any fraction $\frac{n}{d}$ 
a fraction $\frac{d}{n}$.
The fractions in the fifth generation for example,
the one containing the fractions 
at positions 19, 23, 25 and 29 in the Calkin-Wilf sequence,
can be paired up in the following way:

\[
\left(\frac{1}{5}, \frac{5}{1}\right),
\left(\frac{5}{4}, \frac{4}{5}\right),
\left(\frac{4}{7}, \frac{7}{4}\right),
\left(\frac{7}{3}, \frac{3}{7}\right),
\left(\frac{3}{8}, \frac{8}{3}\right),
\left(\frac{8}{5}, \frac{5}{8}\right),
\left(\frac{5}{7}, \frac{7}{5}\right),
\left(\frac{7}{2}, \frac{2}{7}\right).
\]

The product of each pair is 1.
The product of all fractions 
in one generation is therefore 1 as well.
You can try this out with
the simple function 

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~genprod~::~{\itshape Natural}~\char'31~{\itshape CalWiTree}~\char'31~{\itshape Ratio}\\
\texfamily ~~genprod~n~t~=~product~(getKids~n~t)
\end{tabbing}
\end{minipage}

A sequence with so many nice properties,
one might feel to say with some poetic fervour,
cannot be meaningless. 
Isn't there anything in the (more or less) real world
that these numbers would actually count?
It turns out there is.
There are in fact many things the Stern sequence actually counts.
Just to mention two things:
It counts odd binomial coefficients of the form
$\binom{n-r}{r}, 0 \le 2r \le n$.
That is the odd numbers in the first half of the lines $n-r$
in Pascal's triangle.
This would translate to a function 
creating a sequence of numbers of the form

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~oddCos~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~oddCos~n~=~filter~odd~[choose~(n-r)~r~|~r~\char'06~[0..(n~`div`~2)]]
\end{tabbing}
\end{minipage}

\text{\texfamily fusc\Sp (n+1)} is exactly the size of that sequence.
For instance:
\text{\texfamily oddCos\Sp 5} is \text{\texfamily [1,3]} and \text{\texfamily fusc\Sp 6} is 2;
\text{\texfamily oddCos\Sp 6} is \text{\texfamily [1,5,1]} and \text{\texfamily fusc\Sp 7} is 3;
\text{\texfamily oddCos\Sp 7} is \text{\texfamily [1]} and \text{\texfamily fusc\Sp 8}, which is a power of 2, is 1;
\text{\texfamily oddCos\Sp 8} is \text{\texfamily [1,7,15,1]} and \text{\texfamily fusc\Sp 9} is 4;
\text{\texfamily oddCos\Sp 9} is \text{\texfamily [1,21,5]} and \text{\texfamily fusc\Sp 10} is 3.

Moritz Stern arrived at his sequence,
when studying ways to represent numbers as
powers of 2. Any number can be written
as such a sum and most numbers even in various ways.
For instance, $2 = 2^0 + 2^0$,
$3 = 2^0 + 2^1 = 2^0 + 2^0 + 2^0$,
$4 = 2^1 + 2^1 = 2^0 + 2^0 + 2^1$,
$5 = 2^0 + 2^2 = 2^0 + 2^1 + 2^1$
and so on.
Stern focussed on so called \term{hyperbinary} systems,
that is sums of powers of 2, where any power appears
at most twice.
For instance, $3 = 2^0 + 2^1$ is such a system,
but $3 = 2^0 + 2^0 + 2^0$ is not.
Stern's sequence counts the number of ways
this is possible for any number $n-1$. In other words, 
\text{\texfamily fusc\Sp (n+1)} is exactly the number of hyperbinary systems
for $n$.
For 3, as an example, there is only one way
and \text{\texfamily fusc\Sp 4} is 1;
for 4, there are 3 such systems:
$2^0 + 2^0 + 2^1$, $2^1 + 2^1$ and $2^2$ and
\text{\texfamily fusc\Sp 5} is 3;
for 5, there are only 2 such systems:
$2^0 + 2^1 + 2^1$ and $2^0 + 2^2$ and
\text{\texfamily fusc\Sp 6} is 2.

Finding all hyperbinary systems for a number $n$
is quite an interesting problem in its own right.
A brute-force and, hence, inefficient algorithm
could apply the following logic.
We first find all powers of 2 less than or equal to $n$:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~pows2~::~{\itshape Natural}~\char'31~[{\itshape Natural}]\\
\texfamily ~~pows2~n~=~takeWhile~(\char'34~n)~[2\char94 x~|~x~\char'06~[0..]]
\end{tabbing}
\end{minipage}

We then create all permutations of this set
and try to build sums 
that equal $n$:

\begin{minipage}{\textwidth}
\begin{tabbing}\texfamily
~~hyperbin~::~{\itshape Natural}~\char'31~[[{\itshape Natural}]]\\
\texfamily ~~hyperbin~n~=~nub~(go~\$~perms~\$~pows2~n)\\
\texfamily ~~~~{\bfseries where}~~go~pss~=~filter~~(\char'10k~\char'31~sum~k~\char'36~n)~\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~[sort~(sums~0~ps~ps)~|~ps~\char'06~pss]\\
\texfamily ~~~~~~~~~~~sums~\char95 ~[]~[]~~~~~~=~[]~\\
\texfamily ~~~~~~~~~~~sums~s~[]~ds~~~~~~=~sums~s~ds~[]\\
\texfamily ~~~~~~~~~~~sums~s~(p:ps)~ds~~|~s~+~p~>~n~~~=~sums~s~ds~ps\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~s~+~p~\char'36~n~~=~[p]\\
\texfamily ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~otherwise~~~=~p~:~sums~(s+p)~ps~ds
\end{tabbing}
\end{minipage}

Note that we pass the available pool of powers of 2 $\le n$
twice to \text{\texfamily sums}. When the first instance is exhausted or
$s + p > n$, we start to use the second instance of the pool.
This reflects the fact that we are allowed to use every number
twice.
If the sum $s+p$ equals $n$, we have found a valid
hyperbinary system.
Otherwise, if $s+p < n$, we continue adding the current power
to the result set.
In \text{\texfamily go}, we try \text{\texfamily sums} on all permutations of the powers
filtering the resulting sets for which \text{\texfamily sum} equals $n$.
We sort each list to make lists with equal elements equal 
and to, thus, be able to recognise duplicates and
to remove them with \text{\texfamily nub}.


\end{document}
